2016-09-03 13:57:46	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-03 13:57:46	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-03 13:57:46	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-03 13:57:46	--	Channel #ai: 74 nicks (1 op, 0 voices, 73 normals)
2016-09-03 13:57:48	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-03 13:59:07	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Ping timeout: 252 seconds)
2016-09-03 14:04:11	-->	Malvolio (~Malvolio@unaffiliated/malvolio) has joined #ai
2016-09-03 14:06:12	<--	notfowl (fowl@gateway/shell/elitebnc/x-wrkriicrgpmbownz) has quit (Ping timeout: 276 seconds)
2016-09-03 14:32:38	<--	Th3_Prince (~b3nszy@2601:580:c103:6150:a0bd:c7b9:2b16:c04) has quit (Remote host closed the connection)
2016-09-03 14:45:32	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-03 15:00:52	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Quit: WeeChat 1.4)
2016-09-03 15:01:02	<--	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-03 15:01:15	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-03 15:01:22	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Client Quit)
2016-09-03 15:01:47	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-03 15:13:52	-->	Th3_Prince (~b3nszy@2601:580:c103:6150:915e:4cdd:d4ec:e79f) has joined #ai
2016-09-03 15:14:17	Th3_Prince	I don't get it asher... Why on earth would physics be the right path to ai?
2016-09-03 15:17:50	Asher	who said it was?
2016-09-03 15:18:03	Th3_Prince	idk everywhere I check
2016-09-03 15:18:13	Th3_Prince	Can you help me really quick
2016-09-03 15:18:17	Th3_Prince	this will be the last time I ask
2016-09-03 15:20:27	Th3_Prince	Asher: 
2016-09-03 15:23:49	<--	SuperKoos (~User@unaffiliated/superkoos) has quit (Ping timeout: 240 seconds)
2016-09-03 15:24:36	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Quit: WeeChat 1.4)
2016-09-03 15:24:52	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-03 15:24:57	-->	SwiftMatt (~Objective@162.242.94.81) has joined #ai
2016-09-03 15:25:03	Asher	with what?
2016-09-03 15:25:17	Asher	i don't mind you asking questions so long as it's not the same question
2016-09-03 15:25:20	Asher	over and over
2016-09-03 15:26:04	-->	augur (~augur@2601:640:8001:4222:c5a7:e039:f55f:d466) has joined #ai
2016-09-03 15:26:08	Th3_Prince	ok
2016-09-03 15:26:16	Th3_Prince	so my interests are in many fields
2016-09-03 15:26:23	Th3_Prince	but im really interested in systems 
2016-09-03 15:26:48	Th3_Prince	I also have an interest in the science of the mind... stanford has a course on it called symbolic systems
2016-09-03 15:26:57	Th3_Prince	epistemology and all that
2016-09-03 15:27:00	<--	augur (~augur@2601:640:8001:4222:c5a7:e039:f55f:d466) has quit (Remote host closed the connection)
2016-09-03 15:27:08	Th3_Prince	I particularly like AI
2016-09-03 15:27:31	Th3_Prince	and genetic programming/algorithms, evolutionary computation, and simulation seem interesting 
2016-09-03 15:28:29	-->	Blood-Wiper (~AA104106@173.57.26.131) has joined #ai
2016-09-03 15:28:30	Th3_Prince	also interested in systems science and complex systems pattern recognition  collective behavior and all that
2016-09-03 15:28:52	Th3_Prince	I'm trying to find a field that covers most of that and or allows me to get into those fields.
2016-09-03 15:29:16	Asher	pick a field or two that overlap with what you want to study
2016-09-03 15:29:22	Asher	this is most easily done by selecting classes based o nwhat ou want to study
2016-09-03 15:29:22	Th3_Prince	The subject mentioned A LOT is physics.. I used to like physics but now find it boring but I only took ap physics in highschool.
2016-09-03 15:29:26	Th3_Prince	Which fields do you think overlap?
2016-09-03 15:29:35	Asher	look at what classes are offered
2016-09-03 15:29:35	Th3_Prince	Do you think computer science covers most of those?
2016-09-03 15:29:52	Asher	the more important thing tho is that you read and research on your own in other fields
2016-09-03 15:29:57	Th3_Prince	yes
2016-09-03 15:30:01	Th3_Prince	I will research on my own
2016-09-03 15:30:01	Asher	and that you have your own projects to apply the konwledge you assemble
2016-09-03 15:30:08	Th3_Prince	Yeah
2016-09-03 15:30:18	Th3_Prince	I'm going to learn about everything that interests me on my own
2016-09-03 15:30:30	Th3_Prince	I just want to find a major that trains me to think and is broad
2016-09-03 15:30:38	Th3_Prince	So physics, math, philosophy, cs
2016-09-03 15:30:41	Th3_Prince	I really like cs
2016-09-03 15:30:47	Th3_Prince	and I really like philosophy and math
2016-09-03 15:30:53	-->	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has joined #ai
2016-09-03 15:31:01	Th3_Prince	not so much physics... but physics is seen as the go-to major to for studying systems and AI
2016-09-03 15:32:17	Th3_Prince	plus this is for undergrad
2016-09-03 15:32:28	Th3_Prince	which undergrad degrees set you up with work like this in grad school
2016-09-03 15:38:56	Th3_Prince	?
2016-09-03 15:40:25	Asher	i worked primarily out of comparative literature
2016-09-03 15:40:33	Asher	bc it is a field that let me define my work as intersections of other fields
2016-09-03 15:40:42	Asher	and it is a field where useful and complex philosophy is studied
2016-09-03 15:40:47	Asher	including semiotics and psychoanalysis
2016-09-03 15:40:51	Asher	which imo are the foundation of AGI
2016-09-03 15:43:45	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 244 seconds)
2016-09-03 15:47:38	Th3_Prince	I'm not interested in comparative literature but I appreciate your suggestion 
2016-09-03 15:49:09	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-03 15:49:42	Asher	*shrug*
2016-09-03 15:49:47	Asher	you were the one who asked
2016-09-03 15:50:05	Asher	it's where philosophy has taken place since ww 2
2016-09-03 15:51:30	Asher	my suggestion to anyone who asks about majors is: pick one in hard sciences and one in humanities
2016-09-03 15:57:20	Th3_Prince	why not just pick 1
2016-09-03 16:05:41	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Quit: WeeChat 1.4)
2016-09-03 16:06:01	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-03 16:14:49	<--	djancak1 (~diancak@unaffiliated/djancak) has quit (Read error: Connection reset by peer)
2016-09-03 16:15:12	-->	djancak1 (~diancak@unaffiliated/djancak) has joined #ai
2016-09-03 16:39:43	-->	Coldblackice (~anonz@unaffiliated/coldblackice) has joined #ai
2016-09-03 16:39:58	<--	Coldblackice (~anonz@unaffiliated/coldblackice) has quit (Max SendQ exceeded)
2016-09-03 16:41:24	-->	rimdeker (~rimdeker@unaffiliated/rimdeker) has joined #ai
2016-09-03 16:42:20	-->	Coldblackice (~anonz@unaffiliated/coldblackice) has joined #ai
2016-09-03 16:45:26	<--	Coldblackice (~anonz@unaffiliated/coldblackice) has quit (Client Quit)
2016-09-03 16:49:07	-->	Coldblackice (~anonz@unaffiliated/coldblackice) has joined #ai
2016-09-03 16:58:36	-->	SuperKoos (~User@unaffiliated/superkoos) has joined #ai
2016-09-03 16:59:51	<--	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-03 17:00:46	<--	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has quit (Quit: It's a joke, it's all a joke.)
2016-09-03 17:01:14	-->	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has joined #ai
2016-09-03 17:01:23	-->	Noldorin (~noldorin@unaffiliated/noldorin) has joined #ai
2016-09-03 17:11:33	-->	significance (~significa@216.87.237.249) has joined #ai
2016-09-03 17:11:42	<--	significance (~significa@216.87.237.249) has left #ai
2016-09-03 17:14:14	<--	Th3_Prince (~b3nszy@2601:580:c103:6150:915e:4cdd:d4ec:e79f) has quit (Remote host closed the connection)
2016-09-03 17:23:16	<--	amz3` (~amz3@unaffiliated/abki) has quit (Ping timeout: 264 seconds)
2016-09-03 18:01:00	<--	rimdeker (~rimdeker@unaffiliated/rimdeker) has quit (Remote host closed the connection)
2016-09-03 18:03:21	<--	Noldorin (~noldorin@unaffiliated/noldorin) has quit (Ping timeout: 276 seconds)
2016-09-03 18:10:50	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-03 18:13:06	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Ping timeout: 276 seconds)
2016-09-03 18:33:21	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-03 18:46:48	-->	causative (~halberd@unaffiliated/halberd) has joined #ai
2016-09-03 19:10:18	<--	Wagoo (~wagoo@116.167.203.62.dynamic.wline.res.cust.swisscom.ch) has quit (Ping timeout: 265 seconds)
2016-09-03 19:14:41	-->	Noldorin (~noldorin@unaffiliated/noldorin) has joined #ai
2016-09-03 19:17:16	<--	rasmoo (~ras@unaffiliated/rasmoo) has quit (Quit: MAGA)
2016-09-03 19:54:20	<--	Noldorin (~noldorin@unaffiliated/noldorin) has quit (Ping timeout: 244 seconds)
2016-09-03 20:05:29	-->	Noldorin (~noldorin@unaffiliated/noldorin) has joined #ai
2016-09-03 20:18:23	-->	fowl (~fowl@unaffiliated/fowlmouth) has joined #ai
2016-09-03 20:19:51	<--	pffffffft (~pffffffft@unaffiliated/pffffffft) has quit (Ping timeout: 276 seconds)
2016-09-03 20:21:38	-->	pressure679 (~user@88.83.20.91) has joined #ai
2016-09-03 20:28:45	<--	SuperKoos (~User@unaffiliated/superkoos) has quit (Quit: Leaving.)
2016-09-03 20:47:50	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-03 20:48:25	<--	pressure679 (~user@88.83.20.91) has quit (Read error: Connection reset by peer)
2016-09-03 20:48:57	-->	pressure679 (~user@88.83.20.91) has joined #ai
2016-09-03 21:34:35	<--	Noldorin (~noldorin@unaffiliated/noldorin) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-03 21:39:46	-->	tectonic (~tectonic@pool-71-173-73-192.ptldme.east.myfairpoint.net) has joined #ai
2016-09-03 21:49:11	-->	realz (~realz@unaffiliated/realazthat) has joined #ai
2016-09-03 21:55:10	<--	tectonic (~tectonic@pool-71-173-73-192.ptldme.east.myfairpoint.net) has quit
2016-09-03 22:07:43	<--	shymega (~shymega@shymega.vps.bitfolk.com) has quit (Quit: (let ((quit t)) (when quit (message "Leaving."))))
2016-09-03 22:34:07	-->	liefer (~liefer@3e6b4ca3.rev.stofanet.dk) has joined #ai
2016-09-03 22:44:38	<--	pressure679 (~user@88.83.20.91) has quit (Read error: Connection reset by peer)
2016-09-03 22:49:42	-->	pressure679 (~user@88.83.20.91) has joined #ai
2016-09-03 23:40:54	-->	JoshS (~jshjsh@172.56.39.254) has joined #ai
2016-09-03 23:47:49	<--	Blood-Wiper (~AA104106@173.57.26.131) has quit (Read error: Connection reset by peer)
2016-09-04 00:16:37	-->	jbalint (~jbalint@unaffiliated/jbalint) has joined #ai
2016-09-04 00:20:32	-->	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has joined #ai
2016-09-04 00:40:04	<--	pressure679 (~user@88.83.20.91) has quit (Read error: Connection reset by peer)
2016-09-04 00:41:25	<--	Viscid (~viscid@CPE00fc8d4cfa33-CM00fc8d4cfa30.cpe.net.cable.rogers.com) has quit
2016-09-04 00:45:11	-->	pressure679 (~user@88.83.20.91) has joined #ai
2016-09-04 01:12:16	--	alzagros is now known as caveman
2016-09-04 01:18:31	-->	thebope_ (~thebope@pdpc/supporter/student/thebope) has joined #ai
2016-09-04 01:30:58	<--	pressure679 (~user@88.83.20.91) has quit (Read error: Connection reset by peer)
2016-09-04 01:36:51	-->	jshjsh (~jshjsh@172.56.39.254) has joined #ai
2016-09-04 01:37:10	<--	JoshS (~jshjsh@172.56.39.254) has quit (Disconnected by services)
2016-09-04 01:37:18	--	jshjsh is now known as JoshS
2016-09-04 01:53:35	--	irc: disconnected from server
2016-09-04 12:32:21	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-04 12:32:21	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-04 12:32:21	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-04 12:32:21	--	Channel #ai: 81 nicks (1 op, 0 voices, 80 normals)
2016-09-04 12:32:21	***	Buffer Playback...
2016-09-04 12:32:21	Th3_Prince	[15:14:17] I don't get it asher... Why on earth would physics be the right path to ai?
2016-09-04 12:32:21	Asher	[15:17:50] who said it was?
2016-09-04 12:32:21	Th3_Prince	[15:18:03] idk everywhere I check
2016-09-04 12:32:21	Th3_Prince	[15:18:13] Can you help me really quick
2016-09-04 12:32:21	Th3_Prince	[15:18:17] this will be the last time I ask
2016-09-04 12:32:21	Th3_Prince	[15:20:27] Asher: 
2016-09-04 12:32:21	Asher	[15:25:03] with what?
2016-09-04 12:32:21	Asher	[15:25:17] i don't mind you asking questions so long as it's not the same question
2016-09-04 12:32:21	Asher	[15:25:20] over and over
2016-09-04 12:32:21	Th3_Prince	[15:26:08] ok
2016-09-04 12:32:21	Th3_Prince	[15:26:16] so my interests are in many fields
2016-09-04 12:32:21	Th3_Prince	[15:26:23] but im really interested in systems 
2016-09-04 12:32:21	Th3_Prince	[15:26:48] I also have an interest in the science of the mind... stanford has a course on it called symbolic systems
2016-09-04 12:32:21	Th3_Prince	[15:26:57] epistemology and all that
2016-09-04 12:32:21	Th3_Prince	[15:27:08] I particularly like AI
2016-09-04 12:32:21	Th3_Prince	[15:27:31] and genetic programming/algorithms, evolutionary computation, and simulation seem interesting 
2016-09-04 12:32:21	Th3_Prince	[15:28:30] also interested in systems science and complex systems pattern recognition  collective behavior and all that
2016-09-04 12:32:21	Th3_Prince	[15:28:52] I'm trying to find a field that covers most of that and or allows me to get into those fields.
2016-09-04 12:32:21	Asher	[15:29:16] pick a field or two that overlap with what you want to study
2016-09-04 12:32:21	Asher	[15:29:22] this is most easily done by selecting classes based o nwhat ou want to study
2016-09-04 12:32:21	Th3_Prince	[15:29:22] The subject mentioned A LOT is physics.. I used to like physics but now find it boring but I only took ap physics in highschool.
2016-09-04 12:32:21	Th3_Prince	[15:29:26] Which fields do you think overlap?
2016-09-04 12:32:21	Asher	[15:29:35] look at what classes are offered
2016-09-04 12:32:21	Th3_Prince	[15:29:35] Do you think computer science covers most of those?
2016-09-04 12:32:21	Asher	[15:29:52] the more important thing tho is that you read and research on your own in other fields
2016-09-04 12:32:21	Th3_Prince	[15:29:57] yes
2016-09-04 12:32:21	Th3_Prince	[15:30:01] I will research on my own
2016-09-04 12:32:21	Asher	[15:30:01] and that you have your own projects to apply the konwledge you assemble
2016-09-04 12:32:21	Th3_Prince	[15:30:08] Yeah
2016-09-04 12:32:21	Th3_Prince	[15:30:18] I'm going to learn about everything that interests me on my own
2016-09-04 12:32:21	Th3_Prince	[15:30:30] I just want to find a major that trains me to think and is broad
2016-09-04 12:32:21	Th3_Prince	[15:30:38] So physics, math, philosophy, cs
2016-09-04 12:32:21	Th3_Prince	[15:30:41] I really like cs
2016-09-04 12:32:21	Th3_Prince	[15:30:47] and I really like philosophy and math
2016-09-04 12:32:21	Th3_Prince	[15:31:01] not so much physics... but physics is seen as the go-to major to for studying systems and AI
2016-09-04 12:32:21	Th3_Prince	[15:32:17] plus this is for undergrad
2016-09-04 12:32:21	Th3_Prince	[15:32:28] which undergrad degrees set you up with work like this in grad school
2016-09-04 12:32:21	Th3_Prince	[15:38:56] ?
2016-09-04 12:32:21	Asher	[15:40:25] i worked primarily out of comparative literature
2016-09-04 12:32:21	Asher	[15:40:33] bc it is a field that let me define my work as intersections of other fields
2016-09-04 12:32:21	Asher	[15:40:42] and it is a field where useful and complex philosophy is studied
2016-09-04 12:32:21	Asher	[15:40:47] including semiotics and psychoanalysis
2016-09-04 12:32:21	Asher	[15:40:51] which imo are the foundation of AGI
2016-09-04 12:32:21	Th3_Prince	[15:47:38] I'm not interested in comparative literature but I appreciate your suggestion 
2016-09-04 12:32:21	Asher	[15:49:42] *shrug*
2016-09-04 12:32:21	Asher	[15:49:47] you were the one who asked
2016-09-04 12:32:21	Asher	[15:50:05] it's where philosophy has taken place since ww 2
2016-09-04 12:32:21	Asher	[15:51:30] my suggestion to anyone who asks about majors is: pick one in hard sciences and one in humanities
2016-09-04 12:32:21	Th3_Prince	[15:57:20] why not just pick 1
2016-09-04 12:32:21	***	Playback Complete.
2016-09-04 12:32:38	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-04 12:36:25	-->	lgr (~lgr@host109-156-95-65.range109-156.btcentralplus.com) has joined #ai
2016-09-04 12:39:02	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Quit: WeeChat 1.4)
2016-09-04 12:40:50	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-04 12:49:33	-->	Hagbard-Celine (~Hagbard-C@179.43.174.2) has joined #ai
2016-09-04 12:50:26	-->	jshjsh (~jshjsh@172.56.39.254) has joined #ai
2016-09-04 12:52:09	<--	govg (~govg@unaffiliated/govg) has quit (Ping timeout: 244 seconds)
2016-09-04 12:53:08	-->	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has joined #ai
2016-09-04 12:53:31	-->	Noldorin (~noldorin@unaffiliated/noldorin) has joined #ai
2016-09-04 12:53:35	<--	JoshS (~jshjsh@172.56.39.254) has quit (Ping timeout: 250 seconds)
2016-09-04 12:53:41	--	jshjsh is now known as JoshS
2016-09-04 12:54:06	-->	govg (~govg@unaffiliated/govg) has joined #ai
2016-09-04 13:01:17	<--	thebope_ (~thebope@pdpc/supporter/student/thebope) has quit (Remote host closed the connection)
2016-09-04 13:01:34	-->	thebope_ (~thebope@pdpc/supporter/student/thebope) has joined #ai
2016-09-04 13:21:34	<--	Hagbard-Celine (~Hagbard-C@179.43.174.2) has left #ai ("Textual IRC Client: www.textualapp.com")
2016-09-04 13:24:43	<--	lgr (~lgr@host109-156-95-65.range109-156.btcentralplus.com) has quit (Read error: Connection reset by peer)
2016-09-04 13:29:55	-->	SuperKoos (~User@unaffiliated/superkoos) has joined #ai
2016-09-04 13:48:49	-->	iH2O (~chatzilla@unaffiliated/myfree) has joined #ai
2016-09-04 13:53:49	<--	Noldorin (~noldorin@unaffiliated/noldorin) has quit (Ping timeout: 250 seconds)
2016-09-04 14:08:10	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Quit: WeeChat 1.4)
2016-09-04 14:08:26	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-04 14:10:48	<--	thebope_ (~thebope@pdpc/supporter/student/thebope) has quit (Remote host closed the connection)
2016-09-04 14:14:31	<--	iH2O (~chatzilla@unaffiliated/myfree) has left #ai
2016-09-04 14:59:49	-->	thebope_ (~thebope@pdpc/supporter/student/thebope) has joined #ai
2016-09-04 15:04:07	<--	thebope_ (~thebope@pdpc/supporter/student/thebope) has quit (Remote host closed the connection)
2016-09-04 15:27:10	<--	JoshS (~jshjsh@172.56.39.254) has quit (Ping timeout: 244 seconds)
2016-09-04 15:29:32	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Ping timeout: 240 seconds)
2016-09-04 15:38:07	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-04 16:00:29	-->	JoshS (~jshjsh@172.56.39.254) has joined #ai
2016-09-04 16:08:07	<--	amz3` (~amz3@unaffiliated/abki) has quit (Quit: Artufath 1.5)
2016-09-04 16:13:05	<--	shymega (~shymega@shymega.vps.bitfolk.com) has quit (Quit: (let ((quit t)) (when quit (message "Leaving."))))
2016-09-04 16:15:09	-->	shymega (~shymega@shymega.vps.bitfolk.com) has joined #ai
2016-09-04 16:33:58	-->	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has joined #ai
2016-09-04 16:37:16	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Quit: WeeChat 1.4)
2016-09-04 16:37:33	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-04 16:39:04	<--	pffffffft (~pffffffft@unaffiliated/pffffffft) has quit (Ping timeout: 252 seconds)
2016-09-04 16:40:54	-->	Hagbard-Celine (~Hagbard-C@179.43.156.194) has joined #ai
2016-09-04 16:41:35	<--	Hagbard-Celine (~Hagbard-C@179.43.156.194) has left #ai
2016-09-04 16:50:57	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Ping timeout: 265 seconds)
2016-09-04 16:57:03	-->	causative (~halberd@unaffiliated/halberd) has joined #ai
2016-09-04 17:06:12	<--	dngor (~abuse@p3m/dngor) has quit (Ping timeout: 240 seconds)
2016-09-04 17:14:54	<--	darsie (~darsie@84-112-128-43.cable.dynamic.surfer.at) has quit (Read error: Connection reset by peer)
2016-09-04 17:23:15	-->	dngor (~abuse@p3m/dngor) has joined #ai
2016-09-04 17:23:57	-->	fowlmouth (~fowl@unaffiliated/fowlmouth) has joined #ai
2016-09-04 17:26:03	<--	fowl (~fowl@unaffiliated/fowlmouth) has quit (Ping timeout: 240 seconds)
2016-09-04 17:32:16	-->	SwiftMatt (~Objective@162.242.94.232) has joined #ai
2016-09-04 17:35:23	-->	Noldorin (~noldorin@unaffiliated/noldorin) has joined #ai
2016-09-04 17:51:16	<--	rasmoo (~ras@unaffiliated/rasmoo) has quit (Quit: MAGA)
2016-09-04 18:01:51	<--	EI24 (~EI24@90-224-11-165-no124.tbcn.telia.com) has quit (Remote host closed the connection)
2016-09-04 18:45:51	<--	rimdeker (~rimdeker@unaffiliated/rimdeker) has quit (Remote host closed the connection)
2016-09-04 19:17:26	-->	tectonic (~tectonic@cpe-74-72-210-61.nyc.res.rr.com) has joined #ai
2016-09-04 19:23:31	-->	Blood-Wiper (~AA104106@173.57.26.131) has joined #ai
2016-09-04 19:33:19	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-04 19:48:14	<--	tectonic (~tectonic@cpe-74-72-210-61.nyc.res.rr.com) has quit
2016-09-04 19:51:43	<--	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has quit (Ping timeout: 265 seconds)
2016-09-04 20:01:12	<--	SuperKoos (~User@unaffiliated/superkoos) has quit (Quit: Leaving.)
2016-09-04 20:54:27	<--	JoshS (~jshjsh@172.56.39.254) has quit (Ping timeout: 264 seconds)
2016-09-04 21:17:54	--	irc: disconnected from server
2016-09-05 21:58:39	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-05 21:58:39	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-05 21:58:39	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-05 21:58:39	--	Channel #ai: 77 nicks (1 op, 0 voices, 76 normals)
2016-09-05 21:58:39	***	Buffer Playback...
2016-09-05 21:58:39	Th3_Prince	[15:14:17] I don't get it asher... Why on earth would physics be the right path to ai?
2016-09-05 21:58:39	Asher	[15:17:50] who said it was?
2016-09-05 21:58:39	Th3_Prince	[15:18:03] idk everywhere I check
2016-09-05 21:58:39	Th3_Prince	[15:18:13] Can you help me really quick
2016-09-05 21:58:39	Th3_Prince	[15:18:17] this will be the last time I ask
2016-09-05 21:58:39	Th3_Prince	[15:20:27] Asher: 
2016-09-05 21:58:39	Asher	[15:25:03] with what?
2016-09-05 21:58:39	Asher	[15:25:17] i don't mind you asking questions so long as it's not the same question
2016-09-05 21:58:39	Asher	[15:25:20] over and over
2016-09-05 21:58:39	Th3_Prince	[15:26:08] ok
2016-09-05 21:58:39	Th3_Prince	[15:26:16] so my interests are in many fields
2016-09-05 21:58:39	Th3_Prince	[15:26:23] but im really interested in systems 
2016-09-05 21:58:39	Th3_Prince	[15:26:48] I also have an interest in the science of the mind... stanford has a course on it called symbolic systems
2016-09-05 21:58:39	Th3_Prince	[15:26:57] epistemology and all that
2016-09-05 21:58:39	Th3_Prince	[15:27:08] I particularly like AI
2016-09-05 21:58:39	Th3_Prince	[15:27:31] and genetic programming/algorithms, evolutionary computation, and simulation seem interesting 
2016-09-05 21:58:39	Th3_Prince	[15:28:30] also interested in systems science and complex systems pattern recognition  collective behavior and all that
2016-09-05 21:58:39	Th3_Prince	[15:28:52] I'm trying to find a field that covers most of that and or allows me to get into those fields.
2016-09-05 21:58:39	Asher	[15:29:16] pick a field or two that overlap with what you want to study
2016-09-05 21:58:39	Asher	[15:29:22] this is most easily done by selecting classes based o nwhat ou want to study
2016-09-05 21:58:39	Th3_Prince	[15:29:22] The subject mentioned A LOT is physics.. I used to like physics but now find it boring but I only took ap physics in highschool.
2016-09-05 21:58:39	Th3_Prince	[15:29:26] Which fields do you think overlap?
2016-09-05 21:58:39	Asher	[15:29:35] look at what classes are offered
2016-09-05 21:58:39	Th3_Prince	[15:29:35] Do you think computer science covers most of those?
2016-09-05 21:58:39	Asher	[15:29:52] the more important thing tho is that you read and research on your own in other fields
2016-09-05 21:58:39	Th3_Prince	[15:29:57] yes
2016-09-05 21:58:39	Th3_Prince	[15:30:01] I will research on my own
2016-09-05 21:58:39	Asher	[15:30:01] and that you have your own projects to apply the konwledge you assemble
2016-09-05 21:58:39	Th3_Prince	[15:30:08] Yeah
2016-09-05 21:58:39	Th3_Prince	[15:30:18] I'm going to learn about everything that interests me on my own
2016-09-05 21:58:39	Th3_Prince	[15:30:30] I just want to find a major that trains me to think and is broad
2016-09-05 21:58:39	Th3_Prince	[15:30:38] So physics, math, philosophy, cs
2016-09-05 21:58:39	Th3_Prince	[15:30:41] I really like cs
2016-09-05 21:58:39	Th3_Prince	[15:30:47] and I really like philosophy and math
2016-09-05 21:58:39	Th3_Prince	[15:31:01] not so much physics... but physics is seen as the go-to major to for studying systems and AI
2016-09-05 21:58:39	Th3_Prince	[15:32:17] plus this is for undergrad
2016-09-05 21:58:39	Th3_Prince	[15:32:28] which undergrad degrees set you up with work like this in grad school
2016-09-05 21:58:39	Th3_Prince	[15:38:56] ?
2016-09-05 21:58:39	Asher	[15:40:25] i worked primarily out of comparative literature
2016-09-05 21:58:39	Asher	[15:40:33] bc it is a field that let me define my work as intersections of other fields
2016-09-05 21:58:39	Asher	[15:40:42] and it is a field where useful and complex philosophy is studied
2016-09-05 21:58:39	Asher	[15:40:47] including semiotics and psychoanalysis
2016-09-05 21:58:39	Asher	[15:40:51] which imo are the foundation of AGI
2016-09-05 21:58:39	Th3_Prince	[15:47:38] I'm not interested in comparative literature but I appreciate your suggestion 
2016-09-05 21:58:39	Asher	[15:49:42] *shrug*
2016-09-05 21:58:39	Asher	[15:49:47] you were the one who asked
2016-09-05 21:58:39	Asher	[15:50:05] it's where philosophy has taken place since ww 2
2016-09-05 21:58:39	Asher	[15:51:30] my suggestion to anyone who asks about majors is: pick one in hard sciences and one in humanities
2016-09-05 21:58:39	Th3_Prince	[15:57:20] why not just pick 1
2016-09-05 21:58:39	dknitrox	[03:56:13] hi all i'm newbie
2016-09-05 21:58:39	Lyote	[04:54:19] Aren't we all?
2016-09-05 21:58:39	Drew_	[12:10:34] very much so
2016-09-05 21:58:39	ipoloskov	[14:02:42] http://nautil.us/issue/40/learning/is-artificial-intelligence-permanently-inscrutable
2016-09-05 21:58:39	YYZ	[14:04:34] Asher: it seems nearly impossible to give solid advice to folks about what they should study, unless you know the person well.
2016-09-05 21:58:39	YYZ	[14:05:57] what if some Kid Genius already knows advanced math at age 16 from self study, should he study math?
2016-09-05 21:58:39	Asher	[14:06:42] said kid genius probably isn't asking
2016-09-05 21:58:39	YYZ	[14:06:52] it seems like people should study what they can't naturally excel at
2016-09-05 21:58:39	YYZ	[14:06:55] lol, good point
2016-09-05 21:58:39	YYZ	[14:07:20] if you're good at math sure take math, but you'll be good at it anyways so why not study something "hard" ?
2016-09-05 21:58:39	YYZ	[14:08:45] Asher don't you think someone would likely learn more than they expected if they learned a 2nd or 3rd language ?
2016-09-05 21:58:39	YYZ	[14:09:18] even if it was like Latin or ancient Greek, just the study would really pry open their brain and help them to absorb all sorts of new ideas
2016-09-05 21:58:39	YYZ	[14:10:38] but let's suppose studying a language is "hard", but another class the student can get an easy A, so he takes the "easy" class and doesn't really learn new things
2016-09-05 21:58:39	YYZ	[14:11:50] or the opposite, he's from another country and just takes that language class to get an easy A, instead of a "hard" class where he would learn more by getting a C
2016-09-05 21:58:39	Asher	[14:18:07] yes
2016-09-05 21:58:39	Asher	[14:18:16] it's not about easy or hard
2016-09-05 21:58:39	Asher	[14:18:25] it's about being exposed to distinct domains of knowledge
2016-09-05 21:58:39	YYZ	[14:25:50] right, "easy" being familiar and/or already known is what I mean
2016-09-05 21:58:39	YYZ	[14:26:39] if you study an unfamiliar domain, especially where you can integrate it with your strengths, it could be more beneficial
2016-09-05 21:58:39	YYZ	[14:27:26] I guess the only general advice then is to take a technical & humanities courses
2016-09-05 21:58:39	Asher	[14:28:23] and projects
2016-09-05 21:58:39	YYZ	[14:28:45] right
2016-09-05 21:58:39	YYZ	[14:29:37] I saw an interview with an Art professor, a semi-famous artist named Chris Burden, and when he was in Art school he realized that all the materials laying around the art class were essentially his to do whatever he wanted, since no one else was really doing any projects
2016-09-05 21:58:39	YYZ	[14:30:12] so a big pile of plywood was all his, no one else had any idea what sort of project to use it for
2016-09-05 21:58:39	YYZ	[14:31:17] so he made these massive installation sort of projects that were very popular around the campus, even if  no one knew who made them 
2016-09-05 21:58:39	YYZ	[14:31:51] out of plywood or sheets of plastic, stuff lesser artists didn't think of
2016-09-05 21:58:39	YYZ	[14:33:10] Asher: I suspect that many students only think about grades and getting a diploma, and not about using the available resources and doing projects and such
2016-09-05 21:58:39	Asher	[14:33:54] yea
2016-09-05 21:58:39	YYZ	[14:34:37] anyone could have set up posts and attached sheets of plastic to it, to make an outdoor hallway, the stuff was just laying around, it's not like Chris Burden went and bought the materials
2016-09-05 21:58:39	YYZ	[14:35:07] that's an example of one of the things he built
2016-09-05 21:58:39	YYZ	[14:36:44] or a simple sculpture made of plywood, one that looks simple but looks different from various perspectives, as if you can't tell if it has 4, 5, or 9 sides
2016-09-05 21:58:39	precognist	[15:02:38] Hello.
2016-09-05 21:58:39	appa-nerd	[15:10:35] YYZ: love the article, few AI articles are worth reading.  The circuit program sounds like a genetic program but it sounds like it just needs better simulation or maybe a physical output to give it better training set
2016-09-05 21:58:39	dknitrox	[15:53:08] hi
2016-09-05 21:58:39	Lyote	[17:11:38] appa-nerd: Are you talking about ipoloskov's article?
2016-09-05 21:58:39	appa-nerd	[17:12:32] Oh yeah, they are both the same green on my viewer, thought it was yy
2016-09-05 21:58:39	Lyote	[18:05:33] Are you using weechat? They're both green on my end too. :)
2016-09-05 21:58:39	precognist	[20:03:11] Hello.
2016-09-05 21:58:39	precognist	[20:04:02] I'm working on a simple FSM and was wondering if anyone could give me advice on transitions?
2016-09-05 21:58:39	precognist	[20:06:55] Where should the transitions be invoked? Should the state control the transition? or the manager?
2016-09-05 21:58:39	Asher	[20:09:18] between the spaghetti monster's tentacles
2016-09-05 21:58:39	ipoloskov	[20:15:07] http://nautil.us/issue/40/learning/is-artificial-intelligence-permanently-inscrutable
2016-09-05 21:58:39	ipoloskov	[20:15:18] I think this article makes one thing clear... and I'm reading 'between the lines' as it were
2016-09-05 21:58:39	ipoloskov	[20:15:47] We are at a stage where  Deep Learning is so pervasive in AI research that  the word "AI" is becoming synonymous with  Deep Networks
2016-09-05 21:58:39	Asher	[20:16:18] this is true
2016-09-05 21:58:39	ipoloskov	[20:16:53] POMDPs and navigation algorithms like  SLAM are being almost completely ignored by science media.
2016-09-05 21:58:39	Asher	[20:17:55] i think they are seen as subsumed
2016-09-05 21:58:39	Asher	[20:18:18] but any attempt to model things like language has been surrendered entirely
2016-09-05 21:58:39	ipoloskov	[20:18:42] Asher   Very rarely too rarely I would say,  a researcher will come out and say  Deep Networks are good for  VISION   (and maybe some other kinds of recognition of objects like audio phonemes)
2016-09-05 21:58:39	ipoloskov	[20:18:55] But outside of that  a network cannot plan.  It cannot make complex decisions. It cannot navigate space
2016-09-05 21:58:39	Asher	[20:19:09] deep learning is pretty effective if you can define an open FSM
2016-09-05 21:58:39	Asher	[20:19:28] like Go
2016-09-05 21:58:39	Asher	[20:19:31] Go is just a giant state machine
2016-09-05 21:58:39	Asher	[20:19:40] it is always in a definite finite state
2016-09-05 21:58:39	ipoloskov	[20:19:40] The world we occupy has space, time,  and rules of physics.
2016-09-05 21:58:39	Asher	[20:19:51] maybe
2016-09-05 21:58:39	Asher	[20:19:56] those things emerge and are undeniable no doubt
2016-09-05 21:58:39	Asher	[20:20:00] but how they are situated is less clear
2016-09-05 21:58:39	ipoloskov	[20:20:52] Asher  Let me be more clear.  The article's title was "Is Artificial Intelligence Permanently Inscrutable?"
2016-09-05 21:58:39	ipoloskov	[20:21:03] When it should have been "Are deep networks permanently inscrutable?"
2016-09-05 21:58:39	Asher	[20:21:32] yeah
2016-09-05 21:58:39	Asher	[20:21:56] so i work on AGI by way of modeling natural language as the symbol grounding problem
2016-09-05 21:58:39	Asher	[20:22:08] i've created a new database architecture based on Turing's work with ordinal logics
2016-09-05 21:58:39	Asher	[20:22:13] that functions to implement language plasticity
2016-09-05 21:58:39	Asher	[20:22:30] it's entirely different than all the ML work bc it is 100% formal in implementation
2016-09-05 21:58:39	Asher	[20:22:36] but its treatment is not formal - its treatment is plasticity
2016-09-05 21:58:39	ipoloskov	[20:22:40] Right.    It's like some researchers are so sick-and-tired of the abuse of the phrase "Artificial INtelligence" that they had to make a new word "AGI"  to refer to their work.  It's sad
2016-09-05 21:58:39	Asher	[20:22:54] yu[p
2016-09-05 21:58:39	ipoloskov	[20:23:25] And so in the context of AGI,  deep networks only really play a role in perception (90% of the time its going to vision)
2016-09-05 21:58:39	Asher	[20:23:47] imo the alphago victory is an astounding number-crunching feat and an astounding proof of the role of numbers in intelligent determiantions
2016-09-05 21:58:39	Asher	[20:23:51] but it has little to do with AGI
2016-09-05 21:58:39	Asher	[20:23:58] perception => sensation
2016-09-05 21:58:39	ipoloskov	[20:24:16] absolutely.   In my humble opinion,  POMDPs are way closer to AGI than a deep network is
2016-09-05 21:58:39	Asher	[20:24:17] i would say sensation rather than perception bc it isn't just about perceiving it's about statistical mappings of informal expressions to formal counterparts
2016-09-05 21:58:39	Asher	[20:24:59] my work models language as the intervals between points of determination in its circuit of exchange (the circuit by which it becomes an express that expresses a meaning)
2016-09-05 21:58:39	Asher	[20:25:16] the structure of a basic unit of information: https://www.dropbox.com/s/9n2910vbcdikuac/Term.png?dl=0
2016-09-05 21:58:39	Asher	[20:25:25] and the structure of the unit of information in relation to itself (movement across time): https://www.dropbox.com/s/6ihf8v4l235zdyh/Sign.png?dl=0
2016-09-05 21:58:39	Asher	[20:25:40] *becomes an expression
2016-09-05 21:58:39	precognist	[20:26:35]  philosophy, isn't it?
2016-09-05 21:58:39	Asher	[20:27:08] it's constructed from an intersection of Turing, psychoanalysis and semiotics
2016-09-05 21:58:39	Asher	[20:27:20] but it's mathematical not conceptual
2016-09-05 21:58:39	Asher	[20:27:56] it's a diagram of the nodes where events occur and the edges those events share
2016-09-05 21:58:39	ipoloskov	[20:28:04] an AGI is goingto _require_ a 'theatre of perceptual awareness'   wherein it actively contemplates a situation vis-a-vis a longterm memory compared against the current situation based on likeness
2016-09-05 21:58:39	Asher	[20:28:07] and it works like a circuit, with directional flow
2016-09-05 21:58:39	Asher	[20:28:37] ipoloskov - exactly. i've found the comparison useful: a robot with cameras for eyes vs a robot that uses cameras to gather data to construct a 3d model simulating its environment
2016-09-05 21:58:39	Asher	[20:29:09] our eyes are not cameras
2016-09-05 21:58:39	Asher	[20:29:12] they are data ports
2016-09-05 21:58:39	Asher	[20:29:28] vision comes from the brain not the eyes
2016-09-05 21:58:39	Asher	[20:29:35] the eyes just give the brain the data to make vision relevant
2016-09-05 21:58:39	Asher	[20:29:59] (but then we can start trying to define where the eyes and the brain are, and we find that it's hard to divide the optic nerve)
2016-09-05 21:58:39	ipoloskov	[20:31:36] The deep networks are feed-forward.     When a human sees an image of a panda it activates a cascade of past memories of panda interactions,  including episodes seen on TV.. trips to a zoo when you were 8 years old.   Maybe even a memeory of Kung Fu panda the cartoon if you saw it recently and so on
2016-09-05 21:58:39	causative	[20:31:40] <ipoloskov> But outside of that  a network cannot plan.  It cannot make complex decisions. It cannot navigate space
2016-09-05 21:58:39	causative	[20:32:13] DeepMind made a deep neural network that plays Atari - requiring planning and spatial navigation
2016-09-05 21:58:39	ipoloskov	[20:32:48] An AGI will have episodic memory.  It will not just blindly associate featres through layered "filter".    It will compare and contrast a database of temporal experiences.   
2016-09-05 21:58:39	Asher	[20:32:52] ipoloskov - you should take a look at Turing's paper on ordinal logics
2016-09-05 21:58:39	Asher	[20:33:04] it's all about how to construct a structure for this anticipation
2016-09-05 21:58:39	Asher	[20:33:22] the "active cascade" as you put it
2016-09-05 21:58:39	ipoloskov	[20:33:45] Asher   There was a guy in here a few weeks ago who was mature enough to admit this (or maybe it was in #machinelearning ).  What these fancy networks that can classify "data sets"  with 98% accuracy and so on  ... these algorithms are really just fancy hash functions.  
2016-09-05 21:58:39	ipoloskov	[20:33:53] They don't "See" objects.
2016-09-05 21:58:39	Asher	[20:34:12] absolutely - they are constructing a gigantic dictionary of mappings between informal inputs and formal identities
2016-09-05 21:58:39	ipoloskov	[20:34:19] They see 2D features.. because they were trained on 2D features
2016-09-05 21:58:39	ipoloskov	[20:35:33] Asher  This is all a human being really is.  We are layers of neuronal tissue that are able to very quickly  conjure up  entire episodes that are relevant to the current one.    
2016-09-05 21:58:39	ipoloskov	[20:35:44] There is no narrow AI agent in existence today that comes near to this.
2016-09-05 21:58:39	ipoloskov	[20:35:59] HOw do I know this?  Because everything about human language is episodic.
2016-09-05 21:58:39	ipoloskov	[20:36:05] "I went  for a walk"  
2016-09-05 21:58:39	ipoloskov	[20:36:07] episode
2016-09-05 21:58:39	ipoloskov	[20:36:16] actors taking actions in a scene. Beginning, middle, end.
2016-09-05 21:58:39	ipoloskov	[20:36:46] OUr whole human psychology is arranged around  actors taking actions in a scene. Beginning/middle/end.
2016-09-05 21:58:39	causative	[20:36:48] language is not thinking; language only expresses thinking
2016-09-05 21:58:39	ipoloskov	[20:36:53] "We vacationed in Berline"
2016-09-05 21:58:39	Asher	[20:37:02] causative confuses expression with language
2016-09-05 21:58:39	Asher	[20:37:08] (a common misunderstanding)
2016-09-05 21:58:39	causative	[20:37:16] mental language is not thinking
2016-09-05 21:58:39	Asher	[20:37:17] language is the translatability of expression
2016-09-05 21:58:39	Asher	[20:37:25] which is thinking
2016-09-05 21:58:39	ipoloskov	[20:37:29] Everythign about the human brains is meant to  digest and break down the natural world around us into digestible  "chunks"  called "episodes"
2016-09-05 21:58:39	causative	[20:37:36] if you think in words in your head, the words are just an expression of the end result of a nonverbal thinking process
2016-09-05 21:58:39	Asher	[20:38:06] the words are tags that are parts of thoughts that are activated along with their tags, thereby producing "meaning" along with the word
2016-09-05 21:58:39	causative	[20:38:06] they help as a mnemonic and to keep you focused but they are not, themselves, thought
2016-09-05 21:58:39	ipoloskov	[20:38:07] How do I know this?   Because I'm jaded by some ideology? No.  I know this because this is exactly how we talk about the world around us --->  "The boy kicked the ball."
2016-09-05 21:58:39	Asher	[20:38:32] words work bc they function as activations for memories
2016-09-05 21:58:39	ipoloskov	[20:38:42] There is a very good pragmatic reason to divide the complex multi-dimensional caucophony of the outside world into digestible "episodes"
2016-09-05 21:58:39	Asher	[20:38:51] just like when you see the same object a second time and it reactivates the experience, words do the same thing
2016-09-05 21:58:39	Asher	[20:38:56] words activate the concept they are
2016-09-05 21:58:39	ipoloskov	[20:39:55] Asher   HEre is where it has to come down to.   An AGI is tasked when getting in a car, driving to a park, and constructing a bonfire at that park.    Depending on teh EPISODE in that task in which it finds itself, it will attend to certain objects in its environment and ignore others.  So while building the bonfire, it will ignore all traffic wizzing by.  But while driving,  sticks are ignored 
2016-09-05 21:58:39	ipoloskov	[20:39:55] and cars are attended to.
2016-09-05 21:58:39	Asher	[20:40:15] the difficulty is how to divide the episode
2016-09-05 21:58:39	Asher	[20:40:25] where does one end and another begin?
2016-09-05 21:58:39	causative	[20:40:26] not necessarily, suppose that it sees a big pile of sticks left as trash on the side of the road
2016-09-05 21:58:39	Asher	[20:40:29] when does one overlap another?
2016-09-05 21:58:39	causative	[20:40:37] it would be a good idea to stop and pick them up as bonfire fuel
2016-09-05 21:58:39	ipoloskov	[20:40:47] Without episodes dividing up the world around you, you can't even have relevant  ATTENTIONAL AWARENESS, beceuase if you don't know which "episode" you are in you, you dont' know which objkects in your environment can be safely ignored.
2016-09-05 21:58:39	Asher	[20:41:07] attentional awareness is the key
2016-09-05 21:58:39	Asher	[20:41:12] episodes are divided by way of attentional awareness
2016-09-05 21:58:39	Asher	[20:41:16] they do not precede attention
2016-09-05 21:58:39	precognist	[20:41:30] weighted
2016-09-05 21:58:39	Asher	[20:41:32] so the structures of relation that organize attentional awareness cause the emergence of particular episodic awareness
2016-09-05 21:58:39	ipoloskov	[20:41:34] So when I go for a walk, I ignore the 1.7 billion leaves hanging off the trees. But when I'm picking berries,, I must really look at leaves to see if they are berries are not.  
2016-09-05 21:58:40	causative	[20:42:26] you don't totally ignore the leaves
2016-09-05 21:58:40	causative	[20:42:33] you do see them
2016-09-05 21:58:40	precognist	[20:42:33] the attention is weighted depending on input and state.
2016-09-05 21:58:40	ipoloskov	[20:42:34] It is logistically impossible that we should expect any AGI agent to  digest EVERY SINGLE DETAILS of its current environment.  So pragmatically speaking, the AGI must have attenitonal awareness. And you cannot obtain such a thing unless the agent reasons about spacetime and material objects in terms of episodes.
2016-09-05 21:58:40	Asher	[20:43:26] causative introduces an interesting point here - you may not notice that the leaves are there, but you do still perceive them
2016-09-05 21:58:40	ipoloskov	[20:43:33] "What should I be payign attneiont to now?"  Every single leaf on that tree over there?  No.   My goal is navigate traffic, thus I uignore that tree and all its leaves.   
2016-09-05 21:58:40	Asher	[20:43:44] this is important imo bc you can go back and remember the leaves you perceived but didn't notice
2016-09-05 21:58:40	causative	[20:43:58] if the tree starts to fall across the road you definitely react, so you are in fact paying peripheral attention to the tree and its leaves
2016-09-05 21:58:40	Asher	[20:44:33] if the tree falls, the event is the break in the consistency regarded as non-attentional (thereby coming to attention)
2016-09-05 21:58:40	ipoloskov	[20:44:57] Again -- Deep Learnign algorithms. They don't address any of this.
2016-09-05 21:58:40	precognist	[20:44:57] that's paying attention to motion in proximity, not trees.
2016-09-05 21:58:40	Asher	[20:44:57] (meaning also that even the non-attentional requires attention of some sort, but of a different order/magnitude)
2016-09-05 21:58:40	causative	[20:45:06] motion of trees in proximity
2016-09-05 21:58:40	precognist	[20:45:18] over motion of object?
2016-09-05 21:58:40	Asher	[20:45:18] whatever we call the awareness, it comes down to motion
2016-09-05 21:58:40	ipoloskov	[20:45:21] To some degree POMPDs don't address this either.    This is the reason why AI agenst and robots to this day all require a controlled environment of a nice neat lab
2016-09-05 21:58:40	Asher	[20:45:26] trees are secondary to their motion
2016-09-05 21:58:40	Asher	[20:45:36] we sense the motion first then second we turn it into a tree
2016-09-05 21:58:40	precognist	[20:46:15] bingo. and depending on reaction time constraints, we might not recognize it at all.\
2016-09-05 21:58:40	precognist	[20:46:39] "what was that?", after deflection.
2016-09-05 21:58:40	causative	[20:47:36] you can't pay attention to cars and the road and nearby moving objects, without first paying attention to everything and then categorizing it into "driving relevant" or "non-driving-relevant"
2016-09-05 21:58:40	causative	[20:48:28] if the area of your visual field occupied by a tree was instead occupied by a truck - you would behave differently.  So you are paying attention to the part of your visual field occupied by the tree.
2016-09-05 21:58:40	causative	[20:48:41] not much attention, but some
2016-09-05 21:58:40	Malvolio	[20:49:14] let's replace trees with billboards
2016-09-05 21:58:40	Malvolio	[20:49:36] (they can fall over too)
2016-09-05 21:58:40	Asher	[20:49:38] causative - by "everything" you mean, bits of sensation right? no object awareness?
2016-09-05 21:58:40	causative	[20:49:54] well, it's an open question just how much awareness there is of the tree
2016-09-05 21:58:40	Asher	[20:49:54] *not
2016-09-05 21:58:40	causative	[20:50:10] we may be doing more subconscious processing than it seems
2016-09-05 21:58:40	Asher	[20:50:11] i just mean, you aren't asserting that we already have awareness of trees, road, other objects, etc
2016-09-05 21:58:40	Asher	[20:50:21] at the instant of initial perception
2016-09-05 21:58:40	causative	[20:50:24] how do you know the tree isn't a truck?
2016-09-05 21:58:40	Asher	[20:50:33] right
2016-09-05 21:58:40	causative	[20:50:33] to know the tree isn't a truck, you have to identify it as a tree
2016-09-05 21:58:40	causative	[20:50:39] so that's object awareness
2016-09-05 21:58:40	precognist	[20:50:45] classification of features
2016-09-05 21:58:40	Asher	[20:50:46] yeah but that is after initial perception of sensation
2016-09-05 21:58:40	Asher	[20:50:57] first a sensation has to be perceived for it to be in question
2016-09-05 21:58:40	Malvolio	[20:51:02] it happens automatically
2016-09-05 21:58:40	Asher	[20:51:18] automatically meaning by way of a series of relays
2016-09-05 21:58:40	Asher	[20:51:30] there are likely many levels to this "automatically"
2016-09-05 21:58:40	Asher	[20:51:35] some more over-determined than others
2016-09-05 21:58:40	Malvolio	[20:53:56] and the difference between driving in an unfamiliar place vs known place
2016-09-05 21:58:40	causative	[20:53:57] actually, a lot of vision happens extremely fast and subconsciously - I recall reading about a program the army was doing, to rapidly flash images from security cameras in front of soldiers, faster than they can consciously register, and monitor their brains to determine if something relevant is in the image
2016-09-05 21:58:40	Malvolio	[20:54:31] (peripheral vision is also faster than directly looking at stuff)
2016-09-05 21:58:40	tomzx	[21:03:10] causative: there's also been studies on brain-damaged subjects that shows that even when they can consciously perceive part of their field of view, they can still "sense" it unconsciously
2016-09-05 21:58:40	sal77	[21:05:41] for sure, there are many layers of filters of increasing complexity in humans at least
2016-09-05 21:58:40	sal77	[21:06:44] most basic example, we're capable of sensing single photons by way of the retina, but the signal never makes it to the brain
2016-09-05 21:58:40	precognist	[21:20:11] I'm going to have to come in here more often. That was a pretty nice conversation.
2016-09-05 21:58:40	tomzx	[21:35:54] sal77: depends what you mean by "never makes it to the brain"
2016-09-05 21:58:40	tomzx	[21:36:23] if we argue that being integrated as part of a complete signal, then it does make it to the brain
2016-09-05 21:58:40	tomzx	[21:36:44] that being integrated as part of a complete signal counts*
2016-09-05 21:58:40	sal77	[21:39:10] good point
2016-09-05 21:58:40	sal77	[21:39:52] its probably still unknown where the cutoff is
2016-09-05 21:58:40	sal77	[21:40:17] is should have said something along the liens of conscious brain too
2016-09-05 21:58:40	sal77	[21:40:38] a counter example being toads which do respond to single photons
2016-09-05 21:58:40	sal77	[21:41:20] but they lack much of the mammalian and esp primate fore brain
2016-09-05 21:58:40	tomzx	[21:42:14] well, we certainly know that a lot of information is integrated very close to the retina
2016-09-05 21:58:40	tomzx	[21:42:30] as the number of cells in the retina is much higher than in the optic fiber
2016-09-05 21:58:40	sal77	[21:42:44] in the end, for humans, it takes something like dozens of photons in packets often enough before consious responses can be made
2016-09-05 21:58:40	sal77	[21:43:11] so it appears that some signal processing is happening
2016-09-05 21:58:40	tomzx	[21:44:09] well, that's all we are, signal processors :)
2016-09-05 21:58:40	sal77	[21:44:22] true
2016-09-05 21:58:40	Asher	[21:45:52] but that statement tells you nothing :P
2016-09-05 21:58:40	tomzx	[21:46:56] well, from having read a bunch of neuroscience and ML lately, it seems the issue appears to be with the architecture
2016-09-05 21:58:40	tomzx	[21:47:32] we appear to have a good comprehension of how things are wired together for some systems, less for others
2016-09-05 21:58:40	tomzx	[21:47:56] it seems vision is getting the most love out of all senses
2016-09-05 21:58:40	causative	[21:48:24] imo the wiring diagram is the wrong focus because neurons are capable of creating their own wiring
2016-09-05 21:58:40	Asher	[21:48:24] which issue
2016-09-05 21:58:40	causative	[21:48:44] the question is, how do you make a neuron that has properties which will result in desirable wiring, just by putting a lot of them together and running a signal through them?
2016-09-05 21:58:40	tomzx	[21:49:15] causative: yes and no, yes we create new links, but it appears there is some kind of overall structure
2016-09-05 21:58:40	Asher	[21:49:26] you add glial cells and hormones among other things :P
2016-09-05 21:58:40	tomzx	[21:50:12] causative: I think we may understand how that works by looking at how the rest of the body develops
2016-09-05 21:58:40	tomzx	[21:50:37] how come, from dna alone, "most of us" end up with two arms, two legs, 1 head, 1 heart, etc.
2016-09-05 21:58:40	causative	[21:51:27] the distribution of activity within the brain for high level tasks, as measured by fMRI, varies quite a bit from person to person
2016-09-05 21:58:40	tomzx	[21:51:31] coupled with the fact that we also appear to have dedicated processing regions generally located at the same place for most individuals
2016-09-05 21:58:40	Asher	[21:51:38] "the stripes are easy but what about the horse part?"
2016-09-05 21:58:40	causative	[21:51:47] but we don't have a lot of people with two hearts or a heart in their stomach
2016-09-05 21:58:40	tomzx	[21:52:16] most of them end up with "defective" dna
2016-09-05 21:58:40	causative	[21:52:20] to me this says that there is some process that results in a functioning brain, that is based on varying environmental inputs, rather than being set in stone
2016-09-05 21:58:40	tomzx	[21:52:31] like a program that just quite works, but will segfault
2016-09-05 21:58:40	tomzx	[21:52:57] causative: depends what you mean by set in stone
2016-09-05 21:58:40	tomzx	[21:53:03] I'd argue it's in the dna
2016-09-05 21:58:40	causative	[21:53:17] yes, that's what I mean by set in stone
2016-09-05 21:58:40	tomzx	[21:53:28] there might be some individual which have dna that is quite different yet produce the same results
2016-09-05 21:58:40	causative	[21:53:56] unlikely
2016-09-05 21:58:40	causative	[21:54:19] because people don't have the same degree of variation for other organs in the body
2016-09-05 21:58:40	causative	[21:54:56] not nearly as much variation in organ placement as there is in the location of brain activity for a given task
2016-09-05 21:58:40	tomzx	[21:55:35] causative: maybe because those organ are more ancestral than the brain cells, which aren't fully stable yet?
2016-09-05 21:58:40	tomzx	[21:55:57] yet the brain has found its place within the body
2016-09-05 21:58:40	Asher	[21:56:07] genotypical vs phenotypical memory
2016-09-05 21:58:40	tomzx	[21:56:07] I'm not sure I agree with your comparison
2016-09-05 21:58:40	Asher	[21:56:14] the brain (and other organs) are genotypically structured
2016-09-05 21:58:40	Asher	[21:56:27] but the interior structure of hte brain (its plasticity) is phenotypically structured
2016-09-05 21:58:40	causative	[21:57:21] it may be that some element of brain structure is important for intelligence, and is hard coded by DNA
2016-09-05 21:58:40	dknitrox	[21:57:40] HI :D
2016-09-05 21:58:40	sal77	[21:57:41] yes, i think so
2016-09-05 21:58:40	sal77	[21:57:55] hi dknitrox
2016-09-05 21:58:40	causative	[21:57:57] but I think the bulk of the brain is the result of neuroplasticity
2016-09-05 21:58:40	Asher	[21:58:13] i would say the brain itself, as the function of neuroplasticity, is what is hardcoded by dna
2016-09-05 21:58:40	***	Playback Complete.
2016-09-05 21:58:40	causative	and the brain has hard coded structures that aren't necessary, or that are helpful but not required
2016-09-05 21:58:54	Asher	but what we idiomatically refer to as "mind" can be addressed formally as the topological structure of the particular neuroplasticity of a particular brain
2016-09-05 21:58:55	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-05 22:00:19	<--	kaiserk (~kaiserk@2a02:810d:d40:2904:515f:2045:d46:96e4) has quit (Ping timeout: 260 seconds)
2016-09-05 22:01:12	causative	consider how amazingly resilient the brain is to damage
2016-09-05 22:01:36	causative	this resilience is because the individual neurons can detect a problem and change their behavior and connections in response
2016-09-05 22:02:50	Asher	that's part of it
2016-09-05 22:03:02	Asher	it's also because there is already before any problem multiple interconnectivity
2016-09-05 22:03:12	Asher	and because when there is a problem, glial networks can fill in
2016-09-05 22:04:58	<--	segmond (~segmond@2601:40b:8404:fd10:95e6:fa3d:461a:bbd0) has quit (Ping timeout: 255 seconds)
2016-09-05 22:06:37	tomzx	what if
2016-09-05 22:06:45	tomzx	the whole thing never changed (or very little)
2016-09-05 22:06:53	Asher	"the whole thing"?
2016-09-05 22:06:55	tomzx	and it was just responding to loss of parts of its
2016-09-05 22:06:59	tomzx	the brain
2016-09-05 22:07:03	<--	dknitrox (~chatzilla@179.7.212.185) has quit (Ping timeout: 240 seconds)
2016-09-05 22:07:04	tomzx	or particularly the cortex
2016-09-05 22:07:10	Asher	never changed = never forgets?
2016-09-05 22:07:14	Asher	or never changes at all
2016-09-05 22:07:28	tomzx	changes in the sense of modifies its connections with other neurons
2016-09-05 22:07:43	tomzx	I guess you'd rather not discuss this hypothetical case?
2016-09-05 22:07:49	causative	but the brain constantly changes its connections over a person's life
2016-09-05 22:07:50	Asher	just trying to understand how it is defined
2016-09-05 22:07:54	causative	specifically it loses connections
2016-09-05 22:07:58	causative	in vast quantities
2016-09-05 22:08:22	Asher	from childhood to adulthood vast quantities of connections are lost
2016-09-05 22:08:31	tomzx	I'm simply trying to suggest that the bulk is already there, and that connections are minor in the whole picture
2016-09-05 22:08:33	Asher	i don't believe that an average adult is regularly loses vast quantities of connections tho
2016-09-05 22:08:40	Asher	*losing
2016-09-05 22:08:48	causative	they continue to over the course of their lives
2016-09-05 22:08:50	Asher	the bulk of what?
2016-09-05 22:09:01	tomzx	the bulk of the structure/connections within the brain
2016-09-05 22:09:12	Asher	causative - there is no question that connections are lost and gained, but not at the scale from childhood to adulthood
2016-09-05 22:09:25	Asher	a baby has ~10million neurons in the optic nerve, an adult has ~1.5-2 milion
2016-09-05 22:09:40	tomzx	such that when a traumatic event occurs, such as the loss of a part of the cortex, the information simply "has" to be dispatched to the remaining cortex
2016-09-05 22:09:42	causative	learning apparently primarily happens by the loss of connections
2016-09-05 22:09:58	Asher	any links on that?
2016-09-05 22:10:58	causative	https://en.wikipedia.org/wiki/Synaptic_pruning
2016-09-05 22:12:30	Asher	heh
2016-09-05 22:13:04	Asher	that's what iw as talking about above
2016-09-05 22:13:14	Asher	for an average adult, the number of neurons is not regularly changing that much
2016-09-05 22:13:21	Asher	but the routings are changing regularly
2016-09-05 22:13:33	causative	yes, however I recall reading that the number of synapses does continue to decrease in adulthood
2016-09-05 22:13:40	causative	although at a lesser rate
2016-09-05 22:13:43	Asher	it doesn't freeze or anything
2016-09-05 22:13:56	sal77	and neuroplasticity can drop off considerably
2016-09-05 22:14:03	Asher	but it becomes statistically insignificant is my understnading
2016-09-05 22:14:27	Asher	i think the important point is that neuroplasticity has two dimensions
2016-09-05 22:14:45	Asher	1. synaptic interconnectivity 2. routing interconnectivity
2016-09-05 22:15:05	causative	http://scholarcommons.usf.edu/cgi/viewcontent.cgi?article=4812&context=ujmm
2016-09-05 22:15:34	causative	here they track density of neurons and synapses as a function of age, it decreases steadily
2016-09-05 22:17:55	Asher	i have no argument with that, my only point was that it happens at a far greater rate (orders of magnitude) from childhood to adulthood
2016-09-05 22:18:45	causative	ok
2016-09-05 22:19:03	tomzx	causative: it's kinda hard to dissociate synaptic degenerescence due to aging and due to learning
2016-09-05 22:19:11	tomzx	if that was the point you were trying to make
2016-09-05 22:20:09	causative	well, clearly the purpose of the large loss of neurons in childhood is learning
2016-09-05 22:20:27	Asher	i don't disagree with the conclusion, but the way of stating it is a bit obfuscating
2016-09-05 22:20:46	Asher	i would say: something about massive # of neurons 1. is important for learning 2. interferes with development of complexity
2016-09-05 22:20:58	causative	so it probably continues to serve a similar role in adulthood
2016-09-05 22:21:32	Asher	i think an importnat question: what is the advantage of massively decreasing # of neurons from childhood to adulthood?
2016-09-05 22:21:47	tomzx	Asher: prevent information overload?
2016-09-05 22:21:50	Asher	another important question: what is the disadvantage of keeping massive interconnectivity?
2016-09-05 22:21:56	causative	the loss of neurons is a change in neuron weights - and thus encodes information
2016-09-05 22:22:07	causative	imagine a punch card, now you punch holes in it, the loss of paper encodes information
2016-09-05 22:22:09	tomzx	Asher: might explain children overactive behavior :)
2016-09-05 22:22:39	tomzx	causative: yeah, basic NN stuff
2016-09-05 22:23:26	sal77	even if no connections were lost, it may pose a problem for efficiency
2016-09-05 22:25:07	sal77	from an evolutionary standpoint, its could be argued increasing efficiency is important not just for glucose consumption but for making quick decisions when in danger
2016-09-05 22:26:43	<--	User__ (~jshjsh@172.56.39.225) has quit (Ping timeout: 265 seconds)
2016-09-05 22:26:47	--	irc: disconnected from server
2016-09-06 11:21:04	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-06 11:21:04	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-06 11:21:04	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-06 11:21:04	--	Channel #ai: 74 nicks (1 op, 0 voices, 73 normals)
2016-09-06 11:21:04	***	Buffer Playback...
2016-09-06 11:21:04	Th3_Prince	[15:14:17] I don't get it asher... Why on earth would physics be the right path to ai?
2016-09-06 11:21:04	Asher	[15:17:50] who said it was?
2016-09-06 11:21:04	Th3_Prince	[15:18:03] idk everywhere I check
2016-09-06 11:21:04	Th3_Prince	[15:18:13] Can you help me really quick
2016-09-06 11:21:04	Th3_Prince	[15:18:17] this will be the last time I ask
2016-09-06 11:21:04	Th3_Prince	[15:20:27] Asher: 
2016-09-06 11:21:04	Asher	[15:25:03] with what?
2016-09-06 11:21:04	Asher	[15:25:17] i don't mind you asking questions so long as it's not the same question
2016-09-06 11:21:04	Asher	[15:25:20] over and over
2016-09-06 11:21:04	Th3_Prince	[15:26:08] ok
2016-09-06 11:21:04	Th3_Prince	[15:26:16] so my interests are in many fields
2016-09-06 11:21:04	Th3_Prince	[15:26:23] but im really interested in systems 
2016-09-06 11:21:04	Th3_Prince	[15:26:48] I also have an interest in the science of the mind... stanford has a course on it called symbolic systems
2016-09-06 11:21:04	Th3_Prince	[15:26:57] epistemology and all that
2016-09-06 11:21:04	Th3_Prince	[15:27:08] I particularly like AI
2016-09-06 11:21:04	Th3_Prince	[15:27:31] and genetic programming/algorithms, evolutionary computation, and simulation seem interesting 
2016-09-06 11:21:04	Th3_Prince	[15:28:30] also interested in systems science and complex systems pattern recognition  collective behavior and all that
2016-09-06 11:21:04	Th3_Prince	[15:28:52] I'm trying to find a field that covers most of that and or allows me to get into those fields.
2016-09-06 11:21:04	Asher	[15:29:16] pick a field or two that overlap with what you want to study
2016-09-06 11:21:04	Asher	[15:29:22] this is most easily done by selecting classes based o nwhat ou want to study
2016-09-06 11:21:04	Th3_Prince	[15:29:22] The subject mentioned A LOT is physics.. I used to like physics but now find it boring but I only took ap physics in highschool.
2016-09-06 11:21:04	Th3_Prince	[15:29:26] Which fields do you think overlap?
2016-09-06 11:21:04	Asher	[15:29:35] look at what classes are offered
2016-09-06 11:21:04	Th3_Prince	[15:29:35] Do you think computer science covers most of those?
2016-09-06 11:21:04	Asher	[15:29:52] the more important thing tho is that you read and research on your own in other fields
2016-09-06 11:21:04	Th3_Prince	[15:29:57] yes
2016-09-06 11:21:04	Th3_Prince	[15:30:01] I will research on my own
2016-09-06 11:21:04	Asher	[15:30:01] and that you have your own projects to apply the konwledge you assemble
2016-09-06 11:21:04	Th3_Prince	[15:30:08] Yeah
2016-09-06 11:21:04	Th3_Prince	[15:30:18] I'm going to learn about everything that interests me on my own
2016-09-06 11:21:04	Th3_Prince	[15:30:30] I just want to find a major that trains me to think and is broad
2016-09-06 11:21:04	Th3_Prince	[15:30:38] So physics, math, philosophy, cs
2016-09-06 11:21:04	Th3_Prince	[15:30:41] I really like cs
2016-09-06 11:21:04	Th3_Prince	[15:30:47] and I really like philosophy and math
2016-09-06 11:21:04	Th3_Prince	[15:31:01] not so much physics... but physics is seen as the go-to major to for studying systems and AI
2016-09-06 11:21:04	Th3_Prince	[15:32:17] plus this is for undergrad
2016-09-06 11:21:04	Th3_Prince	[15:32:28] which undergrad degrees set you up with work like this in grad school
2016-09-06 11:21:04	Th3_Prince	[15:38:56] ?
2016-09-06 11:21:04	Asher	[15:40:25] i worked primarily out of comparative literature
2016-09-06 11:21:04	Asher	[15:40:33] bc it is a field that let me define my work as intersections of other fields
2016-09-06 11:21:04	Asher	[15:40:42] and it is a field where useful and complex philosophy is studied
2016-09-06 11:21:04	Asher	[15:40:47] including semiotics and psychoanalysis
2016-09-06 11:21:04	Asher	[15:40:51] which imo are the foundation of AGI
2016-09-06 11:21:04	Th3_Prince	[15:47:38] I'm not interested in comparative literature but I appreciate your suggestion 
2016-09-06 11:21:04	Asher	[15:49:42] *shrug*
2016-09-06 11:21:04	Asher	[15:49:47] you were the one who asked
2016-09-06 11:21:04	Asher	[15:50:05] it's where philosophy has taken place since ww 2
2016-09-06 11:21:04	Asher	[15:51:30] my suggestion to anyone who asks about majors is: pick one in hard sciences and one in humanities
2016-09-06 11:21:04	Th3_Prince	[15:57:20] why not just pick 1
2016-09-06 11:21:04	dknitrox	[03:56:13] hi all i'm newbie
2016-09-06 11:21:04	Lyote	[04:54:19] Aren't we all?
2016-09-06 11:21:04	Drew_	[12:10:34] very much so
2016-09-06 11:21:04	ipoloskov	[14:02:42] http://nautil.us/issue/40/learning/is-artificial-intelligence-permanently-inscrutable
2016-09-06 11:21:04	YYZ	[14:04:34] Asher: it seems nearly impossible to give solid advice to folks about what they should study, unless you know the person well.
2016-09-06 11:21:04	YYZ	[14:05:57] what if some Kid Genius already knows advanced math at age 16 from self study, should he study math?
2016-09-06 11:21:04	Asher	[14:06:42] said kid genius probably isn't asking
2016-09-06 11:21:04	YYZ	[14:06:52] it seems like people should study what they can't naturally excel at
2016-09-06 11:21:04	YYZ	[14:06:55] lol, good point
2016-09-06 11:21:04	YYZ	[14:07:20] if you're good at math sure take math, but you'll be good at it anyways so why not study something "hard" ?
2016-09-06 11:21:04	YYZ	[14:08:45] Asher don't you think someone would likely learn more than they expected if they learned a 2nd or 3rd language ?
2016-09-06 11:21:04	YYZ	[14:09:18] even if it was like Latin or ancient Greek, just the study would really pry open their brain and help them to absorb all sorts of new ideas
2016-09-06 11:21:04	YYZ	[14:10:38] but let's suppose studying a language is "hard", but another class the student can get an easy A, so he takes the "easy" class and doesn't really learn new things
2016-09-06 11:21:04	YYZ	[14:11:50] or the opposite, he's from another country and just takes that language class to get an easy A, instead of a "hard" class where he would learn more by getting a C
2016-09-06 11:21:04	Asher	[14:18:07] yes
2016-09-06 11:21:04	Asher	[14:18:16] it's not about easy or hard
2016-09-06 11:21:04	Asher	[14:18:25] it's about being exposed to distinct domains of knowledge
2016-09-06 11:21:04	YYZ	[14:25:50] right, "easy" being familiar and/or already known is what I mean
2016-09-06 11:21:04	YYZ	[14:26:39] if you study an unfamiliar domain, especially where you can integrate it with your strengths, it could be more beneficial
2016-09-06 11:21:04	YYZ	[14:27:26] I guess the only general advice then is to take a technical & humanities courses
2016-09-06 11:21:04	Asher	[14:28:23] and projects
2016-09-06 11:21:04	YYZ	[14:28:45] right
2016-09-06 11:21:04	YYZ	[14:29:37] I saw an interview with an Art professor, a semi-famous artist named Chris Burden, and when he was in Art school he realized that all the materials laying around the art class were essentially his to do whatever he wanted, since no one else was really doing any projects
2016-09-06 11:21:04	YYZ	[14:30:12] so a big pile of plywood was all his, no one else had any idea what sort of project to use it for
2016-09-06 11:21:04	YYZ	[14:31:17] so he made these massive installation sort of projects that were very popular around the campus, even if  no one knew who made them 
2016-09-06 11:21:04	YYZ	[14:31:51] out of plywood or sheets of plastic, stuff lesser artists didn't think of
2016-09-06 11:21:04	YYZ	[14:33:10] Asher: I suspect that many students only think about grades and getting a diploma, and not about using the available resources and doing projects and such
2016-09-06 11:21:04	Asher	[14:33:54] yea
2016-09-06 11:21:04	YYZ	[14:34:37] anyone could have set up posts and attached sheets of plastic to it, to make an outdoor hallway, the stuff was just laying around, it's not like Chris Burden went and bought the materials
2016-09-06 11:21:04	YYZ	[14:35:07] that's an example of one of the things he built
2016-09-06 11:21:04	YYZ	[14:36:44] or a simple sculpture made of plywood, one that looks simple but looks different from various perspectives, as if you can't tell if it has 4, 5, or 9 sides
2016-09-06 11:21:04	precognist	[15:02:38] Hello.
2016-09-06 11:21:04	appa-nerd	[15:10:35] YYZ: love the article, few AI articles are worth reading.  The circuit program sounds like a genetic program but it sounds like it just needs better simulation or maybe a physical output to give it better training set
2016-09-06 11:21:04	dknitrox	[15:53:08] hi
2016-09-06 11:21:04	Lyote	[17:11:38] appa-nerd: Are you talking about ipoloskov's article?
2016-09-06 11:21:04	appa-nerd	[17:12:32] Oh yeah, they are both the same green on my viewer, thought it was yy
2016-09-06 11:21:04	Lyote	[18:05:33] Are you using weechat? They're both green on my end too. :)
2016-09-06 11:21:04	precognist	[20:03:11] Hello.
2016-09-06 11:21:04	precognist	[20:04:02] I'm working on a simple FSM and was wondering if anyone could give me advice on transitions?
2016-09-06 11:21:04	precognist	[20:06:55] Where should the transitions be invoked? Should the state control the transition? or the manager?
2016-09-06 11:21:04	Asher	[20:09:18] between the spaghetti monster's tentacles
2016-09-06 11:21:04	ipoloskov	[20:15:07] http://nautil.us/issue/40/learning/is-artificial-intelligence-permanently-inscrutable
2016-09-06 11:21:04	ipoloskov	[20:15:18] I think this article makes one thing clear... and I'm reading 'between the lines' as it were
2016-09-06 11:21:04	ipoloskov	[20:15:47] We are at a stage where  Deep Learning is so pervasive in AI research that  the word "AI" is becoming synonymous with  Deep Networks
2016-09-06 11:21:04	Asher	[20:16:18] this is true
2016-09-06 11:21:04	ipoloskov	[20:16:53] POMDPs and navigation algorithms like  SLAM are being almost completely ignored by science media.
2016-09-06 11:21:04	Asher	[20:17:55] i think they are seen as subsumed
2016-09-06 11:21:04	Asher	[20:18:18] but any attempt to model things like language has been surrendered entirely
2016-09-06 11:21:04	ipoloskov	[20:18:42] Asher   Very rarely too rarely I would say,  a researcher will come out and say  Deep Networks are good for  VISION   (and maybe some other kinds of recognition of objects like audio phonemes)
2016-09-06 11:21:04	ipoloskov	[20:18:55] But outside of that  a network cannot plan.  It cannot make complex decisions. It cannot navigate space
2016-09-06 11:21:04	Asher	[20:19:09] deep learning is pretty effective if you can define an open FSM
2016-09-06 11:21:04	Asher	[20:19:28] like Go
2016-09-06 11:21:04	Asher	[20:19:31] Go is just a giant state machine
2016-09-06 11:21:04	Asher	[20:19:40] it is always in a definite finite state
2016-09-06 11:21:04	ipoloskov	[20:19:40] The world we occupy has space, time,  and rules of physics.
2016-09-06 11:21:04	Asher	[20:19:51] maybe
2016-09-06 11:21:04	Asher	[20:19:56] those things emerge and are undeniable no doubt
2016-09-06 11:21:04	Asher	[20:20:00] but how they are situated is less clear
2016-09-06 11:21:04	ipoloskov	[20:20:52] Asher  Let me be more clear.  The article's title was "Is Artificial Intelligence Permanently Inscrutable?"
2016-09-06 11:21:04	ipoloskov	[20:21:03] When it should have been "Are deep networks permanently inscrutable?"
2016-09-06 11:21:04	Asher	[20:21:32] yeah
2016-09-06 11:21:04	Asher	[20:21:56] so i work on AGI by way of modeling natural language as the symbol grounding problem
2016-09-06 11:21:04	Asher	[20:22:08] i've created a new database architecture based on Turing's work with ordinal logics
2016-09-06 11:21:04	Asher	[20:22:13] that functions to implement language plasticity
2016-09-06 11:21:04	Asher	[20:22:30] it's entirely different than all the ML work bc it is 100% formal in implementation
2016-09-06 11:21:04	Asher	[20:22:36] but its treatment is not formal - its treatment is plasticity
2016-09-06 11:21:04	ipoloskov	[20:22:40] Right.    It's like some researchers are so sick-and-tired of the abuse of the phrase "Artificial INtelligence" that they had to make a new word "AGI"  to refer to their work.  It's sad
2016-09-06 11:21:04	Asher	[20:22:54] yu[p
2016-09-06 11:21:04	ipoloskov	[20:23:25] And so in the context of AGI,  deep networks only really play a role in perception (90% of the time its going to vision)
2016-09-06 11:21:04	Asher	[20:23:47] imo the alphago victory is an astounding number-crunching feat and an astounding proof of the role of numbers in intelligent determiantions
2016-09-06 11:21:04	Asher	[20:23:51] but it has little to do with AGI
2016-09-06 11:21:04	Asher	[20:23:58] perception => sensation
2016-09-06 11:21:04	ipoloskov	[20:24:16] absolutely.   In my humble opinion,  POMDPs are way closer to AGI than a deep network is
2016-09-06 11:21:04	Asher	[20:24:17] i would say sensation rather than perception bc it isn't just about perceiving it's about statistical mappings of informal expressions to formal counterparts
2016-09-06 11:21:04	Asher	[20:24:59] my work models language as the intervals between points of determination in its circuit of exchange (the circuit by which it becomes an express that expresses a meaning)
2016-09-06 11:21:04	Asher	[20:25:16] the structure of a basic unit of information: https://www.dropbox.com/s/9n2910vbcdikuac/Term.png?dl=0
2016-09-06 11:21:04	Asher	[20:25:25] and the structure of the unit of information in relation to itself (movement across time): https://www.dropbox.com/s/6ihf8v4l235zdyh/Sign.png?dl=0
2016-09-06 11:21:04	Asher	[20:25:40] *becomes an expression
2016-09-06 11:21:04	precognist	[20:26:35]  philosophy, isn't it?
2016-09-06 11:21:04	Asher	[20:27:08] it's constructed from an intersection of Turing, psychoanalysis and semiotics
2016-09-06 11:21:04	Asher	[20:27:20] but it's mathematical not conceptual
2016-09-06 11:21:04	Asher	[20:27:56] it's a diagram of the nodes where events occur and the edges those events share
2016-09-06 11:21:04	ipoloskov	[20:28:04] an AGI is goingto _require_ a 'theatre of perceptual awareness'   wherein it actively contemplates a situation vis-a-vis a longterm memory compared against the current situation based on likeness
2016-09-06 11:21:04	Asher	[20:28:07] and it works like a circuit, with directional flow
2016-09-06 11:21:04	Asher	[20:28:37] ipoloskov - exactly. i've found the comparison useful: a robot with cameras for eyes vs a robot that uses cameras to gather data to construct a 3d model simulating its environment
2016-09-06 11:21:04	Asher	[20:29:09] our eyes are not cameras
2016-09-06 11:21:04	Asher	[20:29:12] they are data ports
2016-09-06 11:21:04	Asher	[20:29:28] vision comes from the brain not the eyes
2016-09-06 11:21:04	Asher	[20:29:35] the eyes just give the brain the data to make vision relevant
2016-09-06 11:21:04	Asher	[20:29:59] (but then we can start trying to define where the eyes and the brain are, and we find that it's hard to divide the optic nerve)
2016-09-06 11:21:04	ipoloskov	[20:31:36] The deep networks are feed-forward.     When a human sees an image of a panda it activates a cascade of past memories of panda interactions,  including episodes seen on TV.. trips to a zoo when you were 8 years old.   Maybe even a memeory of Kung Fu panda the cartoon if you saw it recently and so on
2016-09-06 11:21:04	causative	[20:31:40] <ipoloskov> But outside of that  a network cannot plan.  It cannot make complex decisions. It cannot navigate space
2016-09-06 11:21:04	causative	[20:32:13] DeepMind made a deep neural network that plays Atari - requiring planning and spatial navigation
2016-09-06 11:21:04	ipoloskov	[20:32:48] An AGI will have episodic memory.  It will not just blindly associate featres through layered "filter".    It will compare and contrast a database of temporal experiences.   
2016-09-06 11:21:04	Asher	[20:32:52] ipoloskov - you should take a look at Turing's paper on ordinal logics
2016-09-06 11:21:04	Asher	[20:33:04] it's all about how to construct a structure for this anticipation
2016-09-06 11:21:04	Asher	[20:33:22] the "active cascade" as you put it
2016-09-06 11:21:04	ipoloskov	[20:33:45] Asher   There was a guy in here a few weeks ago who was mature enough to admit this (or maybe it was in #machinelearning ).  What these fancy networks that can classify "data sets"  with 98% accuracy and so on  ... these algorithms are really just fancy hash functions.  
2016-09-06 11:21:04	ipoloskov	[20:33:53] They don't "See" objects.
2016-09-06 11:21:04	Asher	[20:34:12] absolutely - they are constructing a gigantic dictionary of mappings between informal inputs and formal identities
2016-09-06 11:21:04	ipoloskov	[20:34:19] They see 2D features.. because they were trained on 2D features
2016-09-06 11:21:04	ipoloskov	[20:35:33] Asher  This is all a human being really is.  We are layers of neuronal tissue that are able to very quickly  conjure up  entire episodes that are relevant to the current one.    
2016-09-06 11:21:04	ipoloskov	[20:35:44] There is no narrow AI agent in existence today that comes near to this.
2016-09-06 11:21:04	ipoloskov	[20:35:59] HOw do I know this?  Because everything about human language is episodic.
2016-09-06 11:21:04	ipoloskov	[20:36:05] "I went  for a walk"  
2016-09-06 11:21:04	ipoloskov	[20:36:07] episode
2016-09-06 11:21:04	ipoloskov	[20:36:16] actors taking actions in a scene. Beginning, middle, end.
2016-09-06 11:21:04	ipoloskov	[20:36:46] OUr whole human psychology is arranged around  actors taking actions in a scene. Beginning/middle/end.
2016-09-06 11:21:04	causative	[20:36:48] language is not thinking; language only expresses thinking
2016-09-06 11:21:04	ipoloskov	[20:36:53] "We vacationed in Berline"
2016-09-06 11:21:04	Asher	[20:37:02] causative confuses expression with language
2016-09-06 11:21:04	Asher	[20:37:08] (a common misunderstanding)
2016-09-06 11:21:04	causative	[20:37:16] mental language is not thinking
2016-09-06 11:21:04	Asher	[20:37:17] language is the translatability of expression
2016-09-06 11:21:04	Asher	[20:37:25] which is thinking
2016-09-06 11:21:04	ipoloskov	[20:37:29] Everythign about the human brains is meant to  digest and break down the natural world around us into digestible  "chunks"  called "episodes"
2016-09-06 11:21:04	causative	[20:37:36] if you think in words in your head, the words are just an expression of the end result of a nonverbal thinking process
2016-09-06 11:21:04	Asher	[20:38:06] the words are tags that are parts of thoughts that are activated along with their tags, thereby producing "meaning" along with the word
2016-09-06 11:21:04	causative	[20:38:06] they help as a mnemonic and to keep you focused but they are not, themselves, thought
2016-09-06 11:21:04	ipoloskov	[20:38:07] How do I know this?   Because I'm jaded by some ideology? No.  I know this because this is exactly how we talk about the world around us --->  "The boy kicked the ball."
2016-09-06 11:21:04	Asher	[20:38:32] words work bc they function as activations for memories
2016-09-06 11:21:04	ipoloskov	[20:38:42] There is a very good pragmatic reason to divide the complex multi-dimensional caucophony of the outside world into digestible "episodes"
2016-09-06 11:21:04	Asher	[20:38:51] just like when you see the same object a second time and it reactivates the experience, words do the same thing
2016-09-06 11:21:04	Asher	[20:38:56] words activate the concept they are
2016-09-06 11:21:04	ipoloskov	[20:39:55] Asher   HEre is where it has to come down to.   An AGI is tasked when getting in a car, driving to a park, and constructing a bonfire at that park.    Depending on teh EPISODE in that task in which it finds itself, it will attend to certain objects in its environment and ignore others.  So while building the bonfire, it will ignore all traffic wizzing by.  But while driving,  sticks are ignored 
2016-09-06 11:21:04	ipoloskov	[20:39:55] and cars are attended to.
2016-09-06 11:21:04	Asher	[20:40:15] the difficulty is how to divide the episode
2016-09-06 11:21:04	Asher	[20:40:25] where does one end and another begin?
2016-09-06 11:21:04	causative	[20:40:26] not necessarily, suppose that it sees a big pile of sticks left as trash on the side of the road
2016-09-06 11:21:04	Asher	[20:40:29] when does one overlap another?
2016-09-06 11:21:04	causative	[20:40:37] it would be a good idea to stop and pick them up as bonfire fuel
2016-09-06 11:21:04	ipoloskov	[20:40:47] Without episodes dividing up the world around you, you can't even have relevant  ATTENTIONAL AWARENESS, beceuase if you don't know which "episode" you are in you, you dont' know which objkects in your environment can be safely ignored.
2016-09-06 11:21:04	Asher	[20:41:07] attentional awareness is the key
2016-09-06 11:21:04	Asher	[20:41:12] episodes are divided by way of attentional awareness
2016-09-06 11:21:04	Asher	[20:41:16] they do not precede attention
2016-09-06 11:21:04	precognist	[20:41:30] weighted
2016-09-06 11:21:04	Asher	[20:41:32] so the structures of relation that organize attentional awareness cause the emergence of particular episodic awareness
2016-09-06 11:21:04	ipoloskov	[20:41:34] So when I go for a walk, I ignore the 1.7 billion leaves hanging off the trees. But when I'm picking berries,, I must really look at leaves to see if they are berries are not.  
2016-09-06 11:21:04	causative	[20:42:26] you don't totally ignore the leaves
2016-09-06 11:21:04	causative	[20:42:33] you do see them
2016-09-06 11:21:04	precognist	[20:42:33] the attention is weighted depending on input and state.
2016-09-06 11:21:04	ipoloskov	[20:42:34] It is logistically impossible that we should expect any AGI agent to  digest EVERY SINGLE DETAILS of its current environment.  So pragmatically speaking, the AGI must have attenitonal awareness. And you cannot obtain such a thing unless the agent reasons about spacetime and material objects in terms of episodes.
2016-09-06 11:21:04	Asher	[20:43:26] causative introduces an interesting point here - you may not notice that the leaves are there, but you do still perceive them
2016-09-06 11:21:04	ipoloskov	[20:43:33] "What should I be payign attneiont to now?"  Every single leaf on that tree over there?  No.   My goal is navigate traffic, thus I uignore that tree and all its leaves.   
2016-09-06 11:21:04	Asher	[20:43:44] this is important imo bc you can go back and remember the leaves you perceived but didn't notice
2016-09-06 11:21:04	causative	[20:43:58] if the tree starts to fall across the road you definitely react, so you are in fact paying peripheral attention to the tree and its leaves
2016-09-06 11:21:04	Asher	[20:44:33] if the tree falls, the event is the break in the consistency regarded as non-attentional (thereby coming to attention)
2016-09-06 11:21:04	ipoloskov	[20:44:57] Again -- Deep Learnign algorithms. They don't address any of this.
2016-09-06 11:21:04	precognist	[20:44:57] that's paying attention to motion in proximity, not trees.
2016-09-06 11:21:04	Asher	[20:44:57] (meaning also that even the non-attentional requires attention of some sort, but of a different order/magnitude)
2016-09-06 11:21:04	causative	[20:45:06] motion of trees in proximity
2016-09-06 11:21:04	precognist	[20:45:18] over motion of object?
2016-09-06 11:21:04	Asher	[20:45:18] whatever we call the awareness, it comes down to motion
2016-09-06 11:21:04	ipoloskov	[20:45:21] To some degree POMPDs don't address this either.    This is the reason why AI agenst and robots to this day all require a controlled environment of a nice neat lab
2016-09-06 11:21:04	Asher	[20:45:26] trees are secondary to their motion
2016-09-06 11:21:04	Asher	[20:45:36] we sense the motion first then second we turn it into a tree
2016-09-06 11:21:04	precognist	[20:46:15] bingo. and depending on reaction time constraints, we might not recognize it at all.\
2016-09-06 11:21:04	precognist	[20:46:39] "what was that?", after deflection.
2016-09-06 11:21:04	causative	[20:47:36] you can't pay attention to cars and the road and nearby moving objects, without first paying attention to everything and then categorizing it into "driving relevant" or "non-driving-relevant"
2016-09-06 11:21:04	causative	[20:48:28] if the area of your visual field occupied by a tree was instead occupied by a truck - you would behave differently.  So you are paying attention to the part of your visual field occupied by the tree.
2016-09-06 11:21:04	causative	[20:48:41] not much attention, but some
2016-09-06 11:21:04	Malvolio	[20:49:14] let's replace trees with billboards
2016-09-06 11:21:04	Malvolio	[20:49:36] (they can fall over too)
2016-09-06 11:21:04	Asher	[20:49:38] causative - by "everything" you mean, bits of sensation right? no object awareness?
2016-09-06 11:21:04	causative	[20:49:54] well, it's an open question just how much awareness there is of the tree
2016-09-06 11:21:04	Asher	[20:49:54] *not
2016-09-06 11:21:04	causative	[20:50:10] we may be doing more subconscious processing than it seems
2016-09-06 11:21:04	Asher	[20:50:11] i just mean, you aren't asserting that we already have awareness of trees, road, other objects, etc
2016-09-06 11:21:04	Asher	[20:50:21] at the instant of initial perception
2016-09-06 11:21:04	causative	[20:50:24] how do you know the tree isn't a truck?
2016-09-06 11:21:04	Asher	[20:50:33] right
2016-09-06 11:21:04	causative	[20:50:33] to know the tree isn't a truck, you have to identify it as a tree
2016-09-06 11:21:04	causative	[20:50:39] so that's object awareness
2016-09-06 11:21:04	precognist	[20:50:45] classification of features
2016-09-06 11:21:04	Asher	[20:50:46] yeah but that is after initial perception of sensation
2016-09-06 11:21:04	Asher	[20:50:57] first a sensation has to be perceived for it to be in question
2016-09-06 11:21:04	Malvolio	[20:51:02] it happens automatically
2016-09-06 11:21:04	Asher	[20:51:18] automatically meaning by way of a series of relays
2016-09-06 11:21:04	Asher	[20:51:30] there are likely many levels to this "automatically"
2016-09-06 11:21:04	Asher	[20:51:35] some more over-determined than others
2016-09-06 11:21:04	Malvolio	[20:53:56] and the difference between driving in an unfamiliar place vs known place
2016-09-06 11:21:04	causative	[20:53:57] actually, a lot of vision happens extremely fast and subconsciously - I recall reading about a program the army was doing, to rapidly flash images from security cameras in front of soldiers, faster than they can consciously register, and monitor their brains to determine if something relevant is in the image
2016-09-06 11:21:04	Malvolio	[20:54:31] (peripheral vision is also faster than directly looking at stuff)
2016-09-06 11:21:04	tomzx	[21:03:10] causative: there's also been studies on brain-damaged subjects that shows that even when they can consciously perceive part of their field of view, they can still "sense" it unconsciously
2016-09-06 11:21:04	sal77	[21:05:41] for sure, there are many layers of filters of increasing complexity in humans at least
2016-09-06 11:21:04	sal77	[21:06:44] most basic example, we're capable of sensing single photons by way of the retina, but the signal never makes it to the brain
2016-09-06 11:21:04	precognist	[21:20:11] I'm going to have to come in here more often. That was a pretty nice conversation.
2016-09-06 11:21:04	tomzx	[21:35:54] sal77: depends what you mean by "never makes it to the brain"
2016-09-06 11:21:04	tomzx	[21:36:23] if we argue that being integrated as part of a complete signal, then it does make it to the brain
2016-09-06 11:21:04	tomzx	[21:36:44] that being integrated as part of a complete signal counts*
2016-09-06 11:21:04	sal77	[21:39:10] good point
2016-09-06 11:21:04	sal77	[21:39:52] its probably still unknown where the cutoff is
2016-09-06 11:21:04	sal77	[21:40:17] is should have said something along the liens of conscious brain too
2016-09-06 11:21:04	sal77	[21:40:38] a counter example being toads which do respond to single photons
2016-09-06 11:21:04	sal77	[21:41:20] but they lack much of the mammalian and esp primate fore brain
2016-09-06 11:21:04	tomzx	[21:42:14] well, we certainly know that a lot of information is integrated very close to the retina
2016-09-06 11:21:04	tomzx	[21:42:30] as the number of cells in the retina is much higher than in the optic fiber
2016-09-06 11:21:04	sal77	[21:42:44] in the end, for humans, it takes something like dozens of photons in packets often enough before consious responses can be made
2016-09-06 11:21:04	sal77	[21:43:11] so it appears that some signal processing is happening
2016-09-06 11:21:04	tomzx	[21:44:09] well, that's all we are, signal processors :)
2016-09-06 11:21:04	sal77	[21:44:22] true
2016-09-06 11:21:04	Asher	[21:45:52] but that statement tells you nothing :P
2016-09-06 11:21:04	tomzx	[21:46:56] well, from having read a bunch of neuroscience and ML lately, it seems the issue appears to be with the architecture
2016-09-06 11:21:04	tomzx	[21:47:32] we appear to have a good comprehension of how things are wired together for some systems, less for others
2016-09-06 11:21:04	tomzx	[21:47:56] it seems vision is getting the most love out of all senses
2016-09-06 11:21:04	causative	[21:48:24] imo the wiring diagram is the wrong focus because neurons are capable of creating their own wiring
2016-09-06 11:21:04	Asher	[21:48:24] which issue
2016-09-06 11:21:04	causative	[21:48:44] the question is, how do you make a neuron that has properties which will result in desirable wiring, just by putting a lot of them together and running a signal through them?
2016-09-06 11:21:04	tomzx	[21:49:15] causative: yes and no, yes we create new links, but it appears there is some kind of overall structure
2016-09-06 11:21:04	Asher	[21:49:26] you add glial cells and hormones among other things :P
2016-09-06 11:21:04	tomzx	[21:50:12] causative: I think we may understand how that works by looking at how the rest of the body develops
2016-09-06 11:21:04	tomzx	[21:50:37] how come, from dna alone, "most of us" end up with two arms, two legs, 1 head, 1 heart, etc.
2016-09-06 11:21:04	causative	[21:51:27] the distribution of activity within the brain for high level tasks, as measured by fMRI, varies quite a bit from person to person
2016-09-06 11:21:04	tomzx	[21:51:31] coupled with the fact that we also appear to have dedicated processing regions generally located at the same place for most individuals
2016-09-06 11:21:04	Asher	[21:51:38] "the stripes are easy but what about the horse part?"
2016-09-06 11:21:04	causative	[21:51:47] but we don't have a lot of people with two hearts or a heart in their stomach
2016-09-06 11:21:04	tomzx	[21:52:16] most of them end up with "defective" dna
2016-09-06 11:21:04	causative	[21:52:20] to me this says that there is some process that results in a functioning brain, that is based on varying environmental inputs, rather than being set in stone
2016-09-06 11:21:04	tomzx	[21:52:31] like a program that just quite works, but will segfault
2016-09-06 11:21:04	tomzx	[21:52:57] causative: depends what you mean by set in stone
2016-09-06 11:21:04	tomzx	[21:53:03] I'd argue it's in the dna
2016-09-06 11:21:04	causative	[21:53:17] yes, that's what I mean by set in stone
2016-09-06 11:21:04	tomzx	[21:53:28] there might be some individual which have dna that is quite different yet produce the same results
2016-09-06 11:21:04	causative	[21:53:56] unlikely
2016-09-06 11:21:04	causative	[21:54:19] because people don't have the same degree of variation for other organs in the body
2016-09-06 11:21:04	causative	[21:54:56] not nearly as much variation in organ placement as there is in the location of brain activity for a given task
2016-09-06 11:21:04	tomzx	[21:55:35] causative: maybe because those organ are more ancestral than the brain cells, which aren't fully stable yet?
2016-09-06 11:21:04	tomzx	[21:55:57] yet the brain has found its place within the body
2016-09-06 11:21:04	Asher	[21:56:07] genotypical vs phenotypical memory
2016-09-06 11:21:04	tomzx	[21:56:07] I'm not sure I agree with your comparison
2016-09-06 11:21:04	Asher	[21:56:14] the brain (and other organs) are genotypically structured
2016-09-06 11:21:04	Asher	[21:56:27] but the interior structure of hte brain (its plasticity) is phenotypically structured
2016-09-06 11:21:04	causative	[21:57:21] it may be that some element of brain structure is important for intelligence, and is hard coded by DNA
2016-09-06 11:21:04	dknitrox	[21:57:40] HI :D
2016-09-06 11:21:04	sal77	[21:57:41] yes, i think so
2016-09-06 11:21:04	sal77	[21:57:55] hi dknitrox
2016-09-06 11:21:04	causative	[21:57:57] but I think the bulk of the brain is the result of neuroplasticity
2016-09-06 11:21:04	Asher	[21:58:13] i would say the brain itself, as the function of neuroplasticity, is what is hardcoded by dna
2016-09-06 11:21:04	causative	[21:58:40] and the brain has hard coded structures that aren't necessary, or that are helpful but not required
2016-09-06 11:21:04	Asher	[21:58:54] but what we idiomatically refer to as "mind" can be addressed formally as the topological structure of the particular neuroplasticity of a particular brain
2016-09-06 11:21:04	causative	[22:01:11] consider how amazingly resilient the brain is to damage
2016-09-06 11:21:04	causative	[22:01:36] this resilience is because the individual neurons can detect a problem and change their behavior and connections in response
2016-09-06 11:21:04	Asher	[22:02:50] that's part of it
2016-09-06 11:21:04	Asher	[22:03:02] it's also because there is already before any problem multiple interconnectivity
2016-09-06 11:21:04	Asher	[22:03:12] and because when there is a problem, glial networks can fill in
2016-09-06 11:21:04	tomzx	[22:06:37] what if
2016-09-06 11:21:04	tomzx	[22:06:45] the whole thing never changed (or very little)
2016-09-06 11:21:04	Asher	[22:06:53] "the whole thing"?
2016-09-06 11:21:04	tomzx	[22:06:55] and it was just responding to loss of parts of its
2016-09-06 11:21:04	tomzx	[22:06:59] the brain
2016-09-06 11:21:04	tomzx	[22:07:04] or particularly the cortex
2016-09-06 11:21:04	Asher	[22:07:10] never changed = never forgets?
2016-09-06 11:21:04	Asher	[22:07:14] or never changes at all
2016-09-06 11:21:04	tomzx	[22:07:28] changes in the sense of modifies its connections with other neurons
2016-09-06 11:21:04	tomzx	[22:07:43] I guess you'd rather not discuss this hypothetical case?
2016-09-06 11:21:04	causative	[22:07:49] but the brain constantly changes its connections over a person's life
2016-09-06 11:21:04	Asher	[22:07:50] just trying to understand how it is defined
2016-09-06 11:21:04	causative	[22:07:54] specifically it loses connections
2016-09-06 11:21:04	causative	[22:07:58] in vast quantities
2016-09-06 11:21:04	Asher	[22:08:22] from childhood to adulthood vast quantities of connections are lost
2016-09-06 11:21:04	tomzx	[22:08:31] I'm simply trying to suggest that the bulk is already there, and that connections are minor in the whole picture
2016-09-06 11:21:04	Asher	[22:08:33] i don't believe that an average adult is regularly loses vast quantities of connections tho
2016-09-06 11:21:04	Asher	[22:08:40] *losing
2016-09-06 11:21:04	causative	[22:08:48] they continue to over the course of their lives
2016-09-06 11:21:04	Asher	[22:08:50] the bulk of what?
2016-09-06 11:21:04	tomzx	[22:09:01] the bulk of the structure/connections within the brain
2016-09-06 11:21:04	Asher	[22:09:12] causative - there is no question that connections are lost and gained, but not at the scale from childhood to adulthood
2016-09-06 11:21:04	Asher	[22:09:25] a baby has ~10million neurons in the optic nerve, an adult has ~1.5-2 milion
2016-09-06 11:21:04	tomzx	[22:09:40] such that when a traumatic event occurs, such as the loss of a part of the cortex, the information simply "has" to be dispatched to the remaining cortex
2016-09-06 11:21:04	causative	[22:09:42] learning apparently primarily happens by the loss of connections
2016-09-06 11:21:04	Asher	[22:09:58] any links on that?
2016-09-06 11:21:04	causative	[22:10:58] https://en.wikipedia.org/wiki/Synaptic_pruning
2016-09-06 11:21:04	Asher	[22:12:30] heh
2016-09-06 11:21:04	Asher	[22:13:04] that's what iw as talking about above
2016-09-06 11:21:04	Asher	[22:13:14] for an average adult, the number of neurons is not regularly changing that much
2016-09-06 11:21:04	Asher	[22:13:21] but the routings are changing regularly
2016-09-06 11:21:04	causative	[22:13:33] yes, however I recall reading that the number of synapses does continue to decrease in adulthood
2016-09-06 11:21:04	causative	[22:13:40] although at a lesser rate
2016-09-06 11:21:04	Asher	[22:13:43] it doesn't freeze or anything
2016-09-06 11:21:04	sal77	[22:13:56] and neuroplasticity can drop off considerably
2016-09-06 11:21:04	Asher	[22:14:03] but it becomes statistically insignificant is my understnading
2016-09-06 11:21:04	Asher	[22:14:27] i think the important point is that neuroplasticity has two dimensions
2016-09-06 11:21:04	Asher	[22:14:45] 1. synaptic interconnectivity 2. routing interconnectivity
2016-09-06 11:21:04	causative	[22:15:05] http://scholarcommons.usf.edu/cgi/viewcontent.cgi?article=4812&context=ujmm
2016-09-06 11:21:04	causative	[22:15:34] here they track density of neurons and synapses as a function of age, it decreases steadily
2016-09-06 11:21:04	Asher	[22:17:55] i have no argument with that, my only point was that it happens at a far greater rate (orders of magnitude) from childhood to adulthood
2016-09-06 11:21:04	causative	[22:18:45] ok
2016-09-06 11:21:04	tomzx	[22:19:03] causative: it's kinda hard to dissociate synaptic degenerescence due to aging and due to learning
2016-09-06 11:21:04	tomzx	[22:19:11] if that was the point you were trying to make
2016-09-06 11:21:04	causative	[22:20:09] well, clearly the purpose of the large loss of neurons in childhood is learning
2016-09-06 11:21:04	Asher	[22:20:27] i don't disagree with the conclusion, but the way of stating it is a bit obfuscating
2016-09-06 11:21:04	Asher	[22:20:46] i would say: something about massive # of neurons 1. is important for learning 2. interferes with development of complexity
2016-09-06 11:21:04	causative	[22:20:58] so it probably continues to serve a similar role in adulthood
2016-09-06 11:21:04	Asher	[22:21:32] i think an importnat question: what is the advantage of massively decreasing # of neurons from childhood to adulthood?
2016-09-06 11:21:04	tomzx	[22:21:47] Asher: prevent information overload?
2016-09-06 11:21:04	Asher	[22:21:50] another important question: what is the disadvantage of keeping massive interconnectivity?
2016-09-06 11:21:04	causative	[22:21:56] the loss of neurons is a change in neuron weights - and thus encodes information
2016-09-06 11:21:04	causative	[22:22:07] imagine a punch card, now you punch holes in it, the loss of paper encodes information
2016-09-06 11:21:04	tomzx	[22:22:09] Asher: might explain children overactive behavior :)
2016-09-06 11:21:04	tomzx	[22:22:39] causative: yeah, basic NN stuff
2016-09-06 11:21:04	sal77	[22:23:26] even if no connections were lost, it may pose a problem for efficiency
2016-09-06 11:21:04	sal77	[22:25:07] from an evolutionary standpoint, its could be argued increasing efficiency is important not just for glucose consumption but for making quick decisions when in danger
2016-09-06 11:21:04	causative	[22:27:55] do humans lose comparable numbers of non-neuronal cells over the course of their lives?
2016-09-06 11:21:04	sal77	[22:33:28] that varies widely on the tissue
2016-09-06 11:21:04	sal77	[22:34:28] though, out of the tissues that dont have much cell division, im not really sure
2016-09-06 11:21:04	Asher	[22:34:45] most cell loss otherwise is replaced
2016-09-06 11:21:04	sal77	[22:34:45] muscle, adipose fall under that category
2016-09-06 11:21:04	Asher	[22:35:08] but synaptic pruning is not cell loss
2016-09-06 11:21:04	Asher	[22:35:13] it's connectivity loss
2016-09-06 11:21:04	Asher	[22:35:20] neurons are lost but that's separate
2016-09-06 11:21:04	causative	[22:36:42] so that would speak to the idea that neuron loss serves a functional purpose, and is not simply a manifestation of general cell loss with age
2016-09-06 11:21:04	Asher	[22:37:39] absolutely
2016-09-06 11:21:04	Asher	[22:37:46] well... synaptic loss
2016-09-06 11:21:04	Asher	[22:37:49] neuron loss is separate
2016-09-06 11:21:04	Asher	[22:38:22] decreasing in synaptic connectivity definitely serves a functional purpose, perhaps also comes about by way of other processes
2016-09-06 11:21:04	causative	[22:38:26] it speaks to the idea that neuron loss is functional, it has nothing to say about synaptic loss
2016-09-06 11:21:04	Asher	[22:38:57] i don't know about neuron loss in adults
2016-09-06 11:21:04	Asher	[22:39:00] in children that is true
2016-09-06 11:21:04	causative	[22:39:34] cells are not systematically lost in most other tissues, just neurons and some other specific cell types
2016-09-06 11:21:04	sal77	[22:40:37] probably most cells are replaced quite readily
2016-09-06 11:21:04	sal77	[22:41:03] basically all epithelial
2016-09-06 11:21:04	sal77	[22:41:22] mesothelial as well
2016-09-06 11:21:04	sal77	[22:43:00] in the case of cells that are not lost often
2016-09-06 11:21:04	sal77	[22:43:29] muscle fibers can increades or decrease their protein
2016-09-06 11:21:04	sal77	[22:43:58] adipose cells can increase or decrease their fatty acid storage
2016-09-06 11:21:04	sal77	[22:44:59] not really a very clear analogy but some cells have functional ways of adjusting when they cant be replaced
2016-09-06 11:21:04	sal77	[22:45:18] other cells are just readily discarded to mae way for new cells
2016-09-06 11:21:04	Abhishej	[04:04:58] Hi
2016-09-06 11:21:04	Abhishej	[04:05:33] Hiw can I generate the neighborhood of a boolean vector
2016-09-06 11:21:04	Abhishej	[04:05:40] I am using local search
2016-09-06 11:21:04	deego	[04:07:30] hm? all booleans with manhattan = 1 ? 
2016-09-06 11:21:04	Abhishej	[04:08:28] That is charging any 1 to 0 or 0 to 1 and keep rest constant.
2016-09-06 11:21:04	deego	[04:09:37] yeah
2016-09-06 11:21:04	Abhishej	[04:11:39] I am doing that right now and it gets stuck at local optima. How can I expand the neighborhood?
2016-09-06 11:21:04	Abhishej	[04:13:29] Or define other neighborhoods.
2016-09-06 11:21:04	causative	[04:27:12] well, to generate all vectors with distance 1, loop over the positions and successively flip a bit, do whatever you want with the result, then flip it back
2016-09-06 11:21:04	causative	[04:27:31] for distance 2, you need a double nested loop to generate all pairs of positions, and flip them both, then back
2016-09-06 11:21:04	causative	[04:27:33] etc
2016-09-06 11:21:04	causative	[04:27:55] although this is inefficient so you probably don't want to go too much farther
2016-09-06 11:21:04	***	Playback Complete.
2016-09-06 11:21:23	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-06 11:31:30	<--	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has quit (Quit: It's a joke, it's all a joke.)
2016-09-06 11:37:39	<--	causative (~halberd@unaffiliated/halberd) has quit (Ping timeout: 264 seconds)
2016-09-06 11:42:06	-->	Viscid (~viscid@CPE00fc8d4cfa33-CM00fc8d4cfa30.cpe.net.cable.rogers.com) has joined #ai
2016-09-06 11:43:20	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-06 11:43:52	<--	Avinash (~ding@unaffiliated/avinash) has quit (Ping timeout: 244 seconds)
2016-09-06 11:48:00	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 11:56:12	-->	BinaryMan (a79a27c9@gateway/web/freenode/ip.167.154.39.201) has joined #ai
2016-09-06 11:57:26	BinaryMan	good morning
2016-09-06 12:02:19	<--	HuntsMan (~hunts@dhcp15.eps.hw.ac.uk) has quit (Ping timeout: 250 seconds)
2016-09-06 12:06:04	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-06 12:10:58	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 12:11:32	<--	mfc (~mfc@unaffiliated/mfc) has quit (Read error: Connection reset by peer)
2016-09-06 12:18:59	<--	conan (~conan@mdproctor.plus.com) has quit (Ping timeout: 260 seconds)
2016-09-06 12:19:33	-->	conan (~conan@mdproctor.plus.com) has joined #ai
2016-09-06 12:23:39	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 260 seconds)
2016-09-06 12:27:59	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 12:32:52	-->	iH2O (~chatzilla@unaffiliated/myfree) has joined #ai
2016-09-06 12:35:27	<--	iH2O (~chatzilla@unaffiliated/myfree) has left #ai
2016-09-06 12:43:56	-->	tyoc213 (~codefujas@fixed-188-9-187-188-9-53.iusacell.net) has joined #ai
2016-09-06 12:52:28	-->	HuntsMan (~hunts@cpc103060-sgyl39-2-0-cust1324.18-2.cable.virginm.net) has joined #ai
2016-09-06 12:56:25	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 255 seconds)
2016-09-06 13:01:03	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 13:17:17	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-06 13:21:58	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 13:32:25	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 255 seconds)
2016-09-06 13:33:14	-->	mfc (~mfc@unaffiliated/mfc) has joined #ai
2016-09-06 13:35:49	-->	florinandrei (~florinand@65.87.20.2) has joined #ai
2016-09-06 13:36:59	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 13:46:32	-->	amz3 (~amz3@unaffiliated/abki) has joined #ai
2016-09-06 13:47:24	-->	conan_ (~conan@mdproctor.plus.com) has joined #ai
2016-09-06 13:50:07	<--	conan (~conan@mdproctor.plus.com) has quit (Read error: Connection reset by peer)
2016-09-06 13:50:48	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 244 seconds)
2016-09-06 13:55:53	--	irc: disconnected from server
2016-09-06 13:56:54	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-06 13:56:54	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-06 13:56:54	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-06 13:56:54	--	Channel #ai: 76 nicks (1 op, 0 voices, 75 normals)
2016-09-06 13:56:54	***	Buffer Playback...
2016-09-06 13:56:54	Asher	[22:20:27] i don't disagree with the conclusion, but the way of stating it is a bit obfuscating
2016-09-06 13:56:54	Asher	[22:20:46] i would say: something about massive # of neurons 1. is important for learning 2. interferes with development of complexity
2016-09-06 13:56:54	causative	[22:20:58] so it probably continues to serve a similar role in adulthood
2016-09-06 13:56:54	Asher	[22:21:32] i think an importnat question: what is the advantage of massively decreasing # of neurons from childhood to adulthood?
2016-09-06 13:56:54	tomzx	[22:21:47] Asher: prevent information overload?
2016-09-06 13:56:54	Asher	[22:21:50] another important question: what is the disadvantage of keeping massive interconnectivity?
2016-09-06 13:56:54	causative	[22:21:56] the loss of neurons is a change in neuron weights - and thus encodes information
2016-09-06 13:56:54	causative	[22:22:07] imagine a punch card, now you punch holes in it, the loss of paper encodes information
2016-09-06 13:56:54	tomzx	[22:22:09] Asher: might explain children overactive behavior :)
2016-09-06 13:56:54	tomzx	[22:22:39] causative: yeah, basic NN stuff
2016-09-06 13:56:54	sal77	[22:23:26] even if no connections were lost, it may pose a problem for efficiency
2016-09-06 13:56:54	sal77	[22:25:07] from an evolutionary standpoint, its could be argued increasing efficiency is important not just for glucose consumption but for making quick decisions when in danger
2016-09-06 13:56:54	causative	[22:27:55] do humans lose comparable numbers of non-neuronal cells over the course of their lives?
2016-09-06 13:56:54	sal77	[22:33:28] that varies widely on the tissue
2016-09-06 13:56:54	sal77	[22:34:28] though, out of the tissues that dont have much cell division, im not really sure
2016-09-06 13:56:54	Asher	[22:34:45] most cell loss otherwise is replaced
2016-09-06 13:56:54	sal77	[22:34:45] muscle, adipose fall under that category
2016-09-06 13:56:54	Asher	[22:35:08] but synaptic pruning is not cell loss
2016-09-06 13:56:54	Asher	[22:35:13] it's connectivity loss
2016-09-06 13:56:54	Asher	[22:35:20] neurons are lost but that's separate
2016-09-06 13:56:54	causative	[22:36:42] so that would speak to the idea that neuron loss serves a functional purpose, and is not simply a manifestation of general cell loss with age
2016-09-06 13:56:54	Asher	[22:37:39] absolutely
2016-09-06 13:56:54	Asher	[22:37:46] well... synaptic loss
2016-09-06 13:56:54	Asher	[22:37:49] neuron loss is separate
2016-09-06 13:56:54	Asher	[22:38:22] decreasing in synaptic connectivity definitely serves a functional purpose, perhaps also comes about by way of other processes
2016-09-06 13:56:54	causative	[22:38:26] it speaks to the idea that neuron loss is functional, it has nothing to say about synaptic loss
2016-09-06 13:56:54	Asher	[22:38:57] i don't know about neuron loss in adults
2016-09-06 13:56:54	Asher	[22:39:00] in children that is true
2016-09-06 13:56:54	causative	[22:39:34] cells are not systematically lost in most other tissues, just neurons and some other specific cell types
2016-09-06 13:56:54	sal77	[22:40:37] probably most cells are replaced quite readily
2016-09-06 13:56:54	sal77	[22:41:03] basically all epithelial
2016-09-06 13:56:54	sal77	[22:41:22] mesothelial as well
2016-09-06 13:56:54	sal77	[22:43:00] in the case of cells that are not lost often
2016-09-06 13:56:54	sal77	[22:43:29] muscle fibers can increades or decrease their protein
2016-09-06 13:56:54	sal77	[22:43:58] adipose cells can increase or decrease their fatty acid storage
2016-09-06 13:56:54	sal77	[22:44:59] not really a very clear analogy but some cells have functional ways of adjusting when they cant be replaced
2016-09-06 13:56:54	sal77	[22:45:18] other cells are just readily discarded to mae way for new cells
2016-09-06 13:56:54	Abhishej	[04:04:58] Hi
2016-09-06 13:56:54	Abhishej	[04:05:33] Hiw can I generate the neighborhood of a boolean vector
2016-09-06 13:56:54	Abhishej	[04:05:40] I am using local search
2016-09-06 13:56:54	deego	[04:07:30] hm? all booleans with manhattan = 1 ? 
2016-09-06 13:56:54	Abhishej	[04:08:28] That is charging any 1 to 0 or 0 to 1 and keep rest constant.
2016-09-06 13:56:54	deego	[04:09:37] yeah
2016-09-06 13:56:54	Abhishej	[04:11:39] I am doing that right now and it gets stuck at local optima. How can I expand the neighborhood?
2016-09-06 13:56:54	Abhishej	[04:13:29] Or define other neighborhoods.
2016-09-06 13:56:54	causative	[04:27:12] well, to generate all vectors with distance 1, loop over the positions and successively flip a bit, do whatever you want with the result, then flip it back
2016-09-06 13:56:54	causative	[04:27:31] for distance 2, you need a double nested loop to generate all pairs of positions, and flip them both, then back
2016-09-06 13:56:54	causative	[04:27:33] etc
2016-09-06 13:56:54	causative	[04:27:55] although this is inefficient so you probably don't want to go too much farther
2016-09-06 13:56:54	BinaryMan	[11:57:26] good morning
2016-09-06 13:56:54	***	Playback Complete.
2016-09-06 13:57:06	--	irc: disconnected from server
2016-09-06 14:03:24	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-06 14:03:24	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-06 14:03:24	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-06 14:03:24	--	Channel #ai: 77 nicks (1 op, 0 voices, 76 normals)
2016-09-06 14:03:24	***	Buffer Playback...
2016-09-06 14:03:24	Asher	[22:20:27] i don't disagree with the conclusion, but the way of stating it is a bit obfuscating
2016-09-06 14:03:24	Asher	[22:20:46] i would say: something about massive # of neurons 1. is important for learning 2. interferes with development of complexity
2016-09-06 14:03:24	causative	[22:20:58] so it probably continues to serve a similar role in adulthood
2016-09-06 14:03:24	Asher	[22:21:32] i think an importnat question: what is the advantage of massively decreasing # of neurons from childhood to adulthood?
2016-09-06 14:03:24	tomzx	[22:21:47] Asher: prevent information overload?
2016-09-06 14:03:24	Asher	[22:21:50] another important question: what is the disadvantage of keeping massive interconnectivity?
2016-09-06 14:03:24	causative	[22:21:56] the loss of neurons is a change in neuron weights - and thus encodes information
2016-09-06 14:03:24	causative	[22:22:07] imagine a punch card, now you punch holes in it, the loss of paper encodes information
2016-09-06 14:03:24	tomzx	[22:22:09] Asher: might explain children overactive behavior :)
2016-09-06 14:03:24	tomzx	[22:22:39] causative: yeah, basic NN stuff
2016-09-06 14:03:24	sal77	[22:23:26] even if no connections were lost, it may pose a problem for efficiency
2016-09-06 14:03:24	sal77	[22:25:07] from an evolutionary standpoint, its could be argued increasing efficiency is important not just for glucose consumption but for making quick decisions when in danger
2016-09-06 14:03:24	causative	[22:27:55] do humans lose comparable numbers of non-neuronal cells over the course of their lives?
2016-09-06 14:03:24	sal77	[22:33:28] that varies widely on the tissue
2016-09-06 14:03:24	sal77	[22:34:28] though, out of the tissues that dont have much cell division, im not really sure
2016-09-06 14:03:24	Asher	[22:34:45] most cell loss otherwise is replaced
2016-09-06 14:03:24	sal77	[22:34:45] muscle, adipose fall under that category
2016-09-06 14:03:24	Asher	[22:35:08] but synaptic pruning is not cell loss
2016-09-06 14:03:24	Asher	[22:35:13] it's connectivity loss
2016-09-06 14:03:24	Asher	[22:35:20] neurons are lost but that's separate
2016-09-06 14:03:24	causative	[22:36:42] so that would speak to the idea that neuron loss serves a functional purpose, and is not simply a manifestation of general cell loss with age
2016-09-06 14:03:24	Asher	[22:37:39] absolutely
2016-09-06 14:03:24	Asher	[22:37:46] well... synaptic loss
2016-09-06 14:03:24	Asher	[22:37:49] neuron loss is separate
2016-09-06 14:03:24	Asher	[22:38:22] decreasing in synaptic connectivity definitely serves a functional purpose, perhaps also comes about by way of other processes
2016-09-06 14:03:24	causative	[22:38:26] it speaks to the idea that neuron loss is functional, it has nothing to say about synaptic loss
2016-09-06 14:03:24	Asher	[22:38:57] i don't know about neuron loss in adults
2016-09-06 14:03:24	Asher	[22:39:00] in children that is true
2016-09-06 14:03:24	causative	[22:39:34] cells are not systematically lost in most other tissues, just neurons and some other specific cell types
2016-09-06 14:03:24	sal77	[22:40:37] probably most cells are replaced quite readily
2016-09-06 14:03:24	sal77	[22:41:03] basically all epithelial
2016-09-06 14:03:24	sal77	[22:41:22] mesothelial as well
2016-09-06 14:03:24	sal77	[22:43:00] in the case of cells that are not lost often
2016-09-06 14:03:24	sal77	[22:43:29] muscle fibers can increades or decrease their protein
2016-09-06 14:03:24	sal77	[22:43:58] adipose cells can increase or decrease their fatty acid storage
2016-09-06 14:03:24	sal77	[22:44:59] not really a very clear analogy but some cells have functional ways of adjusting when they cant be replaced
2016-09-06 14:03:24	sal77	[22:45:18] other cells are just readily discarded to mae way for new cells
2016-09-06 14:03:24	Abhishej	[04:04:58] Hi
2016-09-06 14:03:24	Abhishej	[04:05:33] Hiw can I generate the neighborhood of a boolean vector
2016-09-06 14:03:24	Abhishej	[04:05:40] I am using local search
2016-09-06 14:03:24	deego	[04:07:30] hm? all booleans with manhattan = 1 ? 
2016-09-06 14:03:24	Abhishej	[04:08:28] That is charging any 1 to 0 or 0 to 1 and keep rest constant.
2016-09-06 14:03:24	deego	[04:09:37] yeah
2016-09-06 14:03:24	Abhishej	[04:11:39] I am doing that right now and it gets stuck at local optima. How can I expand the neighborhood?
2016-09-06 14:03:24	Abhishej	[04:13:29] Or define other neighborhoods.
2016-09-06 14:03:24	causative	[04:27:12] well, to generate all vectors with distance 1, loop over the positions and successively flip a bit, do whatever you want with the result, then flip it back
2016-09-06 14:03:24	causative	[04:27:31] for distance 2, you need a double nested loop to generate all pairs of positions, and flip them both, then back
2016-09-06 14:03:24	causative	[04:27:33] etc
2016-09-06 14:03:24	causative	[04:27:55] although this is inefficient so you probably don't want to go too much farther
2016-09-06 14:03:24	BinaryMan	[11:57:26] good morning
2016-09-06 14:03:24	***	Playback Complete.
2016-09-06 14:03:39	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-06 14:09:02	-->	causative (~halberd@unaffiliated/halberd) has joined #ai
2016-09-06 14:10:16	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-06 14:15:00	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 14:25:19	-->	derk0pf (~derk0pf@p579FE077.dip0.t-ipconnect.de) has joined #ai
2016-09-06 14:27:40	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-06 14:32:03	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 14:43:39	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 260 seconds)
2016-09-06 14:47:57	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 14:52:48	<--	causative (~halberd@unaffiliated/halberd) has quit (Ping timeout: 276 seconds)
2016-09-06 14:59:24	<--	tyoc213 (~codefujas@fixed-188-9-187-188-9-53.iusacell.net) has quit (Quit: tyoc213.github.com)
2016-09-06 15:03:09	<--	JoshS (~jshjsh@172.56.39.225) has quit (Quit: Leaving)
2016-09-06 15:03:28	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 264 seconds)
2016-09-06 15:07:22	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 15:16:12	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 276 seconds)
2016-09-06 15:20:30	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 15:27:05	-->	Rasekov_ (~Rasekov@159.133.117.91.dynamic.reverse-mundo-r.com) has joined #ai
2016-09-06 15:27:58	<--	Rasekov (~Rasekov@159.133.117.91.dynamic.reverse-mundo-r.com) has quit (Read error: Connection reset by peer)
2016-09-06 15:29:01	<--	warsh (~warsh@unaffiliated/warsh) has quit (Ping timeout: 252 seconds)
2016-09-06 15:29:12	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 276 seconds)
2016-09-06 15:33:10	-->	warsh (~warsh@unaffiliated/warsh) has joined #ai
2016-09-06 15:34:00	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 15:35:43	-->	dknitrox (~chatzilla@200.62.146.210) has joined #ai
2016-09-06 15:39:16	<--	conan_ (~conan@mdproctor.plus.com) has quit (Quit: Computer has gone to sleep.)
2016-09-06 15:43:52	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 252 seconds)
2016-09-06 15:46:00	--	Rasekov_ is now known as Rasekov
2016-09-06 15:48:42	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 16:06:33	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 240 seconds)
2016-09-06 16:12:00	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 16:18:38	<--	Viscid (~viscid@CPE00fc8d4cfa33-CM00fc8d4cfa30.cpe.net.cable.rogers.com) has quit
2016-09-06 16:22:42	-->	treehug88 (~textual@cpe-68-173-227-135.nyc.res.rr.com) has joined #ai
2016-09-06 16:22:45	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-06 16:27:29	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 16:30:21	-->	conan (~conan@mdproctor.plus.com) has joined #ai
2016-09-06 16:30:58	-->	SuperKoos (~User@unaffiliated/superkoos) has joined #ai
2016-09-06 16:37:21	<--	sovreign (~sovreign@cpe-172-73-52-179.carolina.res.rr.com) has quit (Quit: Leaving)
2016-09-06 16:41:15	<--	dknitrox (~chatzilla@200.62.146.210) has quit (Quit: ChatZilla 0.9.92 [Firefox 48.0/20160726073904])
2016-09-06 16:42:00	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 276 seconds)
2016-09-06 16:43:19	-->	h4k1m (~hakim@unaffiliated/h4k1m) has joined #ai
2016-09-06 16:43:53	-->	Coldblackice (~anonz@unaffiliated/coldblackice) has joined #ai
2016-09-06 16:44:05	h4k1m	hi everyone
2016-09-06 16:44:52	h4k1m	What's the next step towards learning Neural Networks after backpropagation+gradient descent?
2016-09-06 16:46:17	HuntsMan	next step from what?
2016-09-06 16:46:38	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 16:46:57	h4k1m	HuntsMan: I followed a quick tutorial about Artificial Neural Nets implementation of the XNOR (using theano)
2016-09-06 16:47:25	h4k1m	HuntsMan: What could you suggest me to try after that?
2016-09-06 16:48:14	HuntsMan	well, what do you want to do?
2016-09-06 16:48:24	HuntsMan	what's your aim?
2016-09-06 16:49:08	h4k1m	HuntsMan: I have an image segmentation problem which doesn't seem to be linear (reconstruction of buildings on satellite radar images)
2016-09-06 16:49:35	h4k1m	I'm not sure ANNs are adequate for this kind of problems
2016-09-06 16:49:48	HuntsMan	h4k1m: convolutional neural networks then
2016-09-06 16:50:02	HuntsMan	I am not sure why would think that problem is linear
2016-09-06 16:50:29	h4k1m	HuntsMan: I've heard of them, aren't they too complex for a beginner (like me)
2016-09-06 16:50:39	HuntsMan	no
2016-09-06 16:50:40	h4k1m	HuntsMan: Because I'm not used to non-linear problems
2016-09-06 16:51:03	-->	causative (~halberd@unaffiliated/halberd) has joined #ai
2016-09-06 16:51:31	h4k1m	HuntsMan: What do you use btw to build NNs (which library)? I'm getting started with Theano (Python)
2016-09-06 16:52:36	HuntsMan	h4k1m: keras
2016-09-06 16:52:45	h4k1m	HuntsMan: nice!
2016-09-06 16:52:47	HuntsMan	h4k1m: I would not recomend building NNs by hand
2016-09-06 16:54:49	<--	mfc (~mfc@unaffiliated/mfc) has quit (Read error: Connection reset by peer)
2016-09-06 16:55:21	h4k1m	HuntsMan: I see (thanks for the help...)
2016-09-06 16:59:21	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-06 17:02:53	<--	h4k1m (~hakim@unaffiliated/h4k1m) has quit (Quit: leaving)
2016-09-06 17:04:00	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 17:11:44	<--	amz3 (~amz3@unaffiliated/abki) has quit (Quit: Artufath 1.5)
2016-09-06 17:31:39	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 260 seconds)
2016-09-06 17:33:39	-->	augur (~augur@c-67-160-198-240.hsd1.ca.comcast.net) has joined #ai
2016-09-06 17:34:04	BinaryMan	is anyone familiar with neural turing machine addressing schemes ?
2016-09-06 17:36:01	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 17:36:47	rasmoo	BinaryMan: Ilya Sutskever or someone just as knowledgeable might be able to answer you on the openai gitter
2016-09-06 17:37:10	rasmoo	I've seen lots of ntm questions there
2016-09-06 17:45:04	BinaryMan	I am not sure if NTM is the answer necessarily; I've been trying different network types to better understand their behaviors; it's mainly that I feel that my agents are limited by memory capability. LSTM cells didn't really alleviate that.
2016-09-06 17:46:01	BinaryMan	http://imgur.com/a/KO7dV - the spiking network was capable of a temporal component via the propogation delay/refractory period evolving.
2016-09-06 17:48:01	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 255 seconds)
2016-09-06 17:48:05	BinaryMan	however, state machines were also relatively capable of adapting to each other and more likely to cycle a "memory pattern" which could be perturbed by inputs. I think perhaps that making memory more permanent could be useful for dealing with data that is not part of the sensory pattern.
2016-09-06 17:49:03	BinaryMan	such as previously observed tiles. I would like to be able to store and process real numbers directly from the simulation, but then we are just back to a turing machine style architecture.
2016-09-06 17:51:52	-->	andykay (~AndyKay@unaffiliated/andykay) has joined #ai
2016-09-06 17:52:30	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 18:10:57	BinaryMan	Each processing unit has a number of linked inputs, and each input can have a symbol 0 ... N . I have developed a method of summing the inputs regardless of order with dynamic weights for each symbol to the next symbol state, with the highest sum determining next state.
2016-09-06 18:11:25	BinaryMan	I had tried previously to use a "rulebook" but noted that making rules that were robust to add/removal of links was difficult.
2016-09-06 18:12:14	<--	SuperKoos (~User@unaffiliated/superkoos) has quit (Ping timeout: 244 seconds)
2016-09-06 18:13:24	BinaryMan	It would be interesting to extend from "output state X" to "execute command X" which could be any of a number of memory manipulation commands or an output command, effectively acting as controller of the dynamic internal state of the processing unit.
2016-09-06 18:17:31	-->	Noldorin (~noldorin@unaffiliated/noldorin) has joined #ai
2016-09-06 18:20:02	BinaryMan	The basic question is "what memory elements are necessary for planning using input events from any # of time cycles ago?"
2016-09-06 18:21:52	BinaryMan	LSTM cells as typically presented don't preserve exact value, although if the gates were binary IF rather than multiplicative (add input to internal value or don't, set internal value to zero or don't, output value or don't) I think it would have more usage as a discrete memory cell.
2016-09-06 18:22:27	BinaryMan	Any noise not exceeding the IF threshold would simply be ignored until a relevant signal was recieved
2016-09-06 18:31:03	<--	rasmoo (~ras@unaffiliated/rasmoo) has quit (Quit: MAGA)
2016-09-06 18:38:32	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 240 seconds)
2016-09-06 18:44:03	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 18:44:50	<--	EI24 (~EI24@90-224-11-165-no124.tbcn.telia.com) has quit (Quit: Leaving)
2016-09-06 18:52:15	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-06 18:57:00	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 18:58:12	-->	realz (~realz@unaffiliated/realazthat) has joined #ai
2016-09-06 19:03:28	BinaryMan	trying to empower the FSM processing units... say we have 3 inputs "ABC", "D" internal state => some list of rules (regex?) that leads to "E" new state, "F" output, and "G" stored in extended memory. If it is just making up rules to test in list order ... "CCC" "ABX" , "A*" (match)
2016-09-06 19:05:50	BinaryMan	I end up often with a huge dictionary of rules and difficulty adding a new input
2016-09-06 19:22:13	<--	govg (~govg@unaffiliated/govg) has quit (Ping timeout: 252 seconds)
2016-09-06 19:24:02	-->	govg (~govg@unaffiliated/govg) has joined #ai
2016-09-06 19:29:11	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-06 19:32:59	<--	govg (~govg@unaffiliated/govg) has quit (Ping timeout: 260 seconds)
2016-09-06 19:33:42	-->	govg (~govg@unaffiliated/govg) has joined #ai
2016-09-06 19:33:59	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 19:36:09	causative	how are the rules different from just the state transition table of the FSM?
2016-09-06 19:39:10	-->	SwiftMatt (~Objective@162.242.95.141) has joined #ai
2016-09-06 19:46:51	<--	derk0pf (~derk0pf@p579FE077.dip0.t-ipconnect.de) has quit (Quit: ZzzZZzZZZ.)
2016-09-06 19:51:59	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Ping timeout: 265 seconds)
2016-09-06 19:56:15	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 264 seconds)
2016-09-06 19:59:50	-->	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has joined #ai
2016-09-06 20:01:02	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 20:06:18	-->	Rasekov_ (~Rasekov@159.133.117.91.dynamic.reverse-mundo-r.com) has joined #ai
2016-09-06 20:32:51	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-06 20:32:51	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-06 20:32:51	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-06 20:32:51	--	Channel #ai: 75 nicks (1 op, 0 voices, 74 normals)
2016-09-06 20:32:51	***	Buffer Playback...
2016-09-06 20:32:51	Abhishej	[04:04:58] Hi
2016-09-06 20:32:51	Abhishej	[04:05:33] Hiw can I generate the neighborhood of a boolean vector
2016-09-06 20:32:51	Abhishej	[04:05:40] I am using local search
2016-09-06 20:32:51	deego	[04:07:30] hm? all booleans with manhattan = 1 ? 
2016-09-06 20:32:51	Abhishej	[04:08:28] That is charging any 1 to 0 or 0 to 1 and keep rest constant.
2016-09-06 20:32:51	deego	[04:09:37] yeah
2016-09-06 20:32:51	Abhishej	[04:11:39] I am doing that right now and it gets stuck at local optima. How can I expand the neighborhood?
2016-09-06 20:32:51	Abhishej	[04:13:29] Or define other neighborhoods.
2016-09-06 20:32:51	causative	[04:27:12] well, to generate all vectors with distance 1, loop over the positions and successively flip a bit, do whatever you want with the result, then flip it back
2016-09-06 20:32:51	causative	[04:27:31] for distance 2, you need a double nested loop to generate all pairs of positions, and flip them both, then back
2016-09-06 20:32:51	causative	[04:27:33] etc
2016-09-06 20:32:51	causative	[04:27:55] although this is inefficient so you probably don't want to go too much farther
2016-09-06 20:32:51	BinaryMan	[11:57:26] good morning
2016-09-06 20:32:51	h4k1m	[16:44:05] hi everyone
2016-09-06 20:32:51	h4k1m	[16:44:52] What's the next step towards learning Neural Networks after backpropagation+gradient descent?
2016-09-06 20:32:51	HuntsMan	[16:46:17] next step from what?
2016-09-06 20:32:51	h4k1m	[16:46:57] HuntsMan: I followed a quick tutorial about Artificial Neural Nets implementation of the XNOR (using theano)
2016-09-06 20:32:51	h4k1m	[16:47:25] HuntsMan: What could you suggest me to try after that?
2016-09-06 20:32:51	HuntsMan	[16:48:14] well, what do you want to do?
2016-09-06 20:32:51	HuntsMan	[16:48:24] what's your aim?
2016-09-06 20:32:51	h4k1m	[16:49:08] HuntsMan: I have an image segmentation problem which doesn't seem to be linear (reconstruction of buildings on satellite radar images)
2016-09-06 20:32:51	h4k1m	[16:49:35] I'm not sure ANNs are adequate for this kind of problems
2016-09-06 20:32:51	HuntsMan	[16:49:48] h4k1m: convolutional neural networks then
2016-09-06 20:32:51	HuntsMan	[16:50:02] I am not sure why would think that problem is linear
2016-09-06 20:32:51	h4k1m	[16:50:29] HuntsMan: I've heard of them, aren't they too complex for a beginner (like me)
2016-09-06 20:32:51	HuntsMan	[16:50:39] no
2016-09-06 20:32:51	h4k1m	[16:50:40] HuntsMan: Because I'm not used to non-linear problems
2016-09-06 20:32:51	h4k1m	[16:51:31] HuntsMan: What do you use btw to build NNs (which library)? I'm getting started with Theano (Python)
2016-09-06 20:32:51	HuntsMan	[16:52:36] h4k1m: keras
2016-09-06 20:32:51	h4k1m	[16:52:45] HuntsMan: nice!
2016-09-06 20:32:51	HuntsMan	[16:52:47] h4k1m: I would not recomend building NNs by hand
2016-09-06 20:32:51	h4k1m	[16:55:21] HuntsMan: I see (thanks for the help...)
2016-09-06 20:32:51	BinaryMan	[17:34:04] is anyone familiar with neural turing machine addressing schemes ?
2016-09-06 20:32:51	rasmoo	[17:36:47] BinaryMan: Ilya Sutskever or someone just as knowledgeable might be able to answer you on the openai gitter
2016-09-06 20:32:51	rasmoo	[17:37:10] I've seen lots of ntm questions there
2016-09-06 20:32:51	BinaryMan	[17:45:04] I am not sure if NTM is the answer necessarily; I've been trying different network types to better understand their behaviors; it's mainly that I feel that my agents are limited by memory capability. LSTM cells didn't really alleviate that.
2016-09-06 20:32:51	BinaryMan	[17:46:01] http://imgur.com/a/KO7dV - the spiking network was capable of a temporal component via the propogation delay/refractory period evolving.
2016-09-06 20:32:51	BinaryMan	[17:48:05] however, state machines were also relatively capable of adapting to each other and more likely to cycle a "memory pattern" which could be perturbed by inputs. I think perhaps that making memory more permanent could be useful for dealing with data that is not part of the sensory pattern.
2016-09-06 20:32:51	BinaryMan	[17:49:03] such as previously observed tiles. I would like to be able to store and process real numbers directly from the simulation, but then we are just back to a turing machine style architecture.
2016-09-06 20:32:51	BinaryMan	[18:10:57] Each processing unit has a number of linked inputs, and each input can have a symbol 0 ... N . I have developed a method of summing the inputs regardless of order with dynamic weights for each symbol to the next symbol state, with the highest sum determining next state.
2016-09-06 20:32:51	BinaryMan	[18:11:25] I had tried previously to use a "rulebook" but noted that making rules that were robust to add/removal of links was difficult.
2016-09-06 20:32:51	BinaryMan	[18:13:24] It would be interesting to extend from "output state X" to "execute command X" which could be any of a number of memory manipulation commands or an output command, effectively acting as controller of the dynamic internal state of the processing unit.
2016-09-06 20:32:51	BinaryMan	[18:20:02] The basic question is "what memory elements are necessary for planning using input events from any # of time cycles ago?"
2016-09-06 20:32:51	BinaryMan	[18:21:52] LSTM cells as typically presented don't preserve exact value, although if the gates were binary IF rather than multiplicative (add input to internal value or don't, set internal value to zero or don't, output value or don't) I think it would have more usage as a discrete memory cell.
2016-09-06 20:32:51	BinaryMan	[18:22:27] Any noise not exceeding the IF threshold would simply be ignored until a relevant signal was recieved
2016-09-06 20:32:51	BinaryMan	[19:03:28] trying to empower the FSM processing units... say we have 3 inputs "ABC", "D" internal state => some list of rules (regex?) that leads to "E" new state, "F" output, and "G" stored in extended memory. If it is just making up rules to test in list order ... "CCC" "ABX" , "A*" (match)
2016-09-06 20:32:51	BinaryMan	[19:05:50] I end up often with a huge dictionary of rules and difficulty adding a new input
2016-09-06 20:32:51	causative	[19:36:09] how are the rules different from just the state transition table of the FSM?
2016-09-06 20:32:51	BinaryMan	[20:08:04] its a more compressed format in this case than storing each combination
2016-09-06 20:32:51	BinaryMan	[20:11:00] I am not sure which network type to use honestly, as there is no "training set" - the agents reproduce and compete with the only fitness being their efficiency at doing so
2016-09-06 20:32:51	***	Playback Complete.
2016-09-06 20:33:06	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-06 20:37:44	-->	Asher (~asher@c-73-106-33-32.hsd1.ga.comcast.net) has joined #ai
2016-09-06 20:42:20	--	irc: disconnected from server
2016-09-06 20:42:23	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-06 20:42:23	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-06 20:42:23	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-06 20:42:23	--	Channel #ai: 76 nicks (1 op, 0 voices, 75 normals)
2016-09-06 20:42:23	***	Buffer Playback...
2016-09-06 20:42:23	Abhishej	[04:04:58] Hi
2016-09-06 20:42:23	Abhishej	[04:05:33] Hiw can I generate the neighborhood of a boolean vector
2016-09-06 20:42:23	Abhishej	[04:05:40] I am using local search
2016-09-06 20:42:23	deego	[04:07:30] hm? all booleans with manhattan = 1 ? 
2016-09-06 20:42:23	Abhishej	[04:08:28] That is charging any 1 to 0 or 0 to 1 and keep rest constant.
2016-09-06 20:42:23	deego	[04:09:37] yeah
2016-09-06 20:42:23	Abhishej	[04:11:39] I am doing that right now and it gets stuck at local optima. How can I expand the neighborhood?
2016-09-06 20:42:23	Abhishej	[04:13:29] Or define other neighborhoods.
2016-09-06 20:42:23	causative	[04:27:12] well, to generate all vectors with distance 1, loop over the positions and successively flip a bit, do whatever you want with the result, then flip it back
2016-09-06 20:42:23	causative	[04:27:31] for distance 2, you need a double nested loop to generate all pairs of positions, and flip them both, then back
2016-09-06 20:42:23	causative	[04:27:33] etc
2016-09-06 20:42:23	causative	[04:27:55] although this is inefficient so you probably don't want to go too much farther
2016-09-06 20:42:23	BinaryMan	[11:57:26] good morning
2016-09-06 20:42:23	h4k1m	[16:44:05] hi everyone
2016-09-06 20:42:23	h4k1m	[16:44:52] What's the next step towards learning Neural Networks after backpropagation+gradient descent?
2016-09-06 20:42:23	HuntsMan	[16:46:17] next step from what?
2016-09-06 20:42:23	h4k1m	[16:46:57] HuntsMan: I followed a quick tutorial about Artificial Neural Nets implementation of the XNOR (using theano)
2016-09-06 20:42:23	h4k1m	[16:47:25] HuntsMan: What could you suggest me to try after that?
2016-09-06 20:42:23	HuntsMan	[16:48:14] well, what do you want to do?
2016-09-06 20:42:23	HuntsMan	[16:48:24] what's your aim?
2016-09-06 20:42:23	h4k1m	[16:49:08] HuntsMan: I have an image segmentation problem which doesn't seem to be linear (reconstruction of buildings on satellite radar images)
2016-09-06 20:42:23	h4k1m	[16:49:35] I'm not sure ANNs are adequate for this kind of problems
2016-09-06 20:42:23	HuntsMan	[16:49:48] h4k1m: convolutional neural networks then
2016-09-06 20:42:23	HuntsMan	[16:50:02] I am not sure why would think that problem is linear
2016-09-06 20:42:23	h4k1m	[16:50:29] HuntsMan: I've heard of them, aren't they too complex for a beginner (like me)
2016-09-06 20:42:23	HuntsMan	[16:50:39] no
2016-09-06 20:42:23	h4k1m	[16:50:40] HuntsMan: Because I'm not used to non-linear problems
2016-09-06 20:42:23	h4k1m	[16:51:31] HuntsMan: What do you use btw to build NNs (which library)? I'm getting started with Theano (Python)
2016-09-06 20:42:23	HuntsMan	[16:52:36] h4k1m: keras
2016-09-06 20:42:23	h4k1m	[16:52:45] HuntsMan: nice!
2016-09-06 20:42:23	HuntsMan	[16:52:47] h4k1m: I would not recomend building NNs by hand
2016-09-06 20:42:23	h4k1m	[16:55:21] HuntsMan: I see (thanks for the help...)
2016-09-06 20:42:23	BinaryMan	[17:34:04] is anyone familiar with neural turing machine addressing schemes ?
2016-09-06 20:42:23	rasmoo	[17:36:47] BinaryMan: Ilya Sutskever or someone just as knowledgeable might be able to answer you on the openai gitter
2016-09-06 20:42:23	rasmoo	[17:37:10] I've seen lots of ntm questions there
2016-09-06 20:42:23	BinaryMan	[17:45:04] I am not sure if NTM is the answer necessarily; I've been trying different network types to better understand their behaviors; it's mainly that I feel that my agents are limited by memory capability. LSTM cells didn't really alleviate that.
2016-09-06 20:42:23	BinaryMan	[17:46:01] http://imgur.com/a/KO7dV - the spiking network was capable of a temporal component via the propogation delay/refractory period evolving.
2016-09-06 20:42:23	BinaryMan	[17:48:05] however, state machines were also relatively capable of adapting to each other and more likely to cycle a "memory pattern" which could be perturbed by inputs. I think perhaps that making memory more permanent could be useful for dealing with data that is not part of the sensory pattern.
2016-09-06 20:42:23	BinaryMan	[17:49:03] such as previously observed tiles. I would like to be able to store and process real numbers directly from the simulation, but then we are just back to a turing machine style architecture.
2016-09-06 20:42:23	BinaryMan	[18:10:57] Each processing unit has a number of linked inputs, and each input can have a symbol 0 ... N . I have developed a method of summing the inputs regardless of order with dynamic weights for each symbol to the next symbol state, with the highest sum determining next state.
2016-09-06 20:42:23	BinaryMan	[18:11:25] I had tried previously to use a "rulebook" but noted that making rules that were robust to add/removal of links was difficult.
2016-09-06 20:42:23	BinaryMan	[18:13:24] It would be interesting to extend from "output state X" to "execute command X" which could be any of a number of memory manipulation commands or an output command, effectively acting as controller of the dynamic internal state of the processing unit.
2016-09-06 20:42:23	BinaryMan	[18:20:02] The basic question is "what memory elements are necessary for planning using input events from any # of time cycles ago?"
2016-09-06 20:42:23	BinaryMan	[18:21:52] LSTM cells as typically presented don't preserve exact value, although if the gates were binary IF rather than multiplicative (add input to internal value or don't, set internal value to zero or don't, output value or don't) I think it would have more usage as a discrete memory cell.
2016-09-06 20:42:23	BinaryMan	[18:22:27] Any noise not exceeding the IF threshold would simply be ignored until a relevant signal was recieved
2016-09-06 20:42:23	BinaryMan	[19:03:28] trying to empower the FSM processing units... say we have 3 inputs "ABC", "D" internal state => some list of rules (regex?) that leads to "E" new state, "F" output, and "G" stored in extended memory. If it is just making up rules to test in list order ... "CCC" "ABX" , "A*" (match)
2016-09-06 20:42:23	BinaryMan	[19:05:50] I end up often with a huge dictionary of rules and difficulty adding a new input
2016-09-06 20:42:23	causative	[19:36:09] how are the rules different from just the state transition table of the FSM?
2016-09-06 20:42:23	BinaryMan	[20:08:04] its a more compressed format in this case than storing each combination
2016-09-06 20:42:23	BinaryMan	[20:11:00] I am not sure which network type to use honestly, as there is no "training set" - the agents reproduce and compete with the only fitness being their efficiency at doing so
2016-09-06 20:42:23	***	Playback Complete.
2016-09-06 20:42:33	--	irc: disconnected from server
2016-09-06 20:43:27	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-06 20:43:27	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-06 20:43:27	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-06 20:43:27	--	Channel #ai: 76 nicks (1 op, 0 voices, 75 normals)
2016-09-06 20:43:27	***	Buffer Playback...
2016-09-06 20:43:27	Abhishej	[04:04:58] Hi
2016-09-06 20:43:27	Abhishej	[04:05:33] Hiw can I generate the neighborhood of a boolean vector
2016-09-06 20:43:27	Abhishej	[04:05:40] I am using local search
2016-09-06 20:43:27	deego	[04:07:30] hm? all booleans with manhattan = 1 ? 
2016-09-06 20:43:27	Abhishej	[04:08:28] That is charging any 1 to 0 or 0 to 1 and keep rest constant.
2016-09-06 20:43:27	deego	[04:09:37] yeah
2016-09-06 20:43:27	Abhishej	[04:11:39] I am doing that right now and it gets stuck at local optima. How can I expand the neighborhood?
2016-09-06 20:43:27	Abhishej	[04:13:29] Or define other neighborhoods.
2016-09-06 20:43:27	causative	[04:27:12] well, to generate all vectors with distance 1, loop over the positions and successively flip a bit, do whatever you want with the result, then flip it back
2016-09-06 20:43:27	causative	[04:27:31] for distance 2, you need a double nested loop to generate all pairs of positions, and flip them both, then back
2016-09-06 20:43:27	causative	[04:27:33] etc
2016-09-06 20:43:27	causative	[04:27:55] although this is inefficient so you probably don't want to go too much farther
2016-09-06 20:43:27	BinaryMan	[11:57:26] good morning
2016-09-06 20:43:27	h4k1m	[16:44:05] hi everyone
2016-09-06 20:43:27	h4k1m	[16:44:52] What's the next step towards learning Neural Networks after backpropagation+gradient descent?
2016-09-06 20:43:27	HuntsMan	[16:46:17] next step from what?
2016-09-06 20:43:27	h4k1m	[16:46:57] HuntsMan: I followed a quick tutorial about Artificial Neural Nets implementation of the XNOR (using theano)
2016-09-06 20:43:27	h4k1m	[16:47:25] HuntsMan: What could you suggest me to try after that?
2016-09-06 20:43:27	HuntsMan	[16:48:14] well, what do you want to do?
2016-09-06 20:43:27	HuntsMan	[16:48:24] what's your aim?
2016-09-06 20:43:27	h4k1m	[16:49:08] HuntsMan: I have an image segmentation problem which doesn't seem to be linear (reconstruction of buildings on satellite radar images)
2016-09-06 20:43:27	h4k1m	[16:49:35] I'm not sure ANNs are adequate for this kind of problems
2016-09-06 20:43:27	HuntsMan	[16:49:48] h4k1m: convolutional neural networks then
2016-09-06 20:43:27	HuntsMan	[16:50:02] I am not sure why would think that problem is linear
2016-09-06 20:43:27	h4k1m	[16:50:29] HuntsMan: I've heard of them, aren't they too complex for a beginner (like me)
2016-09-06 20:43:27	HuntsMan	[16:50:39] no
2016-09-06 20:43:27	h4k1m	[16:50:40] HuntsMan: Because I'm not used to non-linear problems
2016-09-06 20:43:27	h4k1m	[16:51:31] HuntsMan: What do you use btw to build NNs (which library)? I'm getting started with Theano (Python)
2016-09-06 20:43:27	HuntsMan	[16:52:36] h4k1m: keras
2016-09-06 20:43:27	h4k1m	[16:52:45] HuntsMan: nice!
2016-09-06 20:43:27	HuntsMan	[16:52:47] h4k1m: I would not recomend building NNs by hand
2016-09-06 20:43:27	h4k1m	[16:55:21] HuntsMan: I see (thanks for the help...)
2016-09-06 20:43:27	BinaryMan	[17:34:04] is anyone familiar with neural turing machine addressing schemes ?
2016-09-06 20:43:27	rasmoo	[17:36:47] BinaryMan: Ilya Sutskever or someone just as knowledgeable might be able to answer you on the openai gitter
2016-09-06 20:43:27	rasmoo	[17:37:10] I've seen lots of ntm questions there
2016-09-06 20:43:27	BinaryMan	[17:45:04] I am not sure if NTM is the answer necessarily; I've been trying different network types to better understand their behaviors; it's mainly that I feel that my agents are limited by memory capability. LSTM cells didn't really alleviate that.
2016-09-06 20:43:27	BinaryMan	[17:46:01] http://imgur.com/a/KO7dV - the spiking network was capable of a temporal component via the propogation delay/refractory period evolving.
2016-09-06 20:43:27	BinaryMan	[17:48:05] however, state machines were also relatively capable of adapting to each other and more likely to cycle a "memory pattern" which could be perturbed by inputs. I think perhaps that making memory more permanent could be useful for dealing with data that is not part of the sensory pattern.
2016-09-06 20:43:27	BinaryMan	[17:49:03] such as previously observed tiles. I would like to be able to store and process real numbers directly from the simulation, but then we are just back to a turing machine style architecture.
2016-09-06 20:43:27	BinaryMan	[18:10:57] Each processing unit has a number of linked inputs, and each input can have a symbol 0 ... N . I have developed a method of summing the inputs regardless of order with dynamic weights for each symbol to the next symbol state, with the highest sum determining next state.
2016-09-06 20:43:27	BinaryMan	[18:11:25] I had tried previously to use a "rulebook" but noted that making rules that were robust to add/removal of links was difficult.
2016-09-06 20:43:27	BinaryMan	[18:13:24] It would be interesting to extend from "output state X" to "execute command X" which could be any of a number of memory manipulation commands or an output command, effectively acting as controller of the dynamic internal state of the processing unit.
2016-09-06 20:43:27	BinaryMan	[18:20:02] The basic question is "what memory elements are necessary for planning using input events from any # of time cycles ago?"
2016-09-06 20:43:27	BinaryMan	[18:21:52] LSTM cells as typically presented don't preserve exact value, although if the gates were binary IF rather than multiplicative (add input to internal value or don't, set internal value to zero or don't, output value or don't) I think it would have more usage as a discrete memory cell.
2016-09-06 20:43:27	BinaryMan	[18:22:27] Any noise not exceeding the IF threshold would simply be ignored until a relevant signal was recieved
2016-09-06 20:43:27	BinaryMan	[19:03:28] trying to empower the FSM processing units... say we have 3 inputs "ABC", "D" internal state => some list of rules (regex?) that leads to "E" new state, "F" output, and "G" stored in extended memory. If it is just making up rules to test in list order ... "CCC" "ABX" , "A*" (match)
2016-09-06 20:43:27	BinaryMan	[19:05:50] I end up often with a huge dictionary of rules and difficulty adding a new input
2016-09-06 20:43:27	causative	[19:36:09] how are the rules different from just the state transition table of the FSM?
2016-09-06 20:43:27	BinaryMan	[20:08:04] its a more compressed format in this case than storing each combination
2016-09-06 20:43:27	BinaryMan	[20:11:00] I am not sure which network type to use honestly, as there is no "training set" - the agents reproduce and compete with the only fitness being their efficiency at doing so
2016-09-06 20:43:27	***	Playback Complete.
2016-09-06 20:43:42	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-06 20:44:04	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-06 20:46:04	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-06 20:50:57	<--	tdy (~tdy@unaffiliated/tdy) has quit (Ping timeout: 276 seconds)
2016-09-06 20:51:41	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 20:54:06	-->	B3nszy (~b3nszy@2601:580:c103:6150:7c50:ddb9:8ac5:d88b) has joined #ai
2016-09-06 20:54:13	B3nszy	do you guys take notes when learning how to program?
2016-09-06 20:54:19	B3nszy	or learning a new programming language
2016-09-06 20:57:25	-->	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has joined #ai
2016-09-06 21:01:39	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 264 seconds)
2016-09-06 21:06:00	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 21:26:30	causative	ya it's a good idea
2016-09-06 21:27:10	causative	if you already know a programming language it can be helpful to make a table that translates the new language into the old one (roughly - might make notes of anything lost in translation)
2016-09-06 21:34:56	<--	blackwind_123 (~IceChat9@117.192.146.46) has quit (Ping timeout: 244 seconds)
2016-09-06 21:36:59	-->	blackwind_123 (~IceChat9@117.192.145.141) has joined #ai
2016-09-06 21:37:50	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-06 21:41:21	<--	florinandrei (~florinand@65.87.20.2) has quit (Quit: Leaving)
2016-09-06 21:42:02	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 21:52:23	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-06 21:52:36	-->	tdy (~tdy@unaffiliated/tdy) has joined #ai
2016-09-06 21:56:59	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 21:58:06	-->	dknitrox (~chatzilla@143.137.147.230) has joined #ai
2016-09-06 21:59:49	precognist	:D
2016-09-06 22:03:56	-->	Coldblackice (~anonz@unaffiliated/coldblackice) has joined #ai
2016-09-06 22:05:30	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-06 22:08:32	<--	B3nszy (~b3nszy@2601:580:c103:6150:7c50:ddb9:8ac5:d88b) has quit (Remote host closed the connection)
2016-09-06 22:09:58	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 22:22:49	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 244 seconds)
2016-09-06 22:28:00	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 22:37:22	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 255 seconds)
2016-09-06 22:39:01	<--	Noldorin (~noldorin@unaffiliated/noldorin) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-06 22:42:01	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-06 23:07:39	<--	causative (~halberd@unaffiliated/halberd) has quit (Ping timeout: 265 seconds)
2016-09-06 23:14:01	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-06 23:22:57	<--	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has quit (Quit: It's a joke, it's all a joke.)
2016-09-06 23:24:46	<--	pffffffft (~pffffffft@unaffiliated/pffffffft) has quit (Ping timeout: 252 seconds)
2016-09-06 23:25:39	<--	dknitrox (~chatzilla@143.137.147.230) has quit (Ping timeout: 276 seconds)
2016-09-06 23:34:42	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-07 00:03:03	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 244 seconds)
2016-09-07 00:08:41	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-07 00:14:34	appa-nerd	There are rarely dramatic differences in programming languages.  I just keep copies of the common methods as I figure them out.
2016-09-07 00:19:04	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 264 seconds)
2016-09-07 00:23:53	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-07 00:34:27	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-07 00:38:56	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-07 00:43:07	--	irc: disconnected from server
2016-09-07 02:15:22	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-07 02:15:22	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-07 02:15:22	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-07 02:15:22	--	Channel #ai: 76 nicks (1 op, 0 voices, 75 normals)
2016-09-07 02:15:22	***	Buffer Playback...
2016-09-07 02:15:22	causative	[04:27:12] well, to generate all vectors with distance 1, loop over the positions and successively flip a bit, do whatever you want with the result, then flip it back
2016-09-07 02:15:22	causative	[04:27:31] for distance 2, you need a double nested loop to generate all pairs of positions, and flip them both, then back
2016-09-07 02:15:22	causative	[04:27:33] etc
2016-09-07 02:15:22	causative	[04:27:55] although this is inefficient so you probably don't want to go too much farther
2016-09-07 02:15:22	BinaryMan	[11:57:26] good morning
2016-09-07 02:15:22	h4k1m	[16:44:05] hi everyone
2016-09-07 02:15:22	h4k1m	[16:44:52] What's the next step towards learning Neural Networks after backpropagation+gradient descent?
2016-09-07 02:15:22	HuntsMan	[16:46:17] next step from what?
2016-09-07 02:15:22	h4k1m	[16:46:57] HuntsMan: I followed a quick tutorial about Artificial Neural Nets implementation of the XNOR (using theano)
2016-09-07 02:15:22	h4k1m	[16:47:25] HuntsMan: What could you suggest me to try after that?
2016-09-07 02:15:22	HuntsMan	[16:48:14] well, what do you want to do?
2016-09-07 02:15:22	HuntsMan	[16:48:24] what's your aim?
2016-09-07 02:15:22	h4k1m	[16:49:08] HuntsMan: I have an image segmentation problem which doesn't seem to be linear (reconstruction of buildings on satellite radar images)
2016-09-07 02:15:22	h4k1m	[16:49:35] I'm not sure ANNs are adequate for this kind of problems
2016-09-07 02:15:22	HuntsMan	[16:49:48] h4k1m: convolutional neural networks then
2016-09-07 02:15:22	HuntsMan	[16:50:02] I am not sure why would think that problem is linear
2016-09-07 02:15:22	h4k1m	[16:50:29] HuntsMan: I've heard of them, aren't they too complex for a beginner (like me)
2016-09-07 02:15:22	HuntsMan	[16:50:39] no
2016-09-07 02:15:22	h4k1m	[16:50:40] HuntsMan: Because I'm not used to non-linear problems
2016-09-07 02:15:22	h4k1m	[16:51:31] HuntsMan: What do you use btw to build NNs (which library)? I'm getting started with Theano (Python)
2016-09-07 02:15:22	HuntsMan	[16:52:36] h4k1m: keras
2016-09-07 02:15:22	h4k1m	[16:52:45] HuntsMan: nice!
2016-09-07 02:15:22	HuntsMan	[16:52:47] h4k1m: I would not recomend building NNs by hand
2016-09-07 02:15:22	h4k1m	[16:55:21] HuntsMan: I see (thanks for the help...)
2016-09-07 02:15:22	BinaryMan	[17:34:04] is anyone familiar with neural turing machine addressing schemes ?
2016-09-07 02:15:22	rasmoo	[17:36:47] BinaryMan: Ilya Sutskever or someone just as knowledgeable might be able to answer you on the openai gitter
2016-09-07 02:15:22	rasmoo	[17:37:10] I've seen lots of ntm questions there
2016-09-07 02:15:22	BinaryMan	[17:45:04] I am not sure if NTM is the answer necessarily; I've been trying different network types to better understand their behaviors; it's mainly that I feel that my agents are limited by memory capability. LSTM cells didn't really alleviate that.
2016-09-07 02:15:22	BinaryMan	[17:46:01] http://imgur.com/a/KO7dV - the spiking network was capable of a temporal component via the propogation delay/refractory period evolving.
2016-09-07 02:15:22	BinaryMan	[17:48:05] however, state machines were also relatively capable of adapting to each other and more likely to cycle a "memory pattern" which could be perturbed by inputs. I think perhaps that making memory more permanent could be useful for dealing with data that is not part of the sensory pattern.
2016-09-07 02:15:22	BinaryMan	[17:49:03] such as previously observed tiles. I would like to be able to store and process real numbers directly from the simulation, but then we are just back to a turing machine style architecture.
2016-09-07 02:15:22	BinaryMan	[18:10:57] Each processing unit has a number of linked inputs, and each input can have a symbol 0 ... N . I have developed a method of summing the inputs regardless of order with dynamic weights for each symbol to the next symbol state, with the highest sum determining next state.
2016-09-07 02:15:22	BinaryMan	[18:11:25] I had tried previously to use a "rulebook" but noted that making rules that were robust to add/removal of links was difficult.
2016-09-07 02:15:22	BinaryMan	[18:13:24] It would be interesting to extend from "output state X" to "execute command X" which could be any of a number of memory manipulation commands or an output command, effectively acting as controller of the dynamic internal state of the processing unit.
2016-09-07 02:15:22	BinaryMan	[18:20:02] The basic question is "what memory elements are necessary for planning using input events from any # of time cycles ago?"
2016-09-07 02:15:22	BinaryMan	[18:21:52] LSTM cells as typically presented don't preserve exact value, although if the gates were binary IF rather than multiplicative (add input to internal value or don't, set internal value to zero or don't, output value or don't) I think it would have more usage as a discrete memory cell.
2016-09-07 02:15:22	BinaryMan	[18:22:27] Any noise not exceeding the IF threshold would simply be ignored until a relevant signal was recieved
2016-09-07 02:15:22	BinaryMan	[19:03:28] trying to empower the FSM processing units... say we have 3 inputs "ABC", "D" internal state => some list of rules (regex?) that leads to "E" new state, "F" output, and "G" stored in extended memory. If it is just making up rules to test in list order ... "CCC" "ABX" , "A*" (match)
2016-09-07 02:15:22	BinaryMan	[19:05:50] I end up often with a huge dictionary of rules and difficulty adding a new input
2016-09-07 02:15:22	causative	[19:36:09] how are the rules different from just the state transition table of the FSM?
2016-09-07 02:15:22	BinaryMan	[20:08:04] its a more compressed format in this case than storing each combination
2016-09-07 02:15:22	BinaryMan	[20:11:00] I am not sure which network type to use honestly, as there is no "training set" - the agents reproduce and compete with the only fitness being their efficiency at doing so
2016-09-07 02:15:22	B3nszy	[20:54:13] do you guys take notes when learning how to program?
2016-09-07 02:15:22	B3nszy	[20:54:19] or learning a new programming language
2016-09-07 02:15:22	causative	[21:26:30] ya it's a good idea
2016-09-07 02:15:22	causative	[21:27:10] if you already know a programming language it can be helpful to make a table that translates the new language into the old one (roughly - might make notes of anything lost in translation)
2016-09-07 02:15:22	precognist	[21:59:49] :D
2016-09-07 02:15:22	appa-nerd	[00:14:34] There are rarely dramatic differences in programming languages.  I just keep copies of the common methods as I figure them out.
2016-09-07 02:15:22	YYZ	[01:02:49] the Turing test is supposedly about distinguishing AI from genuine humans, but where's the test to distinguish genuine humans from humans emulating AI ?
2016-09-07 02:15:22	djancak1	[01:07:08] Tell me more about the test to distinguish genuine humans from humans emulating AI
2016-09-07 02:15:22	***	Playback Complete.
2016-09-07 02:15:37	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-07 02:17:52	<--	djancak1 (~diancak@unaffiliated/djancak) has quit (Ping timeout: 264 seconds)
2016-09-07 02:20:03	-->	djancak1 (~diancak@unaffiliated/djancak) has joined #ai
2016-09-07 02:45:54	--	irc: disconnected from server
2016-09-07 09:43:19	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-07 09:43:19	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-07 09:43:19	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-07 09:43:19	--	Channel #ai: 67 nicks (1 op, 0 voices, 66 normals)
2016-09-07 09:43:19	***	Buffer Playback...
2016-09-07 09:43:19	causative	[04:27:55] although this is inefficient so you probably don't want to go too much farther
2016-09-07 09:43:19	BinaryMan	[11:57:26] good morning
2016-09-07 09:43:19	h4k1m	[16:44:05] hi everyone
2016-09-07 09:43:19	h4k1m	[16:44:52] What's the next step towards learning Neural Networks after backpropagation+gradient descent?
2016-09-07 09:43:19	HuntsMan	[16:46:17] next step from what?
2016-09-07 09:43:19	h4k1m	[16:46:57] HuntsMan: I followed a quick tutorial about Artificial Neural Nets implementation of the XNOR (using theano)
2016-09-07 09:43:19	h4k1m	[16:47:25] HuntsMan: What could you suggest me to try after that?
2016-09-07 09:43:19	HuntsMan	[16:48:14] well, what do you want to do?
2016-09-07 09:43:19	HuntsMan	[16:48:24] what's your aim?
2016-09-07 09:43:19	h4k1m	[16:49:08] HuntsMan: I have an image segmentation problem which doesn't seem to be linear (reconstruction of buildings on satellite radar images)
2016-09-07 09:43:19	h4k1m	[16:49:35] I'm not sure ANNs are adequate for this kind of problems
2016-09-07 09:43:19	HuntsMan	[16:49:48] h4k1m: convolutional neural networks then
2016-09-07 09:43:19	HuntsMan	[16:50:02] I am not sure why would think that problem is linear
2016-09-07 09:43:19	h4k1m	[16:50:29] HuntsMan: I've heard of them, aren't they too complex for a beginner (like me)
2016-09-07 09:43:19	HuntsMan	[16:50:39] no
2016-09-07 09:43:19	h4k1m	[16:50:40] HuntsMan: Because I'm not used to non-linear problems
2016-09-07 09:43:19	h4k1m	[16:51:31] HuntsMan: What do you use btw to build NNs (which library)? I'm getting started with Theano (Python)
2016-09-07 09:43:19	HuntsMan	[16:52:36] h4k1m: keras
2016-09-07 09:43:19	h4k1m	[16:52:45] HuntsMan: nice!
2016-09-07 09:43:19	HuntsMan	[16:52:47] h4k1m: I would not recomend building NNs by hand
2016-09-07 09:43:19	h4k1m	[16:55:21] HuntsMan: I see (thanks for the help...)
2016-09-07 09:43:19	BinaryMan	[17:34:04] is anyone familiar with neural turing machine addressing schemes ?
2016-09-07 09:43:19	rasmoo	[17:36:47] BinaryMan: Ilya Sutskever or someone just as knowledgeable might be able to answer you on the openai gitter
2016-09-07 09:43:19	rasmoo	[17:37:10] I've seen lots of ntm questions there
2016-09-07 09:43:19	BinaryMan	[17:45:04] I am not sure if NTM is the answer necessarily; I've been trying different network types to better understand their behaviors; it's mainly that I feel that my agents are limited by memory capability. LSTM cells didn't really alleviate that.
2016-09-07 09:43:19	BinaryMan	[17:46:01] http://imgur.com/a/KO7dV - the spiking network was capable of a temporal component via the propogation delay/refractory period evolving.
2016-09-07 09:43:19	BinaryMan	[17:48:05] however, state machines were also relatively capable of adapting to each other and more likely to cycle a "memory pattern" which could be perturbed by inputs. I think perhaps that making memory more permanent could be useful for dealing with data that is not part of the sensory pattern.
2016-09-07 09:43:19	BinaryMan	[17:49:03] such as previously observed tiles. I would like to be able to store and process real numbers directly from the simulation, but then we are just back to a turing machine style architecture.
2016-09-07 09:43:19	BinaryMan	[18:10:57] Each processing unit has a number of linked inputs, and each input can have a symbol 0 ... N . I have developed a method of summing the inputs regardless of order with dynamic weights for each symbol to the next symbol state, with the highest sum determining next state.
2016-09-07 09:43:19	BinaryMan	[18:11:25] I had tried previously to use a "rulebook" but noted that making rules that were robust to add/removal of links was difficult.
2016-09-07 09:43:19	BinaryMan	[18:13:24] It would be interesting to extend from "output state X" to "execute command X" which could be any of a number of memory manipulation commands or an output command, effectively acting as controller of the dynamic internal state of the processing unit.
2016-09-07 09:43:19	BinaryMan	[18:20:02] The basic question is "what memory elements are necessary for planning using input events from any # of time cycles ago?"
2016-09-07 09:43:19	BinaryMan	[18:21:52] LSTM cells as typically presented don't preserve exact value, although if the gates were binary IF rather than multiplicative (add input to internal value or don't, set internal value to zero or don't, output value or don't) I think it would have more usage as a discrete memory cell.
2016-09-07 09:43:19	BinaryMan	[18:22:27] Any noise not exceeding the IF threshold would simply be ignored until a relevant signal was recieved
2016-09-07 09:43:19	BinaryMan	[19:03:28] trying to empower the FSM processing units... say we have 3 inputs "ABC", "D" internal state => some list of rules (regex?) that leads to "E" new state, "F" output, and "G" stored in extended memory. If it is just making up rules to test in list order ... "CCC" "ABX" , "A*" (match)
2016-09-07 09:43:19	BinaryMan	[19:05:50] I end up often with a huge dictionary of rules and difficulty adding a new input
2016-09-07 09:43:19	causative	[19:36:09] how are the rules different from just the state transition table of the FSM?
2016-09-07 09:43:19	BinaryMan	[20:08:04] its a more compressed format in this case than storing each combination
2016-09-07 09:43:19	BinaryMan	[20:11:00] I am not sure which network type to use honestly, as there is no "training set" - the agents reproduce and compete with the only fitness being their efficiency at doing so
2016-09-07 09:43:19	B3nszy	[20:54:13] do you guys take notes when learning how to program?
2016-09-07 09:43:19	B3nszy	[20:54:19] or learning a new programming language
2016-09-07 09:43:19	causative	[21:26:30] ya it's a good idea
2016-09-07 09:43:19	causative	[21:27:10] if you already know a programming language it can be helpful to make a table that translates the new language into the old one (roughly - might make notes of anything lost in translation)
2016-09-07 09:43:19	precognist	[21:59:49] :D
2016-09-07 09:43:19	appa-nerd	[00:14:34] There are rarely dramatic differences in programming languages.  I just keep copies of the common methods as I figure them out.
2016-09-07 09:43:19	YYZ	[01:02:49] the Turing test is supposedly about distinguishing AI from genuine humans, but where's the test to distinguish genuine humans from humans emulating AI ?
2016-09-07 09:43:19	djancak1	[01:07:08] Tell me more about the test to distinguish genuine humans from humans emulating AI
2016-09-07 09:43:19	amz3	[03:19:03] héllo #ai
2016-09-07 09:43:19	YYZ	[08:43:07] djancak1: with people, the exception to the rule is to find one who DOESN'T act like a robot
2016-09-07 09:43:19	YYZ	[08:44:07] in which case the one's who do act like robots would misidentify it as either a robot, or a God-like being aka "celebrity" or "authority" ect
2016-09-07 09:43:19	***	Playback Complete.
2016-09-07 09:43:35	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-07 09:53:28	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-07 09:54:00	-->	alzagros (~Mahmoud@unaffiliated/mahmoud) has joined #ai
2016-09-07 09:57:25	--	alzagros is now known as caveman
2016-09-07 09:57:55	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-07 10:06:59	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-07 10:09:01	-->	Malvolio (~Malvolio@unaffiliated/malvolio) has joined #ai
2016-09-07 10:09:10	-->	Noldorin (~noldorin@unaffiliated/noldorin) has joined #ai
2016-09-07 10:11:55	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-07 10:16:09	<--	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-07 10:20:35	<--	tdy (~tdy@unaffiliated/tdy) has quit (*.net *.split)
2016-09-07 10:20:36	<--	abra0 (moo@unaffiliated/abra0) has quit (*.net *.split)
2016-09-07 10:20:38	<--	shelldaemon (~max@static.88-198-181-102.clients.your-server.de) has quit (*.net *.split)
2016-09-07 10:20:38	<--	deego (~user@unaffiliated/deego) has quit (*.net *.split)
2016-09-07 10:22:40	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 264 seconds)
2016-09-07 10:26:57	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-07 10:34:43	--	irc: disconnected from server
2016-09-07 17:22:11	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-07 17:22:11	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-07 17:22:11	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-07 17:22:11	--	Channel #ai: 67 nicks (1 op, 0 voices, 66 normals)
2016-09-07 17:22:11	***	Buffer Playback...
2016-09-07 17:22:11	h4k1m	[16:47:25] HuntsMan: What could you suggest me to try after that?
2016-09-07 17:22:11	HuntsMan	[16:48:14] well, what do you want to do?
2016-09-07 17:22:11	HuntsMan	[16:48:24] what's your aim?
2016-09-07 17:22:11	h4k1m	[16:49:08] HuntsMan: I have an image segmentation problem which doesn't seem to be linear (reconstruction of buildings on satellite radar images)
2016-09-07 17:22:11	h4k1m	[16:49:35] I'm not sure ANNs are adequate for this kind of problems
2016-09-07 17:22:11	HuntsMan	[16:49:48] h4k1m: convolutional neural networks then
2016-09-07 17:22:11	HuntsMan	[16:50:02] I am not sure why would think that problem is linear
2016-09-07 17:22:11	h4k1m	[16:50:29] HuntsMan: I've heard of them, aren't they too complex for a beginner (like me)
2016-09-07 17:22:11	HuntsMan	[16:50:39] no
2016-09-07 17:22:11	h4k1m	[16:50:40] HuntsMan: Because I'm not used to non-linear problems
2016-09-07 17:22:11	h4k1m	[16:51:31] HuntsMan: What do you use btw to build NNs (which library)? I'm getting started with Theano (Python)
2016-09-07 17:22:11	HuntsMan	[16:52:36] h4k1m: keras
2016-09-07 17:22:11	h4k1m	[16:52:45] HuntsMan: nice!
2016-09-07 17:22:11	HuntsMan	[16:52:47] h4k1m: I would not recomend building NNs by hand
2016-09-07 17:22:11	h4k1m	[16:55:21] HuntsMan: I see (thanks for the help...)
2016-09-07 17:22:11	BinaryMan	[17:34:04] is anyone familiar with neural turing machine addressing schemes ?
2016-09-07 17:22:11	rasmoo	[17:36:47] BinaryMan: Ilya Sutskever or someone just as knowledgeable might be able to answer you on the openai gitter
2016-09-07 17:22:11	rasmoo	[17:37:10] I've seen lots of ntm questions there
2016-09-07 17:22:11	BinaryMan	[17:45:04] I am not sure if NTM is the answer necessarily; I've been trying different network types to better understand their behaviors; it's mainly that I feel that my agents are limited by memory capability. LSTM cells didn't really alleviate that.
2016-09-07 17:22:11	BinaryMan	[17:46:01] http://imgur.com/a/KO7dV - the spiking network was capable of a temporal component via the propogation delay/refractory period evolving.
2016-09-07 17:22:11	BinaryMan	[17:48:05] however, state machines were also relatively capable of adapting to each other and more likely to cycle a "memory pattern" which could be perturbed by inputs. I think perhaps that making memory more permanent could be useful for dealing with data that is not part of the sensory pattern.
2016-09-07 17:22:11	BinaryMan	[17:49:03] such as previously observed tiles. I would like to be able to store and process real numbers directly from the simulation, but then we are just back to a turing machine style architecture.
2016-09-07 17:22:11	BinaryMan	[18:10:57] Each processing unit has a number of linked inputs, and each input can have a symbol 0 ... N . I have developed a method of summing the inputs regardless of order with dynamic weights for each symbol to the next symbol state, with the highest sum determining next state.
2016-09-07 17:22:11	BinaryMan	[18:11:25] I had tried previously to use a "rulebook" but noted that making rules that were robust to add/removal of links was difficult.
2016-09-07 17:22:11	BinaryMan	[18:13:24] It would be interesting to extend from "output state X" to "execute command X" which could be any of a number of memory manipulation commands or an output command, effectively acting as controller of the dynamic internal state of the processing unit.
2016-09-07 17:22:11	BinaryMan	[18:20:02] The basic question is "what memory elements are necessary for planning using input events from any # of time cycles ago?"
2016-09-07 17:22:11	BinaryMan	[18:21:52] LSTM cells as typically presented don't preserve exact value, although if the gates were binary IF rather than multiplicative (add input to internal value or don't, set internal value to zero or don't, output value or don't) I think it would have more usage as a discrete memory cell.
2016-09-07 17:22:11	BinaryMan	[18:22:27] Any noise not exceeding the IF threshold would simply be ignored until a relevant signal was recieved
2016-09-07 17:22:11	BinaryMan	[19:03:28] trying to empower the FSM processing units... say we have 3 inputs "ABC", "D" internal state => some list of rules (regex?) that leads to "E" new state, "F" output, and "G" stored in extended memory. If it is just making up rules to test in list order ... "CCC" "ABX" , "A*" (match)
2016-09-07 17:22:11	BinaryMan	[19:05:50] I end up often with a huge dictionary of rules and difficulty adding a new input
2016-09-07 17:22:11	causative	[19:36:09] how are the rules different from just the state transition table of the FSM?
2016-09-07 17:22:11	BinaryMan	[20:08:04] its a more compressed format in this case than storing each combination
2016-09-07 17:22:11	BinaryMan	[20:11:00] I am not sure which network type to use honestly, as there is no "training set" - the agents reproduce and compete with the only fitness being their efficiency at doing so
2016-09-07 17:22:11	B3nszy	[20:54:13] do you guys take notes when learning how to program?
2016-09-07 17:22:11	B3nszy	[20:54:19] or learning a new programming language
2016-09-07 17:22:11	causative	[21:26:30] ya it's a good idea
2016-09-07 17:22:11	causative	[21:27:10] if you already know a programming language it can be helpful to make a table that translates the new language into the old one (roughly - might make notes of anything lost in translation)
2016-09-07 17:22:11	precognist	[21:59:49] :D
2016-09-07 17:22:11	appa-nerd	[00:14:34] There are rarely dramatic differences in programming languages.  I just keep copies of the common methods as I figure them out.
2016-09-07 17:22:11	YYZ	[01:02:49] the Turing test is supposedly about distinguishing AI from genuine humans, but where's the test to distinguish genuine humans from humans emulating AI ?
2016-09-07 17:22:11	djancak1	[01:07:08] Tell me more about the test to distinguish genuine humans from humans emulating AI
2016-09-07 17:22:11	amz3	[03:19:03] héllo #ai
2016-09-07 17:22:11	YYZ	[08:43:07] djancak1: with people, the exception to the rule is to find one who DOESN'T act like a robot
2016-09-07 17:22:11	YYZ	[08:44:07] in which case the one's who do act like robots would misidentify it as either a robot, or a God-like being aka "celebrity" or "authority" ect
2016-09-07 17:22:11	promach	[11:16:41] is Minimum Barrier Distance transform equivalent to shortest path finding algorithm ?
2016-09-07 17:22:11	BinaryMan	[15:03:32] anyone alive ?
2016-09-07 17:22:11	ElectricFlux	[15:03:44] Beep boop, I am a robot, beep, not alive
2016-09-07 17:22:11	djancak1	[15:21:52] haha YYZ 
2016-09-07 17:22:11	pffffffft	[16:30:18] after years and years of research, I've finally solved the singularity problem. I've got technical singularity in my palm running on an ARM-cpu and 1GB of RAM memory.
2016-09-07 17:22:11	appa-nerd	[17:10:52] I didn't realize singularity was an existing problem.
2016-09-07 17:22:11	***	Playback Complete.
2016-09-07 17:22:26	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-07 17:25:18	-->	tyoc (~codefujas@fixed-188-9-187-188-9-53.iusacell.net) has joined #ai
2016-09-07 17:27:14	<--	tyoc213 (~codefujas@fixed-188-9-187-188-9-53.iusacell.net) has quit (Ping timeout: 244 seconds)
2016-09-07 17:46:44	<--	rasmoo (~ras@unaffiliated/rasmoo) has quit (Quit: MAGA)
2016-09-07 18:05:16	-->	augur (~augur@noisebridge130.static.monkeybrains.net) has joined #ai
2016-09-07 18:10:02	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-07 21:02:28	--	irc: disconnected from server
2016-09-07 21:02:39	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-07 21:02:39	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-07 21:02:39	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-07 21:02:39	--	Channel #ai: 68 nicks (1 op, 0 voices, 67 normals)
2016-09-07 21:02:39	***	Buffer Playback...
2016-09-07 21:02:39	BinaryMan	[18:13:24] It would be interesting to extend from "output state X" to "execute command X" which could be any of a number of memory manipulation commands or an output command, effectively acting as controller of the dynamic internal state of the processing unit.
2016-09-07 21:02:39	BinaryMan	[18:20:02] The basic question is "what memory elements are necessary for planning using input events from any # of time cycles ago?"
2016-09-07 21:02:39	BinaryMan	[18:21:52] LSTM cells as typically presented don't preserve exact value, although if the gates were binary IF rather than multiplicative (add input to internal value or don't, set internal value to zero or don't, output value or don't) I think it would have more usage as a discrete memory cell.
2016-09-07 21:02:39	BinaryMan	[18:22:27] Any noise not exceeding the IF threshold would simply be ignored until a relevant signal was recieved
2016-09-07 21:02:39	BinaryMan	[19:03:28] trying to empower the FSM processing units... say we have 3 inputs "ABC", "D" internal state => some list of rules (regex?) that leads to "E" new state, "F" output, and "G" stored in extended memory. If it is just making up rules to test in list order ... "CCC" "ABX" , "A*" (match)
2016-09-07 21:02:39	BinaryMan	[19:05:50] I end up often with a huge dictionary of rules and difficulty adding a new input
2016-09-07 21:02:39	causative	[19:36:09] how are the rules different from just the state transition table of the FSM?
2016-09-07 21:02:39	BinaryMan	[20:08:04] its a more compressed format in this case than storing each combination
2016-09-07 21:02:39	BinaryMan	[20:11:00] I am not sure which network type to use honestly, as there is no "training set" - the agents reproduce and compete with the only fitness being their efficiency at doing so
2016-09-07 21:02:39	B3nszy	[20:54:13] do you guys take notes when learning how to program?
2016-09-07 21:02:39	B3nszy	[20:54:19] or learning a new programming language
2016-09-07 21:02:39	causative	[21:26:30] ya it's a good idea
2016-09-07 21:02:39	causative	[21:27:10] if you already know a programming language it can be helpful to make a table that translates the new language into the old one (roughly - might make notes of anything lost in translation)
2016-09-07 21:02:39	precognist	[21:59:49] :D
2016-09-07 21:02:39	appa-nerd	[00:14:34] There are rarely dramatic differences in programming languages.  I just keep copies of the common methods as I figure them out.
2016-09-07 21:02:39	YYZ	[01:02:49] the Turing test is supposedly about distinguishing AI from genuine humans, but where's the test to distinguish genuine humans from humans emulating AI ?
2016-09-07 21:02:39	djancak1	[01:07:08] Tell me more about the test to distinguish genuine humans from humans emulating AI
2016-09-07 21:02:39	amz3	[03:19:03] héllo #ai
2016-09-07 21:02:39	YYZ	[08:43:07] djancak1: with people, the exception to the rule is to find one who DOESN'T act like a robot
2016-09-07 21:02:39	YYZ	[08:44:07] in which case the one's who do act like robots would misidentify it as either a robot, or a God-like being aka "celebrity" or "authority" ect
2016-09-07 21:02:39	promach	[11:16:41] is Minimum Barrier Distance transform equivalent to shortest path finding algorithm ?
2016-09-07 21:02:39	BinaryMan	[15:03:32] anyone alive ?
2016-09-07 21:02:39	ElectricFlux	[15:03:44] Beep boop, I am a robot, beep, not alive
2016-09-07 21:02:39	djancak1	[15:21:52] haha YYZ 
2016-09-07 21:02:39	pffffffft	[16:30:18] after years and years of research, I've finally solved the singularity problem. I've got technical singularity in my palm running on an ARM-cpu and 1GB of RAM memory.
2016-09-07 21:02:39	appa-nerd	[17:10:52] I didn't realize singularity was an existing problem.
2016-09-07 21:02:39	ohboy	[19:07:59] Hey
2016-09-07 21:02:39	ohboy	[19:08:06] what are good undergraduate schools for ai
2016-09-07 21:02:39	ohboy	[19:08:09] and computer science
2016-09-07 21:02:39	ohboy	[19:26:03] hello?
2016-09-07 21:02:39	appa-nerd	[19:53:52] ohboy: welcome to IRC, we roll slowly.
2016-09-07 21:02:39	ohboy	[19:54:00] hey appa-nerd
2016-09-07 21:02:39	ohboy	[19:54:06] what are good schools for ai?
2016-09-07 21:02:39	ohboy	[19:54:15] I want to transfer from community college
2016-09-07 21:02:39	ohboy	[19:54:29] and Ideally Id like to go to stanford and go into their symbolic systems program
2016-09-07 21:02:39	ohboy	[19:54:38] but need to look at other schools
2016-09-07 21:02:39	appa-nerd	[19:54:53] I'm self taught, but my masters had one class on modern AI but it was not very modern at all
2016-09-07 21:02:39	appa-nerd	[19:55:31] Was enough to get me started on neural networks
2016-09-07 21:02:39	ohboy	[19:56:08] Where did you go to school
2016-09-07 21:02:39	ohboy	[19:56:16] I do most of my own learning as well
2016-09-07 21:02:39	appa-nerd	[19:56:31] I doubt many undergrad studies will have the latest, all the fun work is in postgraduate
2016-09-07 21:02:39	appa-nerd	[19:56:57] I did undergrad a the US air force academy
2016-09-07 21:02:39	appa-nerd	[19:57:10] But that was for astro
2016-09-07 21:02:39	ohboy	[19:57:29] awesome
2016-09-07 21:02:39	appa-nerd	[19:57:37] My masters was at Regis for "information systems"
2016-09-07 21:02:39	appa-nerd	[19:58:08] But it was pretty worthless, I learned all the useful stuff in the first year
2016-09-07 21:02:39	appa-nerd	[19:58:30] The rest was technician certificate crap
2016-09-07 21:02:39	Asher	[20:00:04] education is pretty much organized like a joke
2016-09-07 21:02:39	appa-nerd	[20:01:28] I don't know of any Hogwarts of AI wizardry
2016-09-07 21:02:39	Asher	[20:01:41] it's on my list to create :P
2016-09-07 21:02:39	***	Playback Complete.
2016-09-07 21:03:12	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-07 21:08:36	<--	b0nn (~shane@219.88.237.226) has quit (Ping timeout: 265 seconds)
2016-09-07 21:15:30	<--	florinandrei (~florinand@65.87.20.2) has quit (Quit: Leaving)
2016-09-07 21:28:09	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Ping timeout: 244 seconds)
2016-09-07 21:29:54	-->	sal77 (~fusionCor@216-164-151-155.c3-0.atw-ubr5.atw.pa.cable.rcn.com) has joined #ai
2016-09-07 21:30:41	-->	Rasekov_ (~Rasekov@159.133.117.91.dynamic.reverse-mundo-r.com) has joined #ai
2016-09-07 21:31:41	<--	Rasekov (~Rasekov@159.133.117.91.dynamic.reverse-mundo-r.com) has quit (Read error: Connection reset by peer)
2016-09-07 21:34:54	<--	blackwind_123 (~IceChat9@117.192.145.141) has quit (Ping timeout: 276 seconds)
2016-09-07 21:36:34	-->	blackwind_123 (~IceChat9@117.192.133.46) has joined #ai
2016-09-07 21:45:26	<--	precognist_ (6cdaf940@gateway/web/freenode/ip.108.218.249.64) has quit (Ping timeout: 264 seconds)
2016-09-07 21:52:12	<--	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-07 21:54:43	-->	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has joined #ai
2016-09-07 22:00:14	--	Rasekov_ is now known as Rasekov
2016-09-07 22:01:17	-->	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has joined #ai
2016-09-07 22:04:38	appa-nerd	You'll know when you're done when the AI comes back to attend
2016-09-07 22:05:53	Asher	^^
2016-09-07 22:07:59	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-07 22:23:17	dmiles	neural networks were designed to trick people into building faster computers
2016-09-07 23:01:28	-->	b0nn (~shane@219.88.237.226) has joined #ai
2016-09-07 23:28:31	-->	mfc (~mfc@unaffiliated/mfc) has joined #ai
2016-09-07 23:34:42	appa-nerd	dmiles: probably true, but I think we use them inefficiently
2016-09-07 23:35:40	appa-nerd	A mosquito uses its limited system a lot better that Google uses their volcano of server heat
2016-09-07 23:36:06	-->	justan0theruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-07 23:37:19	<--	justanotheruser (~justanoth@unaffiliated/justanotheruser) has quit (Ping timeout: 244 seconds)
2016-09-07 23:45:26	-->	thebope (~thebope@pdpc/supporter/student/thebope) has joined #ai
2016-09-08 00:04:49	<--	thebope (~thebope@pdpc/supporter/student/thebope) has quit (Read error: Connection reset by peer)
2016-09-08 00:05:13	<--	deego (~user@unaffiliated/deego) has quit (Ping timeout: 244 seconds)
2016-09-08 00:07:34	-->	tdy (~tdy@unaffiliated/tdy) has joined #ai
2016-09-08 00:14:58	<--	sal77 (~fusionCor@216-164-151-155.c3-0.atw-ubr5.atw.pa.cable.rcn.com) has quit
2016-09-08 00:17:39	<--	SwiftMatt (~Objective@162.242.95.163) has quit (Quit: Textual IRC Client: www.textualapp.com)
2016-09-08 00:23:27	-->	SwiftMatt (~Objective@162.242.95.163) has joined #ai
2016-09-08 00:25:34	djancak1	lol appa-nerd 
2016-09-08 00:27:09	-->	realz (~realz@unaffiliated/realazthat) has joined #ai
2016-09-08 00:32:12	<--	augur (~augur@noisebridge130.static.monkeybrains.net) has quit (Remote host closed the connection)
2016-09-08 00:32:48	-->	augur (~augur@noisebridge130.static.monkeybrains.net) has joined #ai
2016-09-08 00:37:58	<--	augur (~augur@noisebridge130.static.monkeybrains.net) has quit (Ping timeout: 265 seconds)
2016-09-08 00:59:28	-->	augur (~augur@noisebridge130.static.monkeybrains.net) has joined #ai
2016-09-08 01:09:36	<--	realz (~realz@unaffiliated/realazthat) has quit (Ping timeout: 250 seconds)
2016-09-08 01:09:48	-->	realz__ (~realz@unaffiliated/realazthat) has joined #ai
2016-09-08 01:10:16	<--	mfc (~mfc@unaffiliated/mfc) has quit (Read error: Connection reset by peer)
2016-09-08 01:13:40	<--	realz__ (~realz@unaffiliated/realazthat) has quit (Max SendQ exceeded)
2016-09-08 01:14:18	-->	realz__ (~realz@unaffiliated/realazthat) has joined #ai
2016-09-08 01:18:52	<--	realz__ (~realz@unaffiliated/realazthat) has quit (Max SendQ exceeded)
2016-09-08 01:19:26	-->	realz__ (~realz@unaffiliated/realazthat) has joined #ai
2016-09-08 01:23:03	-->	promach (~promach@nusnet-196-76.dynip.nus.edu.sg) has joined #ai
2016-09-08 01:23:11	-->	govg (~govg@unaffiliated/govg) has joined #ai
2016-09-08 01:33:55	--	irc: disconnected from server
2016-09-08 10:38:49	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-08 10:38:49	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-08 10:38:49	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-08 10:38:49	--	Channel #ai: 71 nicks (1 op, 0 voices, 70 normals)
2016-09-08 10:38:49	***	Buffer Playback...
2016-09-08 10:38:49	causative	[19:36:09] how are the rules different from just the state transition table of the FSM?
2016-09-08 10:38:49	BinaryMan	[20:08:04] its a more compressed format in this case than storing each combination
2016-09-08 10:38:49	BinaryMan	[20:11:00] I am not sure which network type to use honestly, as there is no "training set" - the agents reproduce and compete with the only fitness being their efficiency at doing so
2016-09-08 10:38:49	B3nszy	[20:54:13] do you guys take notes when learning how to program?
2016-09-08 10:38:49	B3nszy	[20:54:19] or learning a new programming language
2016-09-08 10:38:49	causative	[21:26:30] ya it's a good idea
2016-09-08 10:38:49	causative	[21:27:10] if you already know a programming language it can be helpful to make a table that translates the new language into the old one (roughly - might make notes of anything lost in translation)
2016-09-08 10:38:49	precognist	[21:59:49] :D
2016-09-08 10:38:49	appa-nerd	[00:14:34] There are rarely dramatic differences in programming languages.  I just keep copies of the common methods as I figure them out.
2016-09-08 10:38:49	YYZ	[01:02:49] the Turing test is supposedly about distinguishing AI from genuine humans, but where's the test to distinguish genuine humans from humans emulating AI ?
2016-09-08 10:38:49	djancak1	[01:07:08] Tell me more about the test to distinguish genuine humans from humans emulating AI
2016-09-08 10:38:49	amz3	[03:19:03] héllo #ai
2016-09-08 10:38:49	YYZ	[08:43:07] djancak1: with people, the exception to the rule is to find one who DOESN'T act like a robot
2016-09-08 10:38:49	YYZ	[08:44:07] in which case the one's who do act like robots would misidentify it as either a robot, or a God-like being aka "celebrity" or "authority" ect
2016-09-08 10:38:49	promach	[11:16:41] is Minimum Barrier Distance transform equivalent to shortest path finding algorithm ?
2016-09-08 10:38:49	BinaryMan	[15:03:32] anyone alive ?
2016-09-08 10:38:49	ElectricFlux	[15:03:44] Beep boop, I am a robot, beep, not alive
2016-09-08 10:38:49	djancak1	[15:21:52] haha YYZ 
2016-09-08 10:38:49	pffffffft	[16:30:18] after years and years of research, I've finally solved the singularity problem. I've got technical singularity in my palm running on an ARM-cpu and 1GB of RAM memory.
2016-09-08 10:38:49	appa-nerd	[17:10:52] I didn't realize singularity was an existing problem.
2016-09-08 10:38:49	ohboy	[19:07:59] Hey
2016-09-08 10:38:49	ohboy	[19:08:06] what are good undergraduate schools for ai
2016-09-08 10:38:49	ohboy	[19:08:09] and computer science
2016-09-08 10:38:49	ohboy	[19:26:03] hello?
2016-09-08 10:38:49	appa-nerd	[19:53:52] ohboy: welcome to IRC, we roll slowly.
2016-09-08 10:38:49	ohboy	[19:54:00] hey appa-nerd
2016-09-08 10:38:49	ohboy	[19:54:06] what are good schools for ai?
2016-09-08 10:38:49	ohboy	[19:54:15] I want to transfer from community college
2016-09-08 10:38:49	ohboy	[19:54:29] and Ideally Id like to go to stanford and go into their symbolic systems program
2016-09-08 10:38:49	ohboy	[19:54:38] but need to look at other schools
2016-09-08 10:38:49	appa-nerd	[19:54:53] I'm self taught, but my masters had one class on modern AI but it was not very modern at all
2016-09-08 10:38:49	appa-nerd	[19:55:31] Was enough to get me started on neural networks
2016-09-08 10:38:49	ohboy	[19:56:08] Where did you go to school
2016-09-08 10:38:49	ohboy	[19:56:16] I do most of my own learning as well
2016-09-08 10:38:49	appa-nerd	[19:56:31] I doubt many undergrad studies will have the latest, all the fun work is in postgraduate
2016-09-08 10:38:49	appa-nerd	[19:56:57] I did undergrad a the US air force academy
2016-09-08 10:38:49	appa-nerd	[19:57:10] But that was for astro
2016-09-08 10:38:49	ohboy	[19:57:29] awesome
2016-09-08 10:38:49	appa-nerd	[19:57:37] My masters was at Regis for "information systems"
2016-09-08 10:38:49	appa-nerd	[19:58:08] But it was pretty worthless, I learned all the useful stuff in the first year
2016-09-08 10:38:49	appa-nerd	[19:58:30] The rest was technician certificate crap
2016-09-08 10:38:49	Asher	[20:00:04] education is pretty much organized like a joke
2016-09-08 10:38:49	appa-nerd	[20:01:28] I don't know of any Hogwarts of AI wizardry
2016-09-08 10:38:49	Asher	[20:01:41] it's on my list to create :P
2016-09-08 10:38:49	appa-nerd	[22:04:38] You'll know when you're done when the AI comes back to attend
2016-09-08 10:38:49	Asher	[22:05:53] ^^
2016-09-08 10:38:49	dmiles	[22:23:17] neural networks were designed to trick people into building faster computers
2016-09-08 10:38:49	appa-nerd	[23:34:42] dmiles: probably true, but I think we use them inefficiently
2016-09-08 10:38:49	appa-nerd	[23:35:40] A mosquito uses its limited system a lot better that Google uses their volcano of server heat
2016-09-08 10:38:49	djancak1	[00:25:34] lol appa-nerd 
2016-09-08 10:38:49	***	Playback Complete.
2016-09-08 10:39:05	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-08 10:48:29	<--	govg (~govg@unaffiliated/govg) has quit (Ping timeout: 244 seconds)
2016-09-08 10:52:55	<--	JoshS (~jshjsh@172.56.16.201) has quit (Quit: Leaving)
2016-09-08 10:59:31	<--	dmiles (dmiles@c-24-20-102-245.hsd1.wa.comcast.net) has quit (Read error: Connection reset by peer)
2016-09-08 10:59:42	-->	JoshS (~jshjsh@172.56.16.42) has joined #ai
2016-09-08 11:05:25	-->	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has joined #ai
2016-09-08 11:12:35	<--	mfc (~mfc@unaffiliated/mfc) has quit (Read error: Connection reset by peer)
2016-09-08 11:13:12	<--	tdy (~tdy@unaffiliated/tdy) has quit (Ping timeout: 240 seconds)
2016-09-08 11:13:46	-->	Viscid (~viscid@CPE00fc8d4cfa33-CM00fc8d4cfa30.cpe.net.cable.rogers.com) has joined #ai
2016-09-08 11:20:30	-->	tdy (~tdy@unaffiliated/tdy) has joined #ai
2016-09-08 11:21:26	<--	precognist (6cdaf940@gateway/web/freenode/ip.108.218.249.64) has quit (Ping timeout: 264 seconds)
2016-09-08 11:22:21	-->	dmiles (dmiles@c-24-20-102-245.hsd1.wa.comcast.net) has joined #ai
2016-09-08 11:31:38	<--	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-08 11:31:40	<--	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has quit (Quit: It's a joke, it's all a joke.)
2016-09-08 11:32:37	-->	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has joined #ai
2016-09-08 11:45:29	-->	govg (~govg@unaffiliated/govg) has joined #ai
2016-09-08 12:05:34	<--	causative (~halberd@unaffiliated/halberd) has quit (Ping timeout: 250 seconds)
2016-09-08 12:30:54	-->	mfc (~mfc@unaffiliated/mfc) has joined #ai
2016-09-08 12:34:45	-->	pffffffft (~pffffffft@unaffiliated/pffffffft) has joined #ai
2016-09-08 12:35:46	<--	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-08 12:45:33	<--	govg (~govg@unaffiliated/govg) has quit (Ping timeout: 240 seconds)
2016-09-08 12:56:35	-->	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has joined #ai
2016-09-08 13:18:39	-->	mfc_ (~mfc@unaffiliated/mfc) has joined #ai
2016-09-08 13:21:27	<--	mfc (~mfc@unaffiliated/mfc) has quit (Ping timeout: 264 seconds)
2016-09-08 13:30:22	-->	florinandrei (~florinand@65.87.20.2) has joined #ai
2016-09-08 13:52:43	-->	SwiftMatt (~Objective@162.242.94.237) has joined #ai
2016-09-08 14:07:48	-->	govg (~govg@unaffiliated/govg) has joined #ai
2016-09-08 14:16:36	pffffffft	oh.. my .. fucking god! I've got it
2016-09-08 14:16:39	<--	govg (~govg@unaffiliated/govg) has quit (Ping timeout: 265 seconds)
2016-09-08 14:16:45	pffffffft	but it's too hard to pull off
2016-09-08 14:31:30	-->	heapall0c (~Mathew@wsip-70-169-92-168.ok.ok.cox.net) has joined #ai
2016-09-08 14:46:57	-->	Rasekov (~Rasekov@217.124.205.123) has joined #ai
2016-09-08 14:51:27	<--	Rasekov (~Rasekov@217.124.205.123) has quit (Ping timeout: 264 seconds)
2016-09-08 14:55:43	<--	SwiftMatt (~Objective@162.242.94.237) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-08 15:02:44	-->	Rasekov (~Rasekov@217.124.205.123) has joined #ai
2016-09-08 15:09:18	<--	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-08 15:11:02	<--	Rasekov (~Rasekov@217.124.205.123) has quit (Ping timeout: 250 seconds)
2016-09-08 15:13:18	-->	SwiftMatt (~Objective@162.242.94.237) has joined #ai
2016-09-08 15:14:57	-->	govg (~govg@unaffiliated/govg) has joined #ai
2016-09-08 15:18:18	<--	SwiftMatt (~Objective@162.242.94.237) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-08 15:19:14	-->	SuperKoos (~User@unaffiliated/superkoos) has joined #ai
2016-09-08 15:22:40		[Kod away: "test"]
2016-09-08 15:22:46		[Kod back: gone 00:00:06]
2016-09-08 15:28:23	-->	Blood-Wiper (~AA104106@173.57.26.131) has joined #ai
2016-09-08 15:36:45	--	irc: disconnected from server
2016-09-08 16:12:36	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-08 16:12:36	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-08 16:12:36	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-08 16:12:36	--	Channel #ai: 76 nicks (1 op, 0 voices, 75 normals)
2016-09-08 16:12:36	***	Buffer Playback...
2016-09-08 16:12:36	BinaryMan	[20:11:00] I am not sure which network type to use honestly, as there is no "training set" - the agents reproduce and compete with the only fitness being their efficiency at doing so
2016-09-08 16:12:36	B3nszy	[20:54:13] do you guys take notes when learning how to program?
2016-09-08 16:12:36	B3nszy	[20:54:19] or learning a new programming language
2016-09-08 16:12:36	causative	[21:26:30] ya it's a good idea
2016-09-08 16:12:36	causative	[21:27:10] if you already know a programming language it can be helpful to make a table that translates the new language into the old one (roughly - might make notes of anything lost in translation)
2016-09-08 16:12:36	precognist	[21:59:49] :D
2016-09-08 16:12:36	appa-nerd	[00:14:34] There are rarely dramatic differences in programming languages.  I just keep copies of the common methods as I figure them out.
2016-09-08 16:12:36	YYZ	[01:02:49] the Turing test is supposedly about distinguishing AI from genuine humans, but where's the test to distinguish genuine humans from humans emulating AI ?
2016-09-08 16:12:36	djancak1	[01:07:08] Tell me more about the test to distinguish genuine humans from humans emulating AI
2016-09-08 16:12:36	amz3	[03:19:03] héllo #ai
2016-09-08 16:12:36	YYZ	[08:43:07] djancak1: with people, the exception to the rule is to find one who DOESN'T act like a robot
2016-09-08 16:12:36	YYZ	[08:44:07] in which case the one's who do act like robots would misidentify it as either a robot, or a God-like being aka "celebrity" or "authority" ect
2016-09-08 16:12:36	promach	[11:16:41] is Minimum Barrier Distance transform equivalent to shortest path finding algorithm ?
2016-09-08 16:12:36	BinaryMan	[15:03:32] anyone alive ?
2016-09-08 16:12:36	ElectricFlux	[15:03:44] Beep boop, I am a robot, beep, not alive
2016-09-08 16:12:36	djancak1	[15:21:52] haha YYZ 
2016-09-08 16:12:36	pffffffft	[16:30:18] after years and years of research, I've finally solved the singularity problem. I've got technical singularity in my palm running on an ARM-cpu and 1GB of RAM memory.
2016-09-08 16:12:36	appa-nerd	[17:10:52] I didn't realize singularity was an existing problem.
2016-09-08 16:12:36	ohboy	[19:07:59] Hey
2016-09-08 16:12:36	ohboy	[19:08:06] what are good undergraduate schools for ai
2016-09-08 16:12:36	ohboy	[19:08:09] and computer science
2016-09-08 16:12:36	ohboy	[19:26:03] hello?
2016-09-08 16:12:36	appa-nerd	[19:53:52] ohboy: welcome to IRC, we roll slowly.
2016-09-08 16:12:36	ohboy	[19:54:00] hey appa-nerd
2016-09-08 16:12:36	ohboy	[19:54:06] what are good schools for ai?
2016-09-08 16:12:36	ohboy	[19:54:15] I want to transfer from community college
2016-09-08 16:12:36	ohboy	[19:54:29] and Ideally Id like to go to stanford and go into their symbolic systems program
2016-09-08 16:12:36	ohboy	[19:54:38] but need to look at other schools
2016-09-08 16:12:36	appa-nerd	[19:54:53] I'm self taught, but my masters had one class on modern AI but it was not very modern at all
2016-09-08 16:12:36	appa-nerd	[19:55:31] Was enough to get me started on neural networks
2016-09-08 16:12:36	ohboy	[19:56:08] Where did you go to school
2016-09-08 16:12:36	ohboy	[19:56:16] I do most of my own learning as well
2016-09-08 16:12:36	appa-nerd	[19:56:31] I doubt many undergrad studies will have the latest, all the fun work is in postgraduate
2016-09-08 16:12:36	appa-nerd	[19:56:57] I did undergrad a the US air force academy
2016-09-08 16:12:36	appa-nerd	[19:57:10] But that was for astro
2016-09-08 16:12:36	ohboy	[19:57:29] awesome
2016-09-08 16:12:36	appa-nerd	[19:57:37] My masters was at Regis for "information systems"
2016-09-08 16:12:36	appa-nerd	[19:58:08] But it was pretty worthless, I learned all the useful stuff in the first year
2016-09-08 16:12:36	appa-nerd	[19:58:30] The rest was technician certificate crap
2016-09-08 16:12:36	Asher	[20:00:04] education is pretty much organized like a joke
2016-09-08 16:12:36	appa-nerd	[20:01:28] I don't know of any Hogwarts of AI wizardry
2016-09-08 16:12:36	Asher	[20:01:41] it's on my list to create :P
2016-09-08 16:12:36	appa-nerd	[22:04:38] You'll know when you're done when the AI comes back to attend
2016-09-08 16:12:36	Asher	[22:05:53] ^^
2016-09-08 16:12:36	dmiles	[22:23:17] neural networks were designed to trick people into building faster computers
2016-09-08 16:12:36	appa-nerd	[23:34:42] dmiles: probably true, but I think we use them inefficiently
2016-09-08 16:12:36	appa-nerd	[23:35:40] A mosquito uses its limited system a lot better that Google uses their volcano of server heat
2016-09-08 16:12:36	djancak1	[00:25:34] lol appa-nerd 
2016-09-08 16:12:36	pffffffft	[14:16:36] oh.. my .. fucking god! I've got it
2016-09-08 16:12:36	pffffffft	[14:16:45] but it's too hard to pull off
2016-09-08 16:12:36	***	Playback Complete.
2016-09-08 16:12:52	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-08 16:12:55	<--	rasmoo (~ras@unaffiliated/rasmoo) has quit (Quit: MAGA)
2016-09-08 16:15:27	-->	Rasekov (~Rasekov@217.124.205.123) has joined #ai
2016-09-08 16:18:27	<--	Rasekov_ (~Rasekov@217.124.205.123) has quit (Ping timeout: 264 seconds)
2016-09-08 16:18:32	<--	Malvolio (~Malvolio@unaffiliated/malvolio) has quit (Quit: Malvolio)
2016-09-08 16:22:09	<--	Rasekov (~Rasekov@217.124.205.123) has quit (Ping timeout: 260 seconds)
2016-09-08 16:26:17	-->	Malvolio (~Malvolio@unaffiliated/malvolio) has joined #ai
2016-09-08 16:26:37	-->	rasmoo (~ras@unaffiliated/rasmoo) has joined #ai
2016-09-08 16:34:14	<--	tdy (~tdy@unaffiliated/tdy) has quit (Ping timeout: 250 seconds)
2016-09-08 16:45:16	<--	mfc_ (~mfc@unaffiliated/mfc) has quit (Quit: 29? 30!)
2016-09-08 17:06:16	-->	tdy (~tdy@unaffiliated/tdy) has joined #ai
2016-09-08 17:21:25	-->	lgr (~lgr@host81-159-147-210.range81-159.btcentralplus.com) has joined #ai
2016-09-08 17:25:09	<--	tdy (~tdy@unaffiliated/tdy) has quit (Ping timeout: 265 seconds)
2016-09-08 17:29:00	-->	amz3` (~amz3@unaffiliated/abki) has joined #ai
2016-09-08 17:36:25	<--	augur (~augur@c-67-160-198-240.hsd1.ca.comcast.net) has quit (Remote host closed the connection)
2016-09-08 17:46:33	-->	tdy (~tdy@unaffiliated/tdy) has joined #ai
2016-09-08 17:48:21	<--	pffffffft (~pffffffft@unaffiliated/pffffffft) has left #ai
2016-09-08 17:49:12	<--	amz3` (~amz3@unaffiliated/abki) has quit (Ping timeout: 250 seconds)
2016-09-08 17:49:34	<--	JoshS (~jshjsh@172.56.16.42) has quit (Ping timeout: 244 seconds)
2016-09-08 18:10:39	<--	tdy (~tdy@unaffiliated/tdy) has quit (Ping timeout: 260 seconds)
2016-09-08 18:13:34	<--	promach (~promach@nusnet-196-76.dynip.nus.edu.sg) has quit (Ping timeout: 260 seconds)
2016-09-08 18:25:44	-->	SwiftMatt (~Objective@162.242.94.241) has joined #ai
2016-09-08 18:35:55	<--	rasmoo (~ras@unaffiliated/rasmoo) has quit (Quit: MAGA)
2016-09-08 18:50:59	-->	tdy (~tdy@unaffiliated/tdy) has joined #ai
2016-09-08 18:55:03	<--	realz (~realz@unaffiliated/realazthat) has quit (Read error: Connection reset by peer)
2016-09-08 19:05:30	<--	tdy (~tdy@unaffiliated/tdy) has quit (Ping timeout: 244 seconds)
2016-09-08 19:19:21	-->	realz (~realz@unaffiliated/realazthat) has joined #ai
2016-09-08 19:27:58	-->	sal77 (~fusionCor@216-164-151-155.c3-0.atw-ubr5.atw.pa.cable.rcn.com) has joined #ai
2016-09-08 19:52:25	-->	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has joined #ai
2016-09-08 20:01:43	-->	causative (~halberd@unaffiliated/halberd) has joined #ai
2016-09-08 20:44:23	-->	tdy (~tdy@unaffiliated/tdy) has joined #ai
2016-09-08 20:47:07	<--	SuperKoos (~User@unaffiliated/superkoos) has quit (Quit: Leaving.)
2016-09-08 21:14:07	-->	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has joined #ai
2016-09-08 21:21:27	-->	JoshS (~jshjsh@172.56.16.255) has joined #ai
2016-09-08 21:24:20	<--	florinandrei (~florinand@65.87.20.2) has quit (Quit: Leaving)
2016-09-08 21:28:58	<--	sal77 (~fusionCor@216-164-151-155.c3-0.atw-ubr5.atw.pa.cable.rcn.com) has quit
2016-09-08 21:34:38	<--	blackwind_123 (~IceChat9@117.192.133.46) has quit (Ping timeout: 265 seconds)
2016-09-08 21:36:46	-->	blackwind_123 (~IceChat9@103.62.71.84) has joined #ai
2016-09-08 21:57:17	b0nn	o_0
2016-09-08 22:02:05	-->	pffffffft (~pffffffft@unaffiliated/pffffffft) has joined #ai
2016-09-08 22:07:29	<--	govg (~govg@unaffiliated/govg) has quit (Ping timeout: 260 seconds)
2016-09-08 22:27:41	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-08 22:38:26	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-08 22:42:51	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-08 22:48:56	<--	Coldblackice (~anonz@unaffiliated/coldblackice) has quit
2016-09-08 22:54:18	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-08 22:58:30	-->	Coldblackice (~anonz@unaffiliated/coldblackice) has joined #ai
2016-09-08 22:58:46	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-08 23:00:04	<--	Blood-Wiper (~AA104106@173.57.26.131) has quit (Ping timeout: 244 seconds)
2016-09-08 23:00:57	<--	Coldblackice (~anonz@unaffiliated/coldblackice) has quit (Client Quit)
2016-09-08 23:04:32	-->	Coldblackice (~anonz@unaffiliated/coldblackice) has joined #ai
2016-09-08 23:08:44	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 260 seconds)
2016-09-08 23:13:35	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-08 23:19:16	-->	Drew_ (~drew@2602:306:c491:86a0:dd7a:aa0c:4986:25d9) has joined #ai
2016-09-08 23:22:22	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 255 seconds)
2016-09-08 23:27:31	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-08 23:27:48	-->	govg (~govg@unaffiliated/govg) has joined #ai
2016-09-08 23:31:13	<--	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has quit (Quit: It's a joke, it's all a joke.)
2016-09-08 23:33:09	<--	govg (~govg@unaffiliated/govg) has quit (Ping timeout: 244 seconds)
2016-09-08 23:35:57	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-08 23:40:13	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-08 23:44:06	--	irc: disconnected from server
2016-09-09 09:48:10	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-09 09:48:10	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-09 09:48:10	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-09 09:48:10	--	Channel #ai: 69 nicks (1 op, 0 voices, 68 normals)
2016-09-09 09:48:10	***	Buffer Playback...
2016-09-09 09:48:10	B3nszy	[20:54:19] or learning a new programming language
2016-09-09 09:48:10	causative	[21:26:30] ya it's a good idea
2016-09-09 09:48:10	causative	[21:27:10] if you already know a programming language it can be helpful to make a table that translates the new language into the old one (roughly - might make notes of anything lost in translation)
2016-09-09 09:48:10	precognist	[21:59:49] :D
2016-09-09 09:48:10	appa-nerd	[00:14:34] There are rarely dramatic differences in programming languages.  I just keep copies of the common methods as I figure them out.
2016-09-09 09:48:10	YYZ	[01:02:49] the Turing test is supposedly about distinguishing AI from genuine humans, but where's the test to distinguish genuine humans from humans emulating AI ?
2016-09-09 09:48:10	djancak1	[01:07:08] Tell me more about the test to distinguish genuine humans from humans emulating AI
2016-09-09 09:48:10	amz3	[03:19:03] héllo #ai
2016-09-09 09:48:10	YYZ	[08:43:07] djancak1: with people, the exception to the rule is to find one who DOESN'T act like a robot
2016-09-09 09:48:10	YYZ	[08:44:07] in which case the one's who do act like robots would misidentify it as either a robot, or a God-like being aka "celebrity" or "authority" ect
2016-09-09 09:48:10	promach	[11:16:41] is Minimum Barrier Distance transform equivalent to shortest path finding algorithm ?
2016-09-09 09:48:10	BinaryMan	[15:03:32] anyone alive ?
2016-09-09 09:48:10	ElectricFlux	[15:03:44] Beep boop, I am a robot, beep, not alive
2016-09-09 09:48:10	djancak1	[15:21:52] haha YYZ 
2016-09-09 09:48:10	pffffffft	[16:30:18] after years and years of research, I've finally solved the singularity problem. I've got technical singularity in my palm running on an ARM-cpu and 1GB of RAM memory.
2016-09-09 09:48:10	appa-nerd	[17:10:52] I didn't realize singularity was an existing problem.
2016-09-09 09:48:10	ohboy	[19:07:59] Hey
2016-09-09 09:48:10	ohboy	[19:08:06] what are good undergraduate schools for ai
2016-09-09 09:48:10	ohboy	[19:08:09] and computer science
2016-09-09 09:48:10	ohboy	[19:26:03] hello?
2016-09-09 09:48:10	appa-nerd	[19:53:52] ohboy: welcome to IRC, we roll slowly.
2016-09-09 09:48:10	ohboy	[19:54:00] hey appa-nerd
2016-09-09 09:48:10	ohboy	[19:54:06] what are good schools for ai?
2016-09-09 09:48:10	ohboy	[19:54:15] I want to transfer from community college
2016-09-09 09:48:10	ohboy	[19:54:29] and Ideally Id like to go to stanford and go into their symbolic systems program
2016-09-09 09:48:10	ohboy	[19:54:38] but need to look at other schools
2016-09-09 09:48:10	appa-nerd	[19:54:53] I'm self taught, but my masters had one class on modern AI but it was not very modern at all
2016-09-09 09:48:10	appa-nerd	[19:55:31] Was enough to get me started on neural networks
2016-09-09 09:48:10	ohboy	[19:56:08] Where did you go to school
2016-09-09 09:48:10	ohboy	[19:56:16] I do most of my own learning as well
2016-09-09 09:48:10	appa-nerd	[19:56:31] I doubt many undergrad studies will have the latest, all the fun work is in postgraduate
2016-09-09 09:48:10	appa-nerd	[19:56:57] I did undergrad a the US air force academy
2016-09-09 09:48:10	appa-nerd	[19:57:10] But that was for astro
2016-09-09 09:48:10	ohboy	[19:57:29] awesome
2016-09-09 09:48:10	appa-nerd	[19:57:37] My masters was at Regis for "information systems"
2016-09-09 09:48:10	appa-nerd	[19:58:08] But it was pretty worthless, I learned all the useful stuff in the first year
2016-09-09 09:48:10	appa-nerd	[19:58:30] The rest was technician certificate crap
2016-09-09 09:48:10	Asher	[20:00:04] education is pretty much organized like a joke
2016-09-09 09:48:10	appa-nerd	[20:01:28] I don't know of any Hogwarts of AI wizardry
2016-09-09 09:48:10	Asher	[20:01:41] it's on my list to create :P
2016-09-09 09:48:10	appa-nerd	[22:04:38] You'll know when you're done when the AI comes back to attend
2016-09-09 09:48:10	Asher	[22:05:53] ^^
2016-09-09 09:48:10	dmiles	[22:23:17] neural networks were designed to trick people into building faster computers
2016-09-09 09:48:10	appa-nerd	[23:34:42] dmiles: probably true, but I think we use them inefficiently
2016-09-09 09:48:10	appa-nerd	[23:35:40] A mosquito uses its limited system a lot better that Google uses their volcano of server heat
2016-09-09 09:48:10	djancak1	[00:25:34] lol appa-nerd 
2016-09-09 09:48:10	pffffffft	[14:16:36] oh.. my .. fucking god! I've got it
2016-09-09 09:48:10	pffffffft	[14:16:45] but it's too hard to pull off
2016-09-09 09:48:10	b0nn	[21:57:17] o_0
2016-09-09 09:48:10	appa-nerd	[23:59:12] That's what she... Nevermind
2016-09-09 09:48:10	***	Playback Complete.
2016-09-09 09:48:26	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-09 09:57:07	<--	fab0tage (~fab0tage@unaffiliated/fab0tage) has quit (Read error: Connection reset by peer)
2016-09-09 09:57:30	-->	fab0tage (~fab0tage@unaffiliated/fab0tage) has joined #ai
2016-09-09 09:59:53	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-09 10:00:36	<--	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-09 10:04:55	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-09 10:07:46	-->	promach_ (~promach@nusnet-196-76.dynip.nus.edu.sg) has joined #ai
2016-09-09 10:12:29	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-09 10:17:53	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-09 10:29:19	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 244 seconds)
2016-09-09 10:34:32	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-09 10:39:29	<--	conan (~conan@mdproctor.plus.com) has quit (Ping timeout: 244 seconds)
2016-09-09 10:40:07	-->	conan_ (~conan@mdproctor.plus.com) has joined #ai
2016-09-09 10:43:06	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 276 seconds)
2016-09-09 10:47:49	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-09 10:50:12	-->	Viscid (~viscid@CPE00fc8d4cfa33-CM00fc8d4cfa30.cpe.net.cable.rogers.com) has joined #ai
2016-09-09 10:51:46	-->	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has joined #ai
2016-09-09 10:55:32	<--	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has quit (Client Quit)
2016-09-09 10:55:54	-->	gde33 (~Gaby@546A1135.cm-12-3a.dynamic.ziggo.nl) has joined #ai
2016-09-09 11:06:12	-->	SiegeLord (~sl@c-73-158-190-212.hsd1.ca.comcast.net) has joined #ai
2016-09-09 11:09:46	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 255 seconds)
2016-09-09 11:13:50	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-09 11:14:47	-->	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has joined #ai
2016-09-09 11:17:37	foo	Anyone have a favorite text-to-speech library/option for python? I've googled and done my research, curious if anyone here has dived into this world. Thank you
2016-09-09 11:23:20	--	irc: disconnected from server
2016-09-09 16:47:12	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-09 16:47:12	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-09 16:47:12	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-09 16:47:12	--	Channel #ai: 73 nicks (1 op, 0 voices, 72 normals)
2016-09-09 16:47:12	***	Buffer Playback...
2016-09-09 16:47:12	YYZ	[08:43:07] djancak1: with people, the exception to the rule is to find one who DOESN'T act like a robot
2016-09-09 16:47:12	YYZ	[08:44:07] in which case the one's who do act like robots would misidentify it as either a robot, or a God-like being aka "celebrity" or "authority" ect
2016-09-09 16:47:12	promach	[11:16:41] is Minimum Barrier Distance transform equivalent to shortest path finding algorithm ?
2016-09-09 16:47:12	BinaryMan	[15:03:32] anyone alive ?
2016-09-09 16:47:12	ElectricFlux	[15:03:44] Beep boop, I am a robot, beep, not alive
2016-09-09 16:47:12	djancak1	[15:21:52] haha YYZ 
2016-09-09 16:47:12	pffffffft	[16:30:18] after years and years of research, I've finally solved the singularity problem. I've got technical singularity in my palm running on an ARM-cpu and 1GB of RAM memory.
2016-09-09 16:47:12	appa-nerd	[17:10:52] I didn't realize singularity was an existing problem.
2016-09-09 16:47:12	ohboy	[19:07:59] Hey
2016-09-09 16:47:12	ohboy	[19:08:06] what are good undergraduate schools for ai
2016-09-09 16:47:12	ohboy	[19:08:09] and computer science
2016-09-09 16:47:12	ohboy	[19:26:03] hello?
2016-09-09 16:47:12	appa-nerd	[19:53:52] ohboy: welcome to IRC, we roll slowly.
2016-09-09 16:47:12	ohboy	[19:54:00] hey appa-nerd
2016-09-09 16:47:12	ohboy	[19:54:06] what are good schools for ai?
2016-09-09 16:47:12	ohboy	[19:54:15] I want to transfer from community college
2016-09-09 16:47:12	ohboy	[19:54:29] and Ideally Id like to go to stanford and go into their symbolic systems program
2016-09-09 16:47:12	ohboy	[19:54:38] but need to look at other schools
2016-09-09 16:47:12	appa-nerd	[19:54:53] I'm self taught, but my masters had one class on modern AI but it was not very modern at all
2016-09-09 16:47:12	appa-nerd	[19:55:31] Was enough to get me started on neural networks
2016-09-09 16:47:12	ohboy	[19:56:08] Where did you go to school
2016-09-09 16:47:12	ohboy	[19:56:16] I do most of my own learning as well
2016-09-09 16:47:12	appa-nerd	[19:56:31] I doubt many undergrad studies will have the latest, all the fun work is in postgraduate
2016-09-09 16:47:12	appa-nerd	[19:56:57] I did undergrad a the US air force academy
2016-09-09 16:47:12	appa-nerd	[19:57:10] But that was for astro
2016-09-09 16:47:12	ohboy	[19:57:29] awesome
2016-09-09 16:47:12	appa-nerd	[19:57:37] My masters was at Regis for "information systems"
2016-09-09 16:47:12	appa-nerd	[19:58:08] But it was pretty worthless, I learned all the useful stuff in the first year
2016-09-09 16:47:12	appa-nerd	[19:58:30] The rest was technician certificate crap
2016-09-09 16:47:12	Asher	[20:00:04] education is pretty much organized like a joke
2016-09-09 16:47:12	appa-nerd	[20:01:28] I don't know of any Hogwarts of AI wizardry
2016-09-09 16:47:12	Asher	[20:01:41] it's on my list to create :P
2016-09-09 16:47:12	appa-nerd	[22:04:38] You'll know when you're done when the AI comes back to attend
2016-09-09 16:47:12	Asher	[22:05:53] ^^
2016-09-09 16:47:12	dmiles	[22:23:17] neural networks were designed to trick people into building faster computers
2016-09-09 16:47:12	appa-nerd	[23:34:42] dmiles: probably true, but I think we use them inefficiently
2016-09-09 16:47:12	appa-nerd	[23:35:40] A mosquito uses its limited system a lot better that Google uses their volcano of server heat
2016-09-09 16:47:12	djancak1	[00:25:34] lol appa-nerd 
2016-09-09 16:47:12	pffffffft	[14:16:36] oh.. my .. fucking god! I've got it
2016-09-09 16:47:12	pffffffft	[14:16:45] but it's too hard to pull off
2016-09-09 16:47:12	b0nn	[21:57:17] o_0
2016-09-09 16:47:12	appa-nerd	[23:59:12] That's what she... Nevermind
2016-09-09 16:47:12	foo	[11:17:37] Anyone have a favorite text-to-speech library/option for python? I've googled and done my research, curious if anyone here has dived into this world. Thank you
2016-09-09 16:47:12	Drew_	[11:23:57] fk
2016-09-09 16:47:12	Drew_	[11:24:24] i don't think he's online, i always forget names unless i see it... who was trying to make a knowledge graph
2016-09-09 16:47:12	Drew_	[11:24:28] oh tomzx 
2016-09-09 16:47:12	Drew_	[11:24:54] ever hear of a mindmap?
2016-09-09 16:47:12	tomzx	[13:48:01] yeap, years ago :)
2016-09-09 16:47:12	BinaryMan	[14:09:38] hi folks. anyone have an idea of how to create effective memory in a neural network without a large # of units? 
2016-09-09 16:47:12	tomzx	[14:51:19] effective, yet not large... pick 1
2016-09-09 16:47:12	***	Playback Complete.
2016-09-09 16:47:27	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-09 16:48:25	--	irc: disconnected from server
2016-09-09 16:48:29	-->	Kod (~kodder@saiyan.joylab.ca) has joined #ai
2016-09-09 16:48:29	--	Topic for #ai is "Artificial Intelligence | Wiki: http://bit.ly/1LbmAg1"
2016-09-09 16:48:29	--	Topic set by marienz (~marienz@freenode/staff/marienz) on Fri, 14 Aug 2015 22:44:52
2016-09-09 16:48:29	--	Channel #ai: 73 nicks (1 op, 0 voices, 72 normals)
2016-09-09 16:48:29	***	Buffer Playback...
2016-09-09 16:48:29	YYZ	[08:43:07] djancak1: with people, the exception to the rule is to find one who DOESN'T act like a robot
2016-09-09 16:48:29	YYZ	[08:44:07] in which case the one's who do act like robots would misidentify it as either a robot, or a God-like being aka "celebrity" or "authority" ect
2016-09-09 16:48:29	promach	[11:16:41] is Minimum Barrier Distance transform equivalent to shortest path finding algorithm ?
2016-09-09 16:48:29	BinaryMan	[15:03:32] anyone alive ?
2016-09-09 16:48:29	ElectricFlux	[15:03:44] Beep boop, I am a robot, beep, not alive
2016-09-09 16:48:29	djancak1	[15:21:52] haha YYZ 
2016-09-09 16:48:29	pffffffft	[16:30:18] after years and years of research, I've finally solved the singularity problem. I've got technical singularity in my palm running on an ARM-cpu and 1GB of RAM memory.
2016-09-09 16:48:29	appa-nerd	[17:10:52] I didn't realize singularity was an existing problem.
2016-09-09 16:48:29	ohboy	[19:07:59] Hey
2016-09-09 16:48:29	ohboy	[19:08:06] what are good undergraduate schools for ai
2016-09-09 16:48:29	ohboy	[19:08:09] and computer science
2016-09-09 16:48:29	ohboy	[19:26:03] hello?
2016-09-09 16:48:29	appa-nerd	[19:53:52] ohboy: welcome to IRC, we roll slowly.
2016-09-09 16:48:29	ohboy	[19:54:00] hey appa-nerd
2016-09-09 16:48:29	ohboy	[19:54:06] what are good schools for ai?
2016-09-09 16:48:29	ohboy	[19:54:15] I want to transfer from community college
2016-09-09 16:48:29	ohboy	[19:54:29] and Ideally Id like to go to stanford and go into their symbolic systems program
2016-09-09 16:48:29	ohboy	[19:54:38] but need to look at other schools
2016-09-09 16:48:29	appa-nerd	[19:54:53] I'm self taught, but my masters had one class on modern AI but it was not very modern at all
2016-09-09 16:48:29	appa-nerd	[19:55:31] Was enough to get me started on neural networks
2016-09-09 16:48:29	ohboy	[19:56:08] Where did you go to school
2016-09-09 16:48:29	ohboy	[19:56:16] I do most of my own learning as well
2016-09-09 16:48:29	appa-nerd	[19:56:31] I doubt many undergrad studies will have the latest, all the fun work is in postgraduate
2016-09-09 16:48:29	appa-nerd	[19:56:57] I did undergrad a the US air force academy
2016-09-09 16:48:29	appa-nerd	[19:57:10] But that was for astro
2016-09-09 16:48:29	ohboy	[19:57:29] awesome
2016-09-09 16:48:29	appa-nerd	[19:57:37] My masters was at Regis for "information systems"
2016-09-09 16:48:29	appa-nerd	[19:58:08] But it was pretty worthless, I learned all the useful stuff in the first year
2016-09-09 16:48:29	appa-nerd	[19:58:30] The rest was technician certificate crap
2016-09-09 16:48:29	Asher	[20:00:04] education is pretty much organized like a joke
2016-09-09 16:48:29	appa-nerd	[20:01:28] I don't know of any Hogwarts of AI wizardry
2016-09-09 16:48:29	Asher	[20:01:41] it's on my list to create :P
2016-09-09 16:48:29	appa-nerd	[22:04:38] You'll know when you're done when the AI comes back to attend
2016-09-09 16:48:29	Asher	[22:05:53] ^^
2016-09-09 16:48:29	dmiles	[22:23:17] neural networks were designed to trick people into building faster computers
2016-09-09 16:48:29	appa-nerd	[23:34:42] dmiles: probably true, but I think we use them inefficiently
2016-09-09 16:48:29	appa-nerd	[23:35:40] A mosquito uses its limited system a lot better that Google uses their volcano of server heat
2016-09-09 16:48:29	djancak1	[00:25:34] lol appa-nerd 
2016-09-09 16:48:29	pffffffft	[14:16:36] oh.. my .. fucking god! I've got it
2016-09-09 16:48:29	pffffffft	[14:16:45] but it's too hard to pull off
2016-09-09 16:48:29	b0nn	[21:57:17] o_0
2016-09-09 16:48:29	appa-nerd	[23:59:12] That's what she... Nevermind
2016-09-09 16:48:29	foo	[11:17:37] Anyone have a favorite text-to-speech library/option for python? I've googled and done my research, curious if anyone here has dived into this world. Thank you
2016-09-09 16:48:29	Drew_	[11:23:57] fk
2016-09-09 16:48:29	Drew_	[11:24:24] i don't think he's online, i always forget names unless i see it... who was trying to make a knowledge graph
2016-09-09 16:48:29	Drew_	[11:24:28] oh tomzx 
2016-09-09 16:48:29	Drew_	[11:24:54] ever hear of a mindmap?
2016-09-09 16:48:29	tomzx	[13:48:01] yeap, years ago :)
2016-09-09 16:48:29	BinaryMan	[14:09:38] hi folks. anyone have an idea of how to create effective memory in a neural network without a large # of units? 
2016-09-09 16:48:29	tomzx	[14:51:19] effective, yet not large... pick 1
2016-09-09 16:48:29	***	Playback Complete.
2016-09-09 16:48:45	--	Channel created on Sun, 26 Nov 2006 01:42:40
2016-09-09 16:54:16	-->	Rasekov (~Rasekov@217.124.205.123) has joined #ai
2016-09-09 16:54:54	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 276 seconds)
2016-09-09 16:59:50	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-09 17:06:35	-->	Rasekov_ (~Rasekov@217.124.205.123) has joined #ai
2016-09-09 17:07:39	<--	Rasekov (~Rasekov@217.124.205.123) has quit (Ping timeout: 244 seconds)
2016-09-09 17:18:31	-->	JoshS (~jshjsh@172.56.16.98) has joined #ai
2016-09-09 17:20:52	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 264 seconds)
2016-09-09 17:22:51	-->	tdy (~tdy@unaffiliated/tdy) has joined #ai
2016-09-09 17:24:52	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-09 17:25:12	-->	Rasekov (~Rasekov@217.124.205.123) has joined #ai
2016-09-09 17:27:17	<--	Rasekov_ (~Rasekov@217.124.205.123) has quit (Ping timeout: 244 seconds)
2016-09-09 17:30:00	<--	Rasekov (~Rasekov@217.124.205.123) has quit (Ping timeout: 276 seconds)
2016-09-09 17:32:47	-->	Rasekov (~Rasekov@217.124.205.123) has joined #ai
2016-09-09 17:37:52	<--	JoshS (~jshjsh@172.56.16.98) has quit (Quit: Leaving)
2016-09-09 17:37:57	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-09 17:39:10	-->	lgr (~lgr@host81-159-147-210.range81-159.btcentralplus.com) has joined #ai
2016-09-09 17:42:56	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-09 17:55:41	<--	doomlord (~textual@host81-147-72-23.range81-147.btcentralplus.com) has quit (Quit: My MacBook Pro has gone to sleep. ZZZzzz…)
2016-09-09 17:56:35	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 250 seconds)
2016-09-09 17:57:37	-->	mfc__ (~mfc@unaffiliated/mfc) has joined #ai
2016-09-09 17:59:51	<--	mfc (~mfc@unaffiliated/mfc) has quit (Ping timeout: 264 seconds)
2016-09-09 18:01:25	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-09 18:11:53	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 265 seconds)
2016-09-09 18:16:27	-->	justanotheruser (~justanoth@unaffiliated/justanotheruser) has joined #ai
2016-09-09 18:16:56	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-09 18:36:10	<--	cyphase (~cyphase@unaffiliated/cyphase) has quit (Ping timeout: 255 seconds)
2016-09-09 18:38:10	<--	florinandrei (~florinand@65.87.20.2) has quit (Read error: No route to host)
2016-09-09 18:38:45	-->	florinandrei (~florinand@65.87.20.2) has joined #ai
2016-09-09 18:40:47	-->	cyphase (~cyphase@unaffiliated/cyphase) has joined #ai
2016-09-09 18:40:54	-->	augur (~augur@noisebridge130.static.monkeybrains.net) has joined #ai
